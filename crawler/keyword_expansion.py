import os, sys, re, json,time
from dotenv import load_dotenv
from supabase import create_client
from keybert import KeyBERT
from collections import Counter
from flask import jsonify
from crawler.integrated_crawler import save_articles_from_naver_parallel



# í™˜ê²½ ë³€ìˆ˜ ë° Supabase ì„¤ì •
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))
load_dotenv()
supabase = create_client(os.getenv("SUPABASE_URL"), os.getenv("SUPABASE_KEY"))

# í‚¤ì›Œë“œ ì¶”ì¶œ ëª¨ë¸
kw_model = KeyBERT(model='snunlp/KR-SBERT-V40K-klueNLI-augSTS')


def is_valid_keyword(word):
    if not word: return False
    if len(word) < 2 or len(word) > 10: return False
    if re.match(r"^\d", word): return False  # ìˆ«ìë¡œ ì‹œì‘
    if any(x in word for x in ["ë³´ë„", "ê¸°ì", "ë¶€ê³ ", "ìš´ì„¸", "ë‰´ìŠ¤", "afpbbnews"]): return False
    if re.search(r"[ê°€-í£]+(ì˜|ë¥¼|ì€|ëŠ”|ì´|ê°€|ì—|ë¡œ|ê³¼|ì™€|ë„)$", word): return False
    if any(x in word for x in ["ì–µ", "ì²œë§Œ", "ìœ„ì•ˆ", "ë‹¬ëŸ¬"]): return False
    if word.endswith(("ì„", "ë¥¼", "ì€", "ëŠ”", "ì´", "ê°€", "ì—", "ì˜", "ë¡œ")): return False
    return True

def get_top_keywords_by_title(query_keyword, top_n=3):
    print(f"\nğŸ§© '{query_keyword}' ê´€ë ¨ ê¸°ì‚¬ ì œëª©ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ")

    # Supabaseì—ì„œ query_keywordì— í•´ë‹¹í•˜ëŠ” ê¸°ì‚¬ë“¤ ë¶ˆëŸ¬ì˜¤ê¸°
    response = supabase.table("test").select("title").eq("query_keyword", query_keyword).execute()
    print(f"ğŸ“„ ì œëª© ìˆ˜: {len(response.data)}")

    word_counter = Counter()
    for row in response.data:
        title = row.get("title", "")
        words = re.findall(r"[ê°€-í£]{2,}", title)  # í•œê¸€ 2ê¸€ì ì´ìƒë§Œ ì¶”ì¶œ
        filtered = [w for w in words if is_valid_keyword(w) and w != query_keyword]
        word_counter.update(filtered)

    # ìƒìœ„ top_n ë°˜í™˜
    top_keywords = word_counter.most_common(top_n)
    print(f"ğŸ¯ ìµœë¹ˆë„ í‚¤ì›Œë“œ: {top_keywords}")
    return [{"name": kw, "score": round(freq / top_keywords[0][1], 3)} for kw, freq in top_keywords]


def expand_keywords(query_keyword):
    print(f"âœ… ì‹œì‘: {query_keyword}")

    children_list = get_top_keywords_by_title(query_keyword)
    print(f"í™•ì¥ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸: {children_list}")

    if not children_list:
        print(f"ğŸ”„ ê¸°ì‚¬ ì—†ìŒ, í¬ë¡¤ë§ ìˆ˜í–‰: {query_keyword}")
        save_articles_from_naver_parallel(query_keyword)
        print(f"í¬ë¡¤ë§ ì™„ë£Œ, ì¬ì¡°íšŒ ì‹œì‘: {query_keyword}")
        children_list = get_top_keywords_by_title(query_keyword)
        print(f"í¬ë¡¤ë§ í›„ í™•ì¥ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸: {children_list}")

    return children_list

# def expand_crawl_with_tree(child_keyword):
#     save_articles_from_naver_parallel(child_keyword['name'])

#         # grand_keywords = get_top_keywords_by_title(child['name'], top_n=2)
#         # for g in grand_keywords:
#         #     try:
#         #         print(f"ğŸŒ ì†ì í¬ë¡¤ë§ ì‹œì‘: {g['name']}")
#         #         save_articles_from_naver_parallel(g['name'])
#         #         print(f"ğŸŒ ì†ì í¬ë¡¤ë§ ì™„ë£Œ: {g['name']}")
#         #     except Exception as e:
#         #         print(f"âŒ ì†ì í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
#         # child["children"] = grand_keywords

#     print("í¬ë¡¤ë§+í™•ì¥ ê²°ê³¼ íŠ¸ë¦¬:", )
#     return tree
