{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4a2edb1",
   "metadata": {},
   "source": [
    "# **í¬ë¡¤ëŸ¬ 1ê°œë§Œ ì ìš©í•´ì„œ ì €ì¥**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "aeeb5cda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ì €ì¥ ì™„ë£Œ: ç¾ ê³µë¬´ì›ë“¤ë„ ë‹¹í•˜ëŠ” AI ì‚¬ê¸°â€¦FBIì„œ ìŒì„±ë©”ì‹œì§€ ì£¼ì˜ ê²½ë³´\n",
      "âœ… ì €ì¥ ì™„ë£Œ: \"ì—¬ìë¼ì„œ ì£½ì—ˆë‹¤\" ê°•ë‚¨ì—­ ì‚´ì¸ì‚¬ê±´ 9ì£¼ê¸°â€¦'ì—¬í˜ ê·œíƒ„' ì‹œìœ„ ê³³ê³³ì„œ\n",
      "âœ… ì €ì¥ ì™„ë£Œ: ì†Œê°œíŒ… ì—¬ì„±ì´ ì‹ ì²´ ì ‘ì´‰ ê±°ì ˆí•˜ì 'ë°±ì´ˆí¬'í•œ 20ëŒ€ë‚¨\n",
      "âœ… ì €ì¥ ì™„ë£Œ: ì´ì¬ëª… \"ì—¬ì„± ë°›ëŠ” ì°¨ë³„ ê°œì„  ë…¸ë ¥í•´ì•¼â€¦ë‚¨ë…€ êµ¬ë¶„í•´ ê°ˆë“±í•˜ëŠ” ê²ƒ ì˜³ì§€ ì•Šì•„\"\n",
      "âœ… ì €ì¥ ì™„ë£Œ: ìœ„ì¥ì „ì… ì´ì¬ëª…Â·ìš¸ë³´ ê¹€ë¬¸ìˆ˜â€¦'ìˆ˜ë°±ë§Œ í´ë¦­' ë¶€ë¥´ëŠ” ë§¤ìš´ë§› 'ê°€ì§œë‰´ìŠ¤ ë°ˆ'[ê°€ì§œ íŒì¹˜ëŠ” SNSì •ì¹˜â‘ ]\n",
      "âœ… ì €ì¥ ì™„ë£Œ: ë¶€ì‚°êµìœ¡ì²­, êµì‚¬ 350ëª… ëŒ€ìƒ â€˜ë””ì§€í„¸ ì„±í­ë ¥ ì˜ˆë°© ì—°ìˆ˜â€™â€¦ë”¥í˜ì´í¬ ëŒ€ì‘\n",
      "âœ… ì €ì¥ ì™„ë£Œ: ï§¡ëŒ€í–‰ \"ëŒ€í†µë ¹ ì„ ê±°, êµ­ë¯¼ í†µí•© ì´ë£¨ëŠ” ì—­ì‚¬ì  ì „í™˜ì  ë¼ì•¼\"\n",
      "âœ… ì €ì¥ ì™„ë£Œ: ì˜¤ëŠ” 12ì¼ë¶€í„° 6Â·3ëŒ€ì„  ê³µì‹ì„ ê±°ìš´ë™â€¦\"ì¼ë°˜ ìœ ê¶Œìë„ ì„ ê±°ìš´ë™ ê°€ëŠ¥\"\n",
      "âœ… ì €ì¥ ì™„ë£Œ: ê´‘ì£¼ê²½ì°°, ì¹´ì¹´ì˜¤Tì™€ ë³´ì´ìŠ¤í”¼ì‹±Â·ë”¥í˜ì´í¬ ì˜ˆë°© í™ë³´\n",
      "âœ… ì €ì¥ ì™„ë£Œ: â€˜ëŒ€ì„  ëŒ€ë¹„â€™ ì „êµ­ ê²½ì°° ì§€íœ˜ë¶€ í™”ìƒíšŒì˜â€¦â€œìš°ë°œìƒí™© ì² ì € ëŒ€ë¹„â€\n",
      "âœ… ì €ì¥ ì™„ë£Œ: ë¼ì˜¨ì‹œíì–´, ì£¼ì‹ ì•¡ë©´ë³‘í•© í›„ ê±°ë˜ ì¬ê°œâ€¦ëª¨ë°”ì¼ì‹ ë¶„ì¦Â·ì–‘ìë‚´ì„±ì•”í˜¸Â·AIë³´ì•ˆ ì„±ì¥ê¸°ë°˜ ê°•í™”\n",
      "âœ… ì €ì¥ ì™„ë£Œ: \"ìœ ì „ì´ì•¼ë§ë¡œ ê°€ì¥ ê°•ë ¥í•œ AI\"â€¦ì˜ˆìœ ë§¨ì–¼êµ´ ì˜ìƒìœ¼ë¡œ 'í•©ì„±' ë…¼ë€ ì ì¬ìš´ ì‹ ë¶€\n",
      "âœ… ì €ì¥ ì™„ë£Œ: ì—¬ê°€ë¶€, 10.9ì–µì› ì¶”ê²½â€¦ë””ì§€í„¸ì„±ë²”ì£„ í”¼í•´ì ì§€ì›\n",
      "\n",
      "ğŸ“° ê¸°ì‚¬ ì´ 13ê±´ Supabaseì— ì €ì¥ ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import urllib.parse\n",
    "from urllib.parse import urlparse\n",
    "from bs4 import BeautifulSoup\n",
    "from supabase import create_client, Client\n",
    "\n",
    "# âœ… Supabase ì„¤ì •\n",
    "SUPABASE_URL = \"https://ypyujiaoeaqykbqetjef.supabase.co\"\n",
    "SUPABASE_KEY = \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6InlweXVqaWFvZWFxeWticWV0amVmIiwicm9sZSI6ImFub24iLCJpYXQiOjE3NDY1NDUyNTQsImV4cCI6MjA2MjEyMTI1NH0.RuR9l89gxCcMkSzO053EHluQ0ers-piN4SUjZ-LtWjU\"\n",
    "supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)\n",
    "\n",
    "# âœ… ì•„ì‹œì•„ê²½ì œ í¬ë¡¤ë§ í•¨ìˆ˜\n",
    "def crawl_asiae_news(url):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0'\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        print(\"âŒ ê¸°ì‚¬ ìš”ì²­ ì‹¤íŒ¨:\", response.status_code)\n",
    "        return None\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # ì œëª© ì¶”ì¶œ\n",
    "    title_tag = soup.find('h1')  # ê·¸ëƒ¥ ì²« ë²ˆì§¸ h1 íƒœê·¸ ì°¾ê¸°\n",
    "    title = title_tag.get_text(strip=True) if title_tag else \"ì œëª© ì—†ìŒ\"\n",
    "    \n",
    "    body_tag = soup.find('div', class_='article')\n",
    "\n",
    "    if body_tag:\n",
    "        # classê°€ txt_prohibitionì¸ p íƒœê·¸ëŠ” ì œì™¸í•˜ê³  í…ìŠ¤íŠ¸ ì¶”ì¶œ\n",
    "        p_tags = [p for p in body_tag.find_all('p') if 'txt_prohibition' not in p.get('class', [])]\n",
    "\n",
    "        raw_text = \"\\n\".join([p.get_text(strip=True) for p in p_tags]).strip()\n",
    "\n",
    "        # ê° ì¤„ ê³µë°± ì œê±° ë° ë¹ˆ ì¤„ ì œê±°\n",
    "        body_lines = [line.strip() for line in raw_text.split(\"\\n\") if line.strip()]\n",
    "\n",
    "        body = ''.join(body_lines)\n",
    "    else:\n",
    "        body = \"ë³¸ë¬¸ ì—†ìŒ\"\n",
    "\n",
    "\n",
    "    return {\n",
    "        \"title\": title,\n",
    "        \"url\": url,\n",
    "        \"body\": body,\n",
    "        \"media\": \"ì•„ì‹œì•„ê²½ì œ\"\n",
    "    }\n",
    "\n",
    "# âœ… Supabaseì— ì €ì¥\n",
    "def save_to_supabase(data):\n",
    "    try:\n",
    "        # ì¤‘ë³µ í™•ì¸ (url ê¸°ì¤€)\n",
    "        existing = supabase.table(\"articles\").select(\"id\").eq(\"url\", data[\"url\"]).execute()\n",
    "        if existing.data:\n",
    "            print(\"âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬:\", data[\"url\"])\n",
    "            return\n",
    "        response = supabase.table(\"articles\").insert([data]).execute()\n",
    "        print(\"âœ… ì €ì¥ ì™„ë£Œ:\", data[\"title\"])\n",
    "    except Exception as e:\n",
    "        print(\"âŒ ì €ì¥ ì‹¤íŒ¨:\", e)\n",
    "\n",
    "# âœ… ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ í›„ ì—°í•©ë‰´ìŠ¤ ê¸°ì‚¬ë§Œ ì €ì¥\n",
    "def save_articles_from_naver(query):\n",
    "    client_id = \"_TznE38btYhyzWYsq9XK\"\n",
    "    client_secret = \"06UYVlSHF9\"\n",
    "\n",
    "    encoded_query = urllib.parse.quote(query)\n",
    "    headers = {\n",
    "        \"X-Naver-Client-Id\": client_id,\n",
    "        \"X-Naver-Client-Secret\": client_secret\n",
    "    }\n",
    "\n",
    "    display = 100\n",
    "    total_saved = 0\n",
    "\n",
    "    for start in range(1, 1000 + 1, display):\n",
    "        url = f\"https://openapi.naver.com/v1/search/news.json?query={encoded_query}&display={display}&start={start}&sort=date\"\n",
    "        response = requests.get(url, headers=headers)\n",
    "\n",
    "        if response.status_code != 200:\n",
    "            print(f\"âŒ ìš”ì²­ ì‹¤íŒ¨ at start={start}: {response.status_code}\")\n",
    "            continue\n",
    "\n",
    "        data = response.json()\n",
    "        items = data.get(\"items\", [])\n",
    "        if not items:\n",
    "            break\n",
    "\n",
    "        for item in items:\n",
    "            originallink = item.get(\"originallink\", \"\")\n",
    "            domain = urlparse(originallink).netloc\n",
    "            if domain == \"view.asiae.co.kr\": # ìˆ˜ì • âœ…âœ…âœ…âœ…âœ…âœ…\n",
    "                article = crawl_asiae_news(originallink) # âœ…âœ…âœ…âœ…âœ…âœ…\n",
    "                if article:\n",
    "                    save_to_supabase(article)\n",
    "                    total_saved += 1\n",
    "\n",
    "        if len(items) < display:\n",
    "            break\n",
    "\n",
    "    print(f\"\\nğŸ“° ê¸°ì‚¬ ì´ {total_saved}ê±´ Supabaseì— ì €ì¥ ì™„ë£Œ\")\n",
    "\n",
    "# âœ… ì‹¤í–‰\n",
    "save_articles_from_naver(\"ë”¥í˜ì´í¬\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4241ecb8",
   "metadata": {},
   "source": [
    "## **í†µí•© í¬ë¡¤ëŸ¬ ì½”ë“œ**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537bb5c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.news1.kr/politics/assembly/5785957\n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.newsis.com/view/NISX20250516_0003178759\n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.newsis.com/view/NISX20250517_0003179429\n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.newsis.com/view/NISX20250516_0003178893\n",
      "âœ… ì €ì¥ ì™„ë£Œ: ë””ì§€í„¸ ì•„ë…¸ë¯¸, ì•Œê³ í¬ëŸ¬ì‹œ... AI ë¬¸ëª… ì‹œëŒ€ì— ëŒ€í•œ ì‚¬íšŒí•™ìì˜ ê²½ê³ \n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.chosun.com/politics/election2025/2025/05/16/3KMOI3VZRZGW7OJQ4IZ42RPSJY/?utm_source=naver&utm_medium=referral&utm_campaign=naver-news\n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.yna.co.kr/view/AKR20250516115200001?input=1195m\n",
      "âœ… ì €ì¥ ì™„ë£Œ: ì´ì¬ëª…, ì—¬ì„±ê³µì•½ \"êµì œí­ë ¥Â·ìŠ¤í† í‚¹Â·ë””ì§€í„¸ì„±ë²”ì£„ ê°•ë ¥ ëŒ€ì‘í•  ê²ƒ\"\n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.newsis.com/view/NISX20250516_0003178868\n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.news1.kr/politics/assembly/5785352\n",
      "âœ… ì €ì¥ ì™„ë£Œ: [ì†ë³´]ì´ì¬ëª… \"ë”¥í˜ì´í¬ ë“± ë””ì§€í„¸ ì„±ë²”ì£„ ì§‘ì¤‘ ëª¨ë‹ˆí„°ë§\"\n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.newsis.com/view/NISX20250516_0003178806\n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.newsis.com/view/NISX20250516_0003178097\n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.chosun.com/national/court_law/2025/05/15/3T2BMR6CZ5ACVCGK6HHG6WAZIY/?utm_source=naver&utm_medium=referral&utm_campaign=naver-news\n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.newsis.com/view/NISX20250515_0003177499\n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.news1.kr/it-science/general-it/5784220\n",
      "âœ… ì €ì¥ ì™„ë£Œ: \"AI í™•ì‚°ì— í†µì‹ ì„œë¹„ìŠ¤ í–‰íƒœ ë³€í™”...ì´ìš©ìë³´í˜¸ ìƒˆ ì ‘ê·¼ ë°©ì‹ í•„ìš”\"\n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.yna.co.kr/view/AKR20250515098500056?input=1195m\n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.yna.co.kr/view/AKR20250515061200001?input=1195m\n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.newsis.com/view/NISX20250515_0003176700\n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.news1.kr/local/jeju/5783604\n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.news1.kr/politics/assembly/5783580\n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.news1.kr/society/incident-accident/5783095\n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.yna.co.kr/view/AKR20250514101700505?input=1195m\n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.chosun.com/national/education/2025/05/15/KXEGRV2C4JAUJGSDG3WU2HXHMM/?utm_source=naver&utm_medium=referral&utm_campaign=naver-news\n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.news1.kr/it-science/cc-newmedia/5782936\n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.yna.co.kr/view/AKR20250514138700017?input=1195m\n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.newsis.com/view/NISX20250514_0003175833\n",
      "âœ… ì €ì¥ ì™„ë£Œ: í•˜ë£¨ ë§Œì— 270ê±´ ëŠ˜ì—ˆë‹¤â€¦ 'ë¶ˆë²• ë”¥í˜ì´í¬'ì— ì„ ê´€ìœ„Â·ê²½ì°° ë¹„ìƒ\n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.newsis.com/view/NISX20250514_0003175121\n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.yna.co.kr/view/AKR20250514039200017?input=1195m\n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.newsis.com/view/NISX20250514_0003174700\n",
      "âœ… ì €ì¥ ì™„ë£Œ: \"ë‹¤ìŒ ëŒ€í†µë ¹ì€ ë‹¤ìŒ(Daum)ê³¼ í•¨ê»˜\"â€¦ì œ21ëŒ€ ëŒ€ì„  íŠ¹ì§‘ í˜ì´ì§€\n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.news1.kr/it-science/general-it/5782070\n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.newsis.com/view/NISX20250514_0003174607\n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.news1.kr/society/incident-accident/5781886\n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.newsis.com/view/NISX20250513_0003173845\n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.yna.co.kr/view/AKR20250513114500017?input=1195m\n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.yna.co.kr/view/AKR20250513047600530?input=1195m\n",
      "âœ… ì €ì¥ ì™„ë£Œ: ì‘ë…„ êµê¶Œì¹¨í•´ 4200ì—¬ ê±´â€¦'ì •ë‹¹í•œ ìƒí™œì§€ë„ ë¶ˆì‘'ì´ 32%\n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.newsis.com/view/NISX20250513_0003173430\n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.news1.kr/society/education/5781170\n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.yna.co.kr/view/AKR20250513041200009?input=1195m\n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.yna.co.kr/view/AKR20250512074900004?input=1195m\n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.newsis.com/view/NISX20250512_0003172026\n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.news1.kr/society/incident-accident/5779858\n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.newsis.com/view/NISX20250512_0003171989\n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.news1.kr/politics/general-politics/5779752\n",
      "âœ… ì €ì¥ ì™„ë£Œ: ì´ì£¼í˜¸ ê¶Œí•œëŒ€í–‰ \"êµ­ë¯¼í†µí•© ìœ„í•´ ê³µì • ì„ ê±° ì¤‘ìš”..ìœ„ë²•ì‹œ ë¬´ê´€ìš©\"\n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.yna.co.kr/view/AKR20250512056800530?input=1195m\n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.newsis.com/view/NISX20250512_0003171697\n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.news1.kr/local/daegu-gyeongbuk/5778854\n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.news1.kr/politics/assembly/5777732\n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.yna.co.kr/view/AKR20250509066400052?input=1195m\n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.yna.co.kr/view/AKR20250509050300017?input=1195m\n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.news1.kr/it-science/general-it/5777457\n",
      "âœ… ì €ì¥ ì™„ë£Œ: ê°•ê²½ìœ¤ ê¸°ì, ê¹€ì„¸ì˜ ê³ ì†Œâ€¦\"æ•…ê¹€ìƒˆë¡  ì œë³´ì ì°¾ì•„ê°„ ì  ì—†ì–´\"\n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.newsis.com/view/NISX20250508_0003167665\n",
      "âœ… ì €ì¥ ì™„ë£Œ: \"íˆ¬ì ì„±ê³µ, 40ì–µ ì§‘ ìƒ€ë‹¤\" 120ì–µ ëœ¯ì–´ê°„ ê·¸ë…€ ì •ì²´ì— 'ê¹œì§'â€¦ëŒ€ì²˜ ì–´ë–»ê²Œ\n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.news1.kr/local/gwangju-jeonnam/5776222\n",
      "âœ… ì €ì¥ ì™„ë£Œ: ê²½ì°°, ëŒ€ì„  íˆ¬í‘œì¼ì— 16ë§Œ8000ëª… íˆ¬ì…â€¦\"ìš°ë°œìƒí™© ì² ì € ëŒ€ë¹„\"\n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.newsis.com/view/NISX20250508_0003167022\n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.news1.kr/society/incident-accident/5775389\n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.yna.co.kr/view/AKR20250508047200004?input=1195m\n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.news1.kr/society/general-society/5775503\n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.newsis.com/view/NISX20250507_0003166634\n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.yna.co.kr/view/AKR20250507090300505?input=1195m\n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.newsis.com/view/NISX20250507_0003165527\n",
      "âœ… ì €ì¥ ì™„ë£Œ: ë¼ì˜¨ì‹œíì–´ 5ëŒ€1 ì•¡ë©´ë³‘í•© í›„ ê±°ë˜ ì¬ê°œ... \"êµ­ë‚´ì™¸ ì„±ì¥ê¸°ë°˜ ê°•í™”\"\n",
      "âœ… ì €ì¥ ì™„ë£Œ: ì‹œì‘ë„ ëª»í•œ ëŒ€í•œë¯¼êµ­ AIì‚°ì—…, ë°œëª©ë¶€í„° ì¡ëŠ” 'ì´ê²ƒ'\n",
      "âœ… ì €ì¥ ì™„ë£Œ: ì±—GPT 3ë…„ ëëŠ”ë° í•œêµ­ì€?â€¦'ë” ê°•í•œ AIê·œì œ' ë§Œë“¤ê¸° ë°”ë¹´ë‹¤\n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.news1.kr/society/incident-accident/5773697\n",
      "âœ… ì €ì¥ ì™„ë£Œ: ì–´ì œì˜ í”¼í•´ìê°€ ì˜¤ëŠ˜ì˜ ê°€í•´ìë¡œâ€¦ì‚¬ì´ë²„ ì„±í­ë ¥ ëŠªì— ë¹ ì§„ 10ëŒ€ë“¤\n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.newsis.com/view/NISX20250504_0003164362\n",
      "âœ… ì €ì¥ ì™„ë£Œ: 'AI ìœ¤ì„ì—´Â·ì´ì¬ëª…'ì´ ëŒ€ì„ íŒ í”ë“œë‚˜â€¦'ë¶ˆë²• ë”¥í˜ì´í¬' ì£¼ì˜ë³´\n",
      "âœ… ì €ì¥ ì™„ë£Œ: 'ì¿µì¿µ' ì‹¬ì¥ë°•ë™ê¹Œì§€ í›”ì¹œ ë”¥í˜ì´í¬â€¦\"êµ¬ë¶„ ë” ì–´ë µë„¤\"\n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.newsis.com/view/NISX20250502_0003162438\n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.yna.co.kr/view/AKR20250503034400530?input=1195m\n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.news1.kr/finance/blockchain-fintech/5771501\n",
      "âœ… ì €ì¥ ì™„ë£Œ: 'ë”¥í˜ì´í¬Â·ë¶ˆë²• ë¦¬ë² ì´íŠ¸' ë“± ê²½ì°°ê´€ 11ëª…, íŠ¹ì§„\n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.news1.kr/society/incident-accident/5772097\n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.yna.co.kr/view/AKR20250501112800004?input=1195m\n",
      "âœ… ì €ì¥ ì™„ë£Œ: \"ì´ˆë“±ìƒ ìë…€ ì„±êµìœ¡ ì–´ì©Œì§€\"â€¦ë¶€ëª¨ë“¤ ì‹ ì²­ ëª°ë¦¬ë”ë‹ˆ 'ì¡°ê¸° ë§ˆê°'\n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.yna.co.kr/view/AKR20250502070000530?input=1195m\n",
      "âœ… ì €ì¥ ì™„ë£Œ: ì—¬ê°€ë¶€, ì¶”ê²½ 10.9ì–µ...ë””ì§€í„¸ì„±ë²”ì£„ í”¼í•´ì ì§€ì›\n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.news1.kr/society/general-society/5771963\n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.newsis.com/view/NISX20250502_0003162646\n",
      "\n",
      "âœ… ì €ì¥ ìš”ì•½\n",
      "ğŸ“° ì—°í•©ë‰´ìŠ¤ ê¸°ì‚¬ ì´ 0ê±´ Supabaseì— ì €ì¥ ì™„ë£Œ\n",
      "ğŸ“° ì¡°ì„ ì¼ë³´ ê¸°ì‚¬ ì´ 0ê±´ Supabaseì— ì €ì¥ ì™„ë£Œ\n",
      "ğŸ“° ë‰´ì‹œìŠ¤ ê¸°ì‚¬ ì´ 0ê±´ Supabaseì— ì €ì¥ ì™„ë£Œ\n",
      "ğŸ“° ë‰´ìŠ¤1 ê¸°ì‚¬ ì´ 0ê±´ Supabaseì— ì €ì¥ ì™„ë£Œ\n",
      "ğŸ“° ë¨¸ë‹ˆíˆ¬ë°ì´ ê¸°ì‚¬ ì´ 20ê±´ Supabaseì— ì €ì¥ ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import urllib.parse\n",
    "from urllib.parse import urlparse\n",
    "from bs4 import BeautifulSoup\n",
    "from supabase import create_client, Client\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import time\n",
    "\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "# âœ… Supabase ì„¤ì •\n",
    "SUPABASE_URL = \"https://ypyujiaoeaqykbqetjef.supabase.co\"\n",
    "SUPABASE_KEY = \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6InlweXVqaWFvZWFxeWticWV0amVmIiwicm9sZSI6ImFub24iLCJpYXQiOjE3NDY1NDUyNTQsImV4cCI6MjA2MjEyMTI1NH0.RuR9l89gxCcMkSzO053EHluQ0ers-piN4SUjZ-LtWjU\"\n",
    "supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)\n",
    "\n",
    "# âœ… ì—°í•©ë‰´ìŠ¤ í¬ë¡¤ë§ í•¨ìˆ˜\n",
    "def crawl_yonhap_news(url):\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        return None\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    title_tag = soup.find('h1', class_='tit01')\n",
    "    title = title_tag.get_text(strip=True) if title_tag else \"ì œëª© ì—†ìŒ\"\n",
    "\n",
    "    body_tag = soup.find('div', class_='story-news article')\n",
    "    if body_tag:\n",
    "        paragraphs = body_tag.find_all('p')\n",
    "        body_lines = []\n",
    "        for p in paragraphs:\n",
    "            if 'txt-copyright' in p.get('class', []): continue\n",
    "            text = p.get_text(strip=True)\n",
    "            if text:\n",
    "                body_lines.append(text)\n",
    "        body = '\\n'.join(body_lines)\n",
    "    else:\n",
    "        body = \"ë³¸ë¬¸ ì—†ìŒ\"\n",
    "\n",
    "    return {\"title\": title, \"url\": url, \"body\": body, \"media\": \"ì—°í•©ë‰´ìŠ¤\"}\n",
    "\n",
    "# âœ… ì¡°ì„ ì¼ë³´ í¬ë¡¤ë§ í•¨ìˆ˜\n",
    "def crawl_chosun_news(url):\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument('--headless')\n",
    "    options.add_argument('--no-sandbox')\n",
    "    options.add_argument('--disable-dev-shm-usage')\n",
    "\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "    driver.get(url)\n",
    "    time.sleep(3)\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    driver.quit()\n",
    "\n",
    "    title_tag = soup.find('h1')\n",
    "    title = title_tag.get_text(strip=True) if title_tag else \"ì œëª© ì—†ìŒ\"\n",
    "\n",
    "    body_paragraphs = soup.select('p.article-body__content.article-body__content-text')\n",
    "    body_lines = []\n",
    "    for p in body_paragraphs:\n",
    "        text = p.get_text(strip=True)\n",
    "        if not text:\n",
    "            continue\n",
    "        if any(keyword in text for keyword in ['ê¸°ì', 'ë¬´ë‹¨ ì „ì¬', 'êµ¬ë…', 'Copyright']):\n",
    "            continue\n",
    "        body_lines.append(text)\n",
    "    body = '\\n'.join(body_lines) if body_lines else \"ë³¸ë¬¸ ì—†ìŒ\"\n",
    "\n",
    "    return {\"title\": title, \"url\": url, \"body\": body, \"media\": \"ì¡°ì„ ì¼ë³´\"}\n",
    "\n",
    "# âœ… ë‰´ì‹œìŠ¤ í¬ë¡¤ë§ í•¨ìˆ˜\n",
    "def crawl_newsis_news(url):\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        return None\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    title_tag = soup.select_one('h1.tit.title_area')\n",
    "    title = title_tag.get_text(strip=True) if title_tag else \"ì œëª© ì—†ìŒ\"\n",
    "\n",
    "    body_tag = soup.find('div', class_='viewer')\n",
    "    if body_tag:\n",
    "        raw_text = body_tag.get_text(separator=\"\\n\").strip()\n",
    "        body_lines = [line.strip() for line in raw_text.split(\"\\n\") if line.strip()]\n",
    "        body = '\\n'.join(body_lines)\n",
    "    else:\n",
    "        body = \"ë³¸ë¬¸ ì—†ìŒ\"\n",
    "\n",
    "    return {\"title\": title, \"url\": url, \"body\": body, \"media\": \"ë‰´ì‹œìŠ¤\"}\n",
    "\n",
    "\n",
    "def crawl_news1_news(url):\n",
    "    # âœ… ì…€ë ˆë‹ˆì›€ ì˜µì…˜ ì„¤ì •\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument('--headless')  # ì°½ ì—†ì´ ì‹¤í–‰\n",
    "    options.add_argument('--no-sandbox')\n",
    "    options.add_argument('--disable-dev-shm-usage')\n",
    "\n",
    "    # âœ… ë“œë¼ì´ë²„ ì‹¤í–‰\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "    driver.get(url)\n",
    "\n",
    "    # âœ… ëª…ì‹œì  ëŒ€ê¸°: ë³¸ë¬¸ì´ ë¡œë”©ë  ë•Œê¹Œì§€ ìµœëŒ€ 10ì´ˆ ëŒ€ê¸°\n",
    "    try:\n",
    "        WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.ID, \"articleBodyContent\"))\n",
    "        )\n",
    "    except:\n",
    "        print(\"â° ë¡œë”© ì‹¤íŒ¨ ë˜ëŠ” íƒ€ì„ì•„ì›ƒ ë°œìƒ\")\n",
    "        driver.quit()\n",
    "        return None\n",
    "\n",
    "    # âœ… í˜ì´ì§€ íŒŒì‹±\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    driver.quit()\n",
    "\n",
    "    # âœ… ì œëª© ì¶”ì¶œ\n",
    "    title_tag = soup.select_one('h1.article-h2-header-title.mb-40')\n",
    "    title = title_tag.get_text(strip=True) if title_tag else \"ì œëª© ì—†ìŒ\"\n",
    "\n",
    "    # âœ… ë³¸ë¬¸ ì¶”ì¶œ\n",
    "    body_tag = soup.find('div', id='articleBodyContent')\n",
    "    if body_tag:\n",
    "        paragraphs = body_tag.find_all('p')\n",
    "        body_lines = [p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True)]\n",
    "        body = '\\n'.join(body_lines)\n",
    "    else:\n",
    "        body = \"ë³¸ë¬¸ ì—†ìŒ\"\n",
    "\n",
    "    return {\n",
    "        \"title\": title,\n",
    "        \"url\": url,\n",
    "        \"body\": body,\n",
    "        \"media\": \"ë‰´ìŠ¤1\"\n",
    "    }\n",
    "    \n",
    "# âœ… ë¨¸ë‹ˆíˆ¬ë°ì´ ë‰´ìŠ¤ í¬ë¡¤ë§ í•¨ìˆ˜\n",
    "def crawl_mt_news(url):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0'\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        print(\"âŒ ê¸°ì‚¬ ìš”ì²­ ì‹¤íŒ¨:\", response.status_code)\n",
    "        return None\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # ì œëª© ì¶”ì¶œ\n",
    "    title_tag = soup.find('h1', class_=['subject'])\n",
    "    title = title_tag.get_text(strip=True) if title_tag else \"ì œëª© ì—†ìŒ\"\n",
    "\n",
    "    # ë³¸ë¬¸ ì¶”ì¶œ (p íƒœê·¸ ì¤‘ì—ì„œ ì €ì‘ê¶Œ ì •ë³´ ì œì™¸)\n",
    "    # 'view_text' í´ë˜ìŠ¤ë¥¼ ê°€ì§„ divë¥¼ ì°¾ìŒ (ë³¸ë¬¸ ì „ì²´ ì˜ì—­)\n",
    "    body_tag = soup.find('div', class_='view_text')\n",
    "\n",
    "    if body_tag:\n",
    "    # ë³¸ë¬¸ í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ (ì¤„ë°”ê¿ˆ ê¸°ì¤€ ë¶„ë¦¬)\n",
    "        raw_text = body_tag.get_text(separator=\"\\n\").strip()\n",
    "\n",
    "    # ê° ì¤„ì˜ ê³µë°± ì œê±° ë° ë¹ˆ ì¤„ ì œê±°\n",
    "        body_lines = [line.strip() for line in raw_text.split(\"\\n\") if line.strip()]\n",
    "\n",
    "    # ë‹¤ì‹œ ì¤„ë°”ê¿ˆ ë¬¸ìë¡œ í•©ì¹¨\n",
    "        body = '\\n'.join(body_lines)\n",
    "    else:\n",
    "        body = \"ë³¸ë¬¸ ì—†ìŒ\"\n",
    "\n",
    "    return {\n",
    "        \"title\": title,\n",
    "        \"url\": url,\n",
    "        \"body\": body,\n",
    "        \"media\": \"ë¨¸ë‹ˆíˆ¬ë°ì´\"\n",
    "    }\n",
    "\n",
    "# âœ… í—¤ëŸ´ë“œê²½ì œ í¬ë¡¤ë§ í•¨ìˆ˜\n",
    "def crawl_heraldcorp_news(url):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0'\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        print(\"âŒ ê¸°ì‚¬ ìš”ì²­ ì‹¤íŒ¨:\", response.status_code)\n",
    "        return None\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # ì œëª© ì¶”ì¶œ\n",
    "    title = soup.select_one('div.news_title > h1')\n",
    "    title = title.get_text(strip=True) if title else \"ì œëª© ì—†ìŒ\"\n",
    "\n",
    "    # ë³¸ë¬¸ ì¶”ì¶œ (p íƒœê·¸ ì¤‘ì—ì„œ ì €ì‘ê¶Œ ì •ë³´ ì œì™¸)\n",
    "    # 'view_text' í´ë˜ìŠ¤ë¥¼ ê°€ì§„ divë¥¼ ì°¾ìŒ (ë³¸ë¬¸ ì „ì²´ ì˜ì—­)\n",
    "    body_tag = soup.find('article', id='articleText')\n",
    "\n",
    "    if body_tag:\n",
    "        # article ì•ˆì˜ ëª¨ë“  p íƒœê·¸ í…ìŠ¤íŠ¸ë¥¼ ì¤„ë°”ê¿ˆ ê¸°ì¤€ìœ¼ë¡œ í•©ì¹¨\n",
    "        raw_text = \"\\n\".join([p.get_text(strip=True) for p in body_tag.find_all('p')]).strip()\n",
    "\n",
    "        # ê° ì¤„ì˜ ê³µë°± ì œê±° ë° ë¹ˆ ì¤„ ì œê±°\n",
    "        body_lines = [line.strip() for line in raw_text.split(\"\\n\") if line.strip()]\n",
    "\n",
    "        # ë‹¤ì‹œ ì¤„ë°”ê¿ˆ ë¬¸ìë¡œ í•©ì¹¨\n",
    "        body = '\\n'.join(body_lines)\n",
    "    else:\n",
    "        body = \"ë³¸ë¬¸ ì—†ìŒ\"\n",
    "\n",
    "    return {\n",
    "        \"title\": title,\n",
    "        \"url\": url,\n",
    "        \"body\": body,\n",
    "        \"media\": \"í—¤ëŸ´ë“œê²½ì œ\"\n",
    "    }\n",
    "\n",
    "\n",
    "# âœ… ì„œìš¸ê²½ì œ í¬ë¡¤ë§ í•¨ìˆ˜\n",
    "def crawl_sedaily_news(url):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0'\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        print(\"âŒ ê¸°ì‚¬ ìš”ì²­ ì‹¤íŒ¨:\", response.status_code)\n",
    "        return None\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # ì œëª© ì¶”ì¶œ\n",
    "    title_tag = soup.find('h1', class_=['art_tit'])\n",
    "    title = title_tag.get_text(strip=True) if title_tag else \"ì œëª© ì—†ìŒ\"\n",
    "\n",
    "    # ë³¸ë¬¸ ì¶”ì¶œ (p íƒœê·¸ ì¤‘ì—ì„œ ì €ì‘ê¶Œ ì •ë³´ ì œì™¸)\n",
    "    # 'view_text' í´ë˜ìŠ¤ë¥¼ ê°€ì§„ divë¥¼ ì°¾ìŒ (ë³¸ë¬¸ ì „ì²´ ì˜ì—­)\n",
    "    body_tag = soup.find('div', class_='article_view')\n",
    "\n",
    "    if body_tag:\n",
    "    # figure íƒœê·¸ ì œê±° (caption í¬í•¨)\n",
    "        for fig in body_tag.find_all('figure', class_='art_photo'):\n",
    "            fig.decompose()  # í•´ë‹¹ íƒœê·¸ ë° í•˜ìœ„ ë‚´ìš© ì™„ì „ ì‚­ì œ\n",
    "\n",
    "    # <br> íƒœê·¸ë¥¼ '\\n'ìœ¼ë¡œ ë³€í™˜\n",
    "        for br in body_tag.find_all('br'):\n",
    "            br.replace_with('\\n')\n",
    "\n",
    "    # í…ìŠ¤íŠ¸ ì¶”ì¶œ í›„ strip\n",
    "        raw_text = body_tag.get_text(separator=\"\\n\").strip()\n",
    "\n",
    "    # ê³µë°± ì¤„ ì œê±°\n",
    "        body_lines = [line.strip() for line in raw_text.split(\"\\n\") if line.strip()]\n",
    "\n",
    "    # ë‹¤ì‹œ ì¤„ë°”ê¿ˆìœ¼ë¡œ í•©ì¹¨\n",
    "        body = '\\n'.join(body_lines)\n",
    "    else:\n",
    "        body = \"ë³¸ë¬¸ ì—†ìŒ\"\n",
    "\n",
    "    return {\n",
    "        \"title\": title,\n",
    "        \"url\": url,\n",
    "        \"body\": body,\n",
    "        \"media\": \"ì„œìš¸ê²½ì œ\"\n",
    "    }\n",
    "\n",
    "# âœ… ë‰´ìŠ¤í•Œ í¬ë¡¤ë§ í•¨ìˆ˜\n",
    "def crawl_newspim_news(url):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0'\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        print(\"âŒ ê¸°ì‚¬ ìš”ì²­ ì‹¤íŒ¨:\", response.status_code)\n",
    "        return None\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # ì œëª© ì¶”ì¶œ\n",
    "    title_tag = soup.find('h2')\n",
    "    title = title_tag.get_text(strip=True) if title_tag else \"ì œëª© ì—†ìŒ\"\n",
    "\n",
    "    body_tag = soup.find('div', class_='contents', itemprop='articleBody')\n",
    "\n",
    "    if body_tag:\n",
    "        # articleBody ë‚´ ëª¨ë“  p íƒœê·¸ í…ìŠ¤íŠ¸ë¥¼ ì¤„ë°”ê¿ˆ ê¸°ì¤€ìœ¼ë¡œ í•©ì¹¨\n",
    "        raw_text = \"\\n\".join([p.get_text(strip=True) for p in body_tag.find_all('p')]).strip()\n",
    "\n",
    "        # ê° ì¤„ ê³µë°± ì œê±° ë° ë¹ˆ ì¤„ ì œê±°\n",
    "        body_lines = [line.strip() for line in raw_text.split(\"\\n\") if line.strip()]\n",
    "\n",
    "        body = ''.join(body_lines)\n",
    "\n",
    "    else:\n",
    "        body = \"ë³¸ë¬¸ ì—†ìŒ\"\n",
    "\n",
    "    return {\n",
    "        \"title\": title,\n",
    "        \"url\": url,\n",
    "        \"body\": body,\n",
    "        \"media\": \"ë‰´ìŠ¤í•Œ\"\n",
    "    }\n",
    "\n",
    "\n",
    "# âœ… ë°ì¼ë¦¬ì•ˆ í¬ë¡¤ë§ í•¨ìˆ˜\n",
    "def crawl_dailian_news(url):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0'\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        print(\"âŒ ê¸°ì‚¬ ìš”ì²­ ì‹¤íŒ¨:\", response.status_code)\n",
    "        return None\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # ì œëª© ì¶”ì¶œ\n",
    "    title_tag = soup.find('h1', class_=['title'])\n",
    "    title = title_tag.get_text(strip=True) if title_tag else \"ì œëª© ì—†ìŒ\"\n",
    "\n",
    "    body_tag = soup.find('div', class_='article')\n",
    "\n",
    "    if body_tag:\n",
    "        # articleBody ë‚´ ëª¨ë“  p íƒœê·¸ í…ìŠ¤íŠ¸ë¥¼ ì¤„ë°”ê¿ˆ ê¸°ì¤€ìœ¼ë¡œ í•©ì¹¨\n",
    "        raw_text = \"\\n\".join([p.get_text(strip=True) for p in body_tag.find_all('p')]).strip()\n",
    "\n",
    "        # ê° ì¤„ ê³µë°± ì œê±° ë° ë¹ˆ ì¤„ ì œê±°\n",
    "        body_lines = [line.strip() for line in raw_text.split(\"\\n\") if line.strip()]\n",
    "\n",
    "        body = ''.join(body_lines)\n",
    "\n",
    "    else:\n",
    "        body = \"ë³¸ë¬¸ ì—†ìŒ\"\n",
    "\n",
    "    return {\n",
    "        \"title\": title,\n",
    "        \"url\": url,\n",
    "        \"body\": body,\n",
    "        \"media\": \"ë°ì¼ë¦¬ì•ˆ\"\n",
    "    }\n",
    "\n",
    "# âœ… ë§¤ì¼ê²½ì œ í¬ë¡¤ë§ í•¨ìˆ˜\n",
    "def crawl_mk_news(url):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0'\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        print(\"âŒ ê¸°ì‚¬ ìš”ì²­ ì‹¤íŒ¨:\", response.status_code)\n",
    "        return None\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # ì œëª© ì¶”ì¶œ\n",
    "    title_tag = soup.find('h2', class_=['news_ttl'])\n",
    "    title = title_tag.get_text(strip=True) if title_tag else \"ì œëª© ì—†ìŒ\"\n",
    "    \n",
    "    body_tag = soup.find('div', class_='news_cnt_detail_wrap', itemprop='articleBody')\n",
    "\n",
    "    if body_tag:\n",
    "        p_texts = [p.get_text(strip=True) for p in body_tag.find_all('p')]\n",
    "        p_texts = [text for text in p_texts if text]\n",
    "\n",
    "        if p_texts:\n",
    "            body = ''.join(p_texts)\n",
    "        else:\n",
    "            # p íƒœê·¸ ì—†ê±°ë‚˜ ë¹ˆ ê²½ìš° br ê¸°ì¤€ í…ìŠ¤íŠ¸ ë…¸ë“œ ì¶”ì¶œ\n",
    "            br_texts = [str(t).strip() for t in body_tag.children if t and str(t).strip() and not getattr(t, 'name', None)]\n",
    "            br_texts = [text for text in br_texts if text]\n",
    "            if br_texts:\n",
    "                body = ''.join(br_texts)\n",
    "            else:\n",
    "                body = \"ë³¸ë¬¸ ì—†ìŒ\"\n",
    "    else:\n",
    "        body = \"ë³¸ë¬¸ ì—†ìŒ\"\n",
    "        \n",
    "    return {\n",
    "        \"title\": title,\n",
    "        \"url\": url,\n",
    "        \"body\": body,\n",
    "        \"media\": \"ë§¤ì¼ê²½ì œ\"\n",
    "    }\n",
    "\n",
    "# âœ… ì•„ì‹œì•„ê²½ì œ í¬ë¡¤ë§ í•¨ìˆ˜\n",
    "def crawl_asiae_news(url):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0'\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        print(\"âŒ ê¸°ì‚¬ ìš”ì²­ ì‹¤íŒ¨:\", response.status_code)\n",
    "        return None\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # ì œëª© ì¶”ì¶œ\n",
    "    title_tag = soup.find('h1')  # ê·¸ëƒ¥ ì²« ë²ˆì§¸ h1 íƒœê·¸ ì°¾ê¸°\n",
    "    title = title_tag.get_text(strip=True) if title_tag else \"ì œëª© ì—†ìŒ\"\n",
    "    \n",
    "    body_tag = soup.find('div', class_='article')\n",
    "\n",
    "    if body_tag:\n",
    "        # classê°€ txt_prohibitionì¸ p íƒœê·¸ëŠ” ì œì™¸í•˜ê³  í…ìŠ¤íŠ¸ ì¶”ì¶œ\n",
    "        p_tags = [p for p in body_tag.find_all('p') if 'txt_prohibition' not in p.get('class', [])]\n",
    "\n",
    "        raw_text = \"\\n\".join([p.get_text(strip=True) for p in p_tags]).strip()\n",
    "\n",
    "        # ê° ì¤„ ê³µë°± ì œê±° ë° ë¹ˆ ì¤„ ì œê±°\n",
    "        body_lines = [line.strip() for line in raw_text.split(\"\\n\") if line.strip()]\n",
    "\n",
    "        body = ''.join(body_lines)\n",
    "    else:\n",
    "        body = \"ë³¸ë¬¸ ì—†ìŒ\"\n",
    "\n",
    "\n",
    "    return {\n",
    "        \"title\": title,\n",
    "        \"url\": url,\n",
    "        \"body\": body,\n",
    "        \"media\": \"ì•„ì‹œì•„ê²½ì œ\"\n",
    "    }\n",
    "\n",
    "\n",
    "# âœ… Supabase ì €ì¥ í•¨ìˆ˜\n",
    "def save_to_supabase(data):\n",
    "    try:\n",
    "        existing = supabase.table(\"articles\").select(\"id\").eq(\"url\", data[\"url\"]).execute()\n",
    "        if existing.data:\n",
    "            print(\"âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬:\", data[\"url\"])\n",
    "            return False\n",
    "        supabase.table(\"articles\").insert([data]).execute()\n",
    "        print(\"âœ… ì €ì¥ ì™„ë£Œ:\", data['title'])\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(\"âŒ ì €ì¥ ì‹¤íŒ¨:\", e)\n",
    "        return False\n",
    "\n",
    "# âœ… ì–¸ë¡ ì‚¬ ë„ë©”ì¸ â†’ í¬ë¡¤ë§ í•¨ìˆ˜ ë§¤í•‘\n",
    "CRAWLER_FUNCTION_MAP = {\n",
    "    \"www.yna.co.kr\": crawl_yonhap_news,\n",
    "    \"www.chosun.com\": crawl_chosun_news,\n",
    "    \"www.newsis.com\": crawl_newsis_news,\n",
    "    \"www.news1.kr\" : crawl_news1_news,\n",
    "    \"news.mt.co.kr\" : crawl_mt_news,\n",
    "    \"biz.heraldcorp.com\" : crawl_heraldcorp_news,\n",
    "    \"www.sedaily.com\" : crawl_sedaily_news,\n",
    "    \"www.newspim.com\" : crawl_newspim_news,\n",
    "    \"www.dailian.co.kr\" : crawl_dailian_news,\n",
    "    \"www.mk.co.kr\" : crawl_mk_news,\n",
    "    \"view.asiae.co.kr\" : crawl_asiae_news,\n",
    "    \"www.khan.co.kr\" : crawl_khan_news,\n",
    "    \n",
    "    \n",
    "}\n",
    "\n",
    "# âœ… ì–¸ë¡ ì‚¬ ë„ë©”ì¸ â†’ ì–¸ë¡ ì‚¬ ì´ë¦„ ë§¤í•‘\n",
    "MEDIA_NAME_MAP = {\n",
    "    \"www.yna.co.kr\": \"ì—°í•©ë‰´ìŠ¤\",\n",
    "    \"www.chosun.com\": \"ì¡°ì„ ì¼ë³´\",\n",
    "    \"www.newsis.com\": \"ë‰´ì‹œìŠ¤\",\n",
    "    \"www.news1.kr\" : \"ë‰´ìŠ¤1\",\n",
    "    \"news.mt.co.kr\" : \"ë¨¸ë‹ˆíˆ¬ë°ì´\",\n",
    "    \"biz.heraldcorp.com\" : \"í—¤ëŸ´ë“œê²½ì œ\",\n",
    "    \"www.sedaily.com\" : \"ì„œìš¸ê²½ì œ\",\n",
    "    \"www.newspim.com\" : \"ë‰´ìŠ¤í•Œ\",\n",
    "    \"www.dailian.co.kr\" : \"ë°ì¼ë¦¬ì•ˆ\",\n",
    "    \"www.mk.co.kr\" : \"ë§¤ì¼ê²½ì œ\",\n",
    "    \"view.asiae.co.kr\" : \"ì•„ì‹œì•„ê²½ì œ\",\n",
    "    \"www.khan.co.kr\" : \"ê²½í–¥ì‹ ë¬¸\",\n",
    "    \n",
    "    \n",
    "}\n",
    "\n",
    "# âœ… ë„¤ì´ë²„ ë‰´ìŠ¤ ìˆ˜ì§‘ ë° ì €ì¥\n",
    "def save_articles_from_naver(query):\n",
    "    client_id = \"_TznE38btYhyzWYsq9XK\"\n",
    "    client_secret = \"06UYVlSHF9\"\n",
    "\n",
    "    encoded_query = urllib.parse.quote(query)\n",
    "    headers = {\n",
    "        \"X-Naver-Client-Id\": client_id,\n",
    "        \"X-Naver-Client-Secret\": client_secret\n",
    "    }\n",
    "\n",
    "    display = 100\n",
    "    saved_count_by_domain = {domain: 0 for domain in CRAWLER_FUNCTION_MAP.keys()}\n",
    "\n",
    "    for start in range(1, 1000 + 1, display):\n",
    "        url = f\"https://openapi.naver.com/v1/search/news.json?query={encoded_query}&display={display}&start={start}&sort=date\"\n",
    "        response = requests.get(url, headers=headers)\n",
    "\n",
    "        if response.status_code != 200:\n",
    "            print(f\"âŒ ìš”ì²­ ì‹¤íŒ¨ at start={start}: {response.status_code}\")\n",
    "            continue\n",
    "\n",
    "        data = response.json()\n",
    "        items = data.get(\"items\", [])\n",
    "        if not items:\n",
    "            break\n",
    "\n",
    "        for item in items:\n",
    "            originallink = item.get(\"originallink\", \"\")\n",
    "            domain = urlparse(originallink).netloc\n",
    "\n",
    "            if domain in CRAWLER_FUNCTION_MAP:\n",
    "                article = CRAWLER_FUNCTION_MAP[domain](originallink)\n",
    "                if article:\n",
    "                    success = save_to_supabase(article)\n",
    "                    if success:\n",
    "                        saved_count_by_domain[domain] += 1\n",
    "\n",
    "        if len(items) < display:\n",
    "            break\n",
    "\n",
    "    # âœ… ìµœì¢… ê²°ê³¼ ì¶œë ¥\n",
    "    print(\"\\nâœ… ì €ì¥ ìš”ì•½\")\n",
    "    for domain, count in saved_count_by_domain.items():\n",
    "        media = MEDIA_NAME_MAP.get(domain, domain)\n",
    "        print(f\"ğŸ“° {media} ê¸°ì‚¬ ì´ {count}ê±´ Supabaseì— ì €ì¥ ì™„ë£Œ\")\n",
    "\n",
    "# âœ… ì‹¤í–‰\n",
    "save_articles_from_naver(\"ë”¥í˜ì´í¬\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87579bb",
   "metadata": {},
   "source": [
    "## **í¬ë¡¤ëŸ¬ ê²½ëŸ‰í™”**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b86729e",
   "metadata": {},
   "source": [
    "ë™ì  í˜ì´ì§€ ì–¸ë¡ ì‚¬ë§Œ ë¹¼ê³  ì œì¼ ë§ì´ ê²€ìƒ‰ë˜ëŠ” ì–¸ë¡ ì‚¬ ëª‡ê°œë§Œ ë„£ì–´ë³´ê¸°."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334453cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ì €ì¥ ì™„ë£Œ: ë‘ë‚˜ë¬´, ì›¹3 ë³´ì•ˆ ì¸ì¬ ì–‘ì„± ì—…ì‚¬ì´ë“œ ì•„ì¹´ë°ë¯¸ 3ê¸° ì°¸ê°€ì ëª¨ì§‘\n",
      "âœ… ì €ì¥ ì™„ë£Œ: [ì§€ë°©ì˜íšŒ, ë‚´ê°€ ë›´ë‹¤] í•˜ë‚¨ì‹œì˜íšŒ ì˜¤ì§€ì—° ì˜ì›\n",
      "âœ… ì €ì¥ ì™„ë£Œ: ì„±ë‚¨ì‹œ, ì‚¬ì´ë²„ëŒ€í•™ 4ê³³ê³¼ ìˆ˜ì—…ë£Œ ê°ë©´ í˜‘ì•½\n",
      "âœ… ì €ì¥ ì™„ë£Œ: ë§¥ë¶ì— ì§€ì›ê¸ˆê¹Œì§€Â·Â·Â·ë‘ë‚˜ë¬´ ë³´ì•ˆ ì¸ì¬ ìœ¡ì„± í”„ë¡œê·¸ë¨ ì°¸ê°€ì ëª¨ì§‘\n",
      "âœ… ì €ì¥ ì™„ë£Œ: 'ë§ˆíŠ¸ ëŒ€ëª»' 13ë…„ì§¸â€¦ ì „í†µì‹œì¥ ìƒì¸ë„ \"ë§ˆíŠ¸ íœ´ì—…ì¼ ì˜ë¯¸ ì—†ë‹¤\"\n",
      "âœ… ì €ì¥ ì™„ë£Œ: ë™êµ­ì œê°• ì²« ì…§ë‹¤ìš´â€¦ìµœì•… ê²½ê¸°ë¶€ì§„ì— ì „ê¸°ë£Œê¹Œì§€ ê¸‰ë“±[Pickì½”ë…¸ë¯¸]\n",
      "âœ… ì €ì¥ ì™„ë£Œ: ì‹œì¥ë§ˆì € ë¬´ë„ˆëœ¨ë¦° 'ë§ˆíŠ¸ ëŒ€ëª»' 13ë…„\n",
      "âœ… ì €ì¥ ì™„ë£Œ: â€œë³´ì•ˆ ì¸ì¬ ìœ¡ì„±â€ ë‘ë‚˜ë¬´ â€˜ì—…ì‚¬ì´ë“œ ì•„ì¹´ë°ë¯¸â€™ 3ê¸° ëª¨ì§‘\n",
      "âœ… ì €ì¥ ì™„ë£Œ: ì‹œì¥ ìƒì¸ \"ë‘ë¶€ í•œëª¨ë„ ë°°ë‹¬ì‹œí‚¤ëŠ”ë°â€¦ë§ˆíŠ¸ ê·œì œ ì˜ë¯¸ ì—†ì–´\"\n",
      "âœ… ì €ì¥ ì™„ë£Œ: ìµœì•… ê²½ê¸°ë¶€ì§„ì— ì „ê¸°ë£Œê¹Œì§€ ê¸‰ë“±â€¦\"ì¶”ê²½ ì§‘í–‰ì´ë¼ë„ ì„œë‘˜ëŸ¬ì•¼\"\n",
      "âœ… ì €ì¥ ì™„ë£Œ: êµ­íšŒì„œ â€˜ìƒì„±í˜• AIì™€ ë‰´ìŠ¤ ì €ì‘ê¶Œâ€™ ê°„ë‹´íšŒ 28ì¼ ì—´ë¦°ë‹¤\n",
      "âœ… ì €ì¥ ì™„ë£Œ: ì œëª© ì—†ìŒ\n",
      "âœ… ì €ì¥ ì™„ë£Œ: í•œêµ­ê³µì¸íšŒê³„ì‚¬íšŒ, ì œ18íšŒ ì§€ì†ê°€ëŠ¥ì„±ì¸ì¦í¬ëŸ¼ ê°œìµœ\n",
      "âœ… ì €ì¥ ì™„ë£Œ: ì˜¤ì„í™˜ ì°¨ê´€, ìœ ë³´í†µí•© ìœ„í•œ ë””ì§€í„¸ ê¸°ë°˜ êµ¬ì¶• ì—…ë¬´í˜‘ì•½ ì²´ê²°\n",
      "âœ… ì €ì¥ ì™„ë£Œ: ì˜¤ì„í™˜ ì°¨ê´€, ìœ ë³´í†µí•© ìœ„í•œ ë””ì§€í„¸ ê¸°ë°˜ êµ¬ì¶• ì—…ë¬´í˜‘ì•½ ì²´ê²°\n",
      "âœ… ì €ì¥ ì™„ë£Œ: ë‘ë‚˜ë¬´, ë³´ì•ˆ ì¸ì¬ ì–‘ì„± 'ì—…ì‚¬ì´ë“œ ì•„ì¹´ë°ë¯¸' 3ê¸° ëª¨ì§‘\n",
      "âœ… ì €ì¥ ì™„ë£Œ: \n",
      "âœ… ì €ì¥ ì™„ë£Œ: ì„±ë‚¨ì‹œë¯¼ì´ë©´ ì„¸ì¢…ì‚¬ì´ë²„å¤§ ë“± 4ê³³ì„œ ìˆ˜ì—…ë£Œ ìµœëŒ€ 30% ê°ë©´\n",
      "âœ… ì €ì¥ ì™„ë£Œ: ì˜¤ì„í™˜ ì°¨ê´€, ìœ ë³´í†µí•© ìœ„í•œ ë””ì§€í„¸ ê¸°ë°˜ êµ¬ì¶• ì—…ë¬´í˜‘ì•½ ì²´ê²°\n",
      "âœ… ì €ì¥ ì™„ë£Œ: ì„¸ì¢…ëŒ€, ì• ë‹ˆë©”ì´ì…˜ ì œì‘ì‚¬ 'ìŠ¤íŠœë””ì˜¤ ë¯¸ë¥´' ê²¬í•™ í”„ë¡œê·¸ë¨ ìš´ì˜\n",
      "âœ… ì €ì¥ ì™„ë£Œ: êµìœ¡ë¶€, BK21 ìš°ì£¼ ë¶„ì•¼ 4ê°œ ì—°êµ¬ë‹¨ ì¶”ê°€ ì„ ì •â€¦ë§¤ë…„ 54ì–µ ì§€ì›\n",
      "âœ… ì €ì¥ ì™„ë£Œ: '4ë‹¨ê³„ BK21 í˜ì‹ ì¸ì¬ ì–‘ì„±ì‚¬ì—…' ìš°ì£¼ë¶„ì•¼ 4ê³³ ì¶”ê°€ ì„ ì •\n",
      "âœ… ì €ì¥ ì™„ë£Œ: [ë¶€ê³ ]ì†í•„í›ˆ(ê³ ìš©ë…¸ë™ë¶€ ê³ ìš©ì„œë¹„ìŠ¤ì •ì±…ê´€)ì”¨ ëª¨ì¹œìƒ\n",
      "âœ… ì €ì¥ ì™„ë£Œ: AIÂ·ì„¼ì„œÂ·IoTÂ·ë„¤íŠ¸ì›Œí¬Â·ì—£ì§€ ì»´í“¨íŒ…Â·ë°ì´í„° ìœµí•© í•„ìš”í•œ ì— ë¹„ì–¸íŠ¸ AI\n",
      "âœ… ì €ì¥ ì™„ë£Œ: [ë¶€ê³ ] ì†í•„í›ˆ(ê³ ìš©ë…¸ë™ë¶€ ê³ ìš©ì„œë¹„ìŠ¤ì •ì±…ê´€) ì”¨ ëª¨ì¹œìƒ\n",
      "âœ… ì €ì¥ ì™„ë£Œ: ì‹œí‚¤ì§€ ì•Šì•„ë„â€¦ì•Œì•„ì„œ ì˜ˆì¸¡í•´ í–‰ë™í•˜ëŠ” AIë¥¼ ì£¼ëª©í•˜ë¼\n",
      "âœ… ì €ì¥ ì™„ë£Œ: ì—”ë¹„ë””ì•„, KíŠ¹í—ˆ ì¹¨í•´í–ˆë‚˜? \"ìˆ˜ì²œì–µ ë¡œì—´í‹° ìˆ˜ìµ ê°€ëŠ¥\"[ì•Œì§œë°°ê¸° ì§€ì‹ì¬ì‚°]\n",
      "âœ… ì €ì¥ ì™„ë£Œ: ìš°ì£¼ ë¶„ì•¼ ì¸ì¬ í‚¤ìš´ë‹¤â€¦êµìœ¡ë¶€, ì‹ ê·œ êµìœ¡ì—°êµ¬ë‹¨ 4ê°œ ì˜ˆë¹„ ì„ ì •\n",
      "âœ… ì €ì¥ ì™„ë£Œ: ì„±ë‚¨ì‹œ, ì‚¬ì´ë²„ëŒ€ 4ê³³ê³¼ í˜‘ë ¥â€¦ì‹œë¯¼ ìˆ˜ì—…ë£Œ ìµœëŒ€ 30% ê°ë©´\n",
      "âœ… ì €ì¥ ì™„ë£Œ: êµìœ¡ë¶€, BK21 ìš°ì£¼ ë¶„ì•¼ 4ê°œ ì—°êµ¬ë‹¨ ì¶”ê°€ ì„ ì •â€¦ë§¤ë…„ 54ì–µ ì§€ì›\n",
      "âœ… ì €ì¥ ì™„ë£Œ: â€˜ìˆí¼ ìŠ¤í† ë¦¬í…”ë§â€™ ì™€ì´ë‚«ë¯¸ë””ì–´ ì´ë¯¼ì„ ëŒ€í‘œ ë°œì œâ€¦28ì¼ 8ì°¨ ì½”ì¹´í”„(KOCAF) í¬ëŸ¼ ê°œìµœ\n",
      "âœ… ì €ì¥ ì™„ë£Œ: ì „êµ­ 78ê°œ ì‹œí—˜ì¥ì„œ â€˜TOPCIT ì œ23íšŒ ì •ê¸°í‰ê°€â€™ ë™ì‹œ ì‹¤ì‹œ\n",
      "âœ… ì €ì¥ ì™„ë£Œ: ì„±ë‚¨ì‹œë¯¼ì´ë¼ë©´ 'ì‚¬ì´ë²„ëŒ€' 4ê³³ ìˆ˜ì—…ë£Œ ìµœëŒ€ 30% ê°ë©´\n",
      "âœ… ì €ì¥ ì™„ë£Œ: ì œëª© ì—†ìŒ\n",
      "âœ… ì €ì¥ ì™„ë£Œ: â€œ1500ì› ê°„ë‹¤ë©°â€â€¦ì£¼ì¶¤í•˜ëŠ” ë‹¬ëŸ¬í™”, ì§€ê¸ˆ ì‚¬ë‘ëŠ” ê²Œ ì´ë“ì¼ê¹Œ\n",
      "âœ… ì €ì¥ ì™„ë£Œ: ì†Œí”„íŠ¸ì›¨ì–´ í† ìµ â€˜TOPCITâ€™â€¦ì •ê¸°í‰ê°€ 7586ëª… ì‘ì‹œ\n",
      "âœ… ì €ì¥ ì™„ë£Œ: ì„¸ì¢…ëŒ€, 'ìƒë°˜ê¸° ì±„ìš© ë¦´ë ˆì´ íŠ¹ê°•' ìš´ì˜\n",
      "âœ… ì €ì¥ ì™„ë£Œ: ìƒˆ ì •ë¶€ ì¶œë²” ì½”ì• í„°ì§„ â€˜ì£¼í•œë¯¸êµ° ê°ì¶•ë¡ â€™â€¦í•œë¯¸ë™ë§¹ ì‹œí—˜ëŒ€\n",
      "âœ… ì €ì¥ ì™„ë£Œ: â€œê¸°ì—… ì´ìµ ê°œì„  ì—†ìœ¼ë©´ ì¦ì‹œ ë¶€ì–‘ë„ ì—†ë‹¤â€ ëŒ€ì„  í›„ë³´ ëª¨ë‘ ë†“ì¹œ ì¦ì‹œ ë¶€ì–‘ì±… [íˆ¬ì360]\n",
      "âœ… ì €ì¥ ì™„ë£Œ: [ì˜¤ëŠ˜ì˜ ì£¼ìš”ì¼ì •]ì‚¬íšŒ(5ì›”25ì¼ ì¼ìš”ì¼)\n",
      "âœ… ì €ì¥ ì™„ë£Œ: [ì˜¤ëŠ˜ì˜ ì£¼ìš”ì¼ì •]ì‚¬íšŒ(5ì›”24ì¼ í† ìš”ì¼)\n",
      "âœ… ì €ì¥ ì™„ë£Œ: ë¶ˆí™©ì— â€˜ì‹¤íƒ„â€™ ë§ˆë¥¸ë‹¤â€¦ê¸°ì—… ë³´ìœ í˜„ê¸ˆ ê¸‰ê°\n",
      "âœ… ì €ì¥ ì™„ë£Œ: ë°•ìš°ì°¬ ì„¸ì¢…ëŒ€ êµìˆ˜, ì°¨ì„¸ëŒ€ GPU ê¸°ìˆ  ê°œë°œ...ì£¼ìš” ì™¸ì‹ ë“¤ ì£¼ëª©\n",
      "âœ… ì €ì¥ ì™„ë£Œ: \"ì •ì±…Â·ì§‘í–‰ ë¶„ì‚°ë¼ ë¹„íš¨ìœ¨ì , ê¸ˆìœµê°ë… í•œ ê¸°ê´€ì— ëª¨ì•„ì•¼\"\n",
      "âœ… ì €ì¥ ì™„ë£Œ: \"ë…ë¦½ê¸°êµ¬ ì‹ ì„¤\" \"ë‚´ë¶€ì¡°ì§ ê°•í™”\"â€¦ê¸ˆìœµì†Œë¹„ìë³´í˜¸ ê¸°ëŠ¥ ë†“ê³  ê°ˆë ¤\n",
      "âœ… ì €ì¥ ì™„ë£Œ: [ì˜¤ëŠ˜ì˜ ì •ì¹˜ì¼ì •] 5ì›” 22ì¼(ëª©)\n",
      "âœ… ì €ì¥ ì™„ë£Œ: ê´‘ê³ ì£¼í˜‘íšŒ, êµ¿ë°ì´í„°ì½”í¼ë ˆì´ì…˜ê³¼ ì—…ë¬´í˜‘ì•½\n",
      "âœ… ì €ì¥ ì™„ë£Œ: ì„¸ì¢…ëŒ€ ë°•ìš°ì°¬ êµìˆ˜, ë ˆì´íŠ¸ë ˆì´ì‹± GPU ê¸°ìˆ ë¡œ êµ­ì œì  ì£¼ëª©\n",
      "âœ… ì €ì¥ ì™„ë£Œ: IB ê³ ë“±êµìœ¡ í¬ëŸ¼, 'ì¤‘Â·ê³  êµìœ¡ í†µí•© í˜‘ë ¥ ë°©ì•ˆ' ë¨¸ë¦¬ ë§ëŒ”ë‹¤\n",
      "âœ… ì €ì¥ ì™„ë£Œ: ì‚¬íšŒíƒêµ¬ ì‘ì‹œì 10ë§Œëª… ëŠ˜ì–´â€¦\"'ì‚¬íƒëŸ°', ì˜¬í•´ ì…ì‹œ ì¤‘ëŒ€ ë³€ìˆ˜\"\n",
      "âœ… ì €ì¥ ì™„ë£Œ: ì„¸ì¢…ëŒ€, '2025 ìƒë°˜ê¸° ì±„ìš© ë¦´ë ˆì´ íŠ¹ê°•' ìš´ì˜\n",
      "âŒ í¬ë¡¤ë§ ì¤‘ ì˜ˆì™¸ ë°œìƒ: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "âœ… ì €ì¥ ì™„ë£Œ: ê³¼ê¸°ì •í†µë¶€, í˜ì‹ ë„ì „í˜• R&D ê´€ë¦¬ìë“¤ê³¼ ì •ì±…ë°©í–¥ ë…¼ì˜\n",
      "âœ… ì €ì¥ ì™„ë£Œ: [ì•Œì§œë°°ê¸° ì§€ì‹ì¬ì‚°]ì €ì‘ë¬¼ ë“±ë¡ ì‚¬ìƒ ì²« ê°ì†Œâ€¦\"íˆ¬ì ìœ„ì¶•, ë¶ˆí™© ë•Œë¬¸\"\n",
      "âœ… ì €ì¥ ì™„ë£Œ: [2025 ê¸ˆìœµë¹„ì „í¬ëŸ¼-ì´ëª¨ì €ëª¨3] ëŒ€ì„  ì •êµ­ì„œ ë‹¤ì‹œ ë¶ˆë¶™ì€ 'ìƒë²• ê°œì •ì•ˆ'\n",
      "âœ… ì €ì¥ ì™„ë£Œ: ë…¼ë€ ë”›ê³  ì™„íŒâ€¦ì‚¼ì„±SDI ìœ ìƒì¦ì, ì¡°ì§ ì‹ ë¢°ë¡œ ìˆœí•­\n",
      "âœ… ì €ì¥ ì™„ë£Œ: [2025 ê¸ˆìœµë¹„ì „í¬ëŸ¼-í† ë¡ ì¢…í•©] ìƒë²• ê°œì •, ì°¬ë°˜ í† ë¡ â€¦\"ê²½ì˜ ìœ„ì¶• ë¶€ì‘ìš© vs ê¸°ì—… ì‹ ë¢° íšŒë³µ\"\n",
      "âœ… ì €ì¥ ì™„ë£Œ: ë¶ˆí™©ì— â€˜ë¹„ìƒì‹¤íƒ„â€™ ë§ˆë¥¸ë‹¤â€¦10ëŒ€ ê¸°ì—… ì¤‘ 8ê³³ ë³´ìœ í˜„ê¸ˆ ê°ì†Œ\n",
      "âœ… ì €ì¥ ì™„ë£Œ: \"ì¶”ê²©ì—ì„œ ì„ ë„í•˜ëŠ” R&Dë¡œ\"â€¦ë²”ë¶€ì²˜ R&D ì œë„ ê°œì„  ëª¨ìƒ‰\n",
      "âœ… ì €ì¥ ì™„ë£Œ: ì„¸ì¢…ì‚¬ì´ë²„ëŒ€ ë°”ë¦¬ìŠ¤íƒ€Â·ì†Œë¯ˆë¦¬ì—í•™ê³¼, ì•ˆë™ 'ì§„ë§¥ì†Œì£¼â€™ ê²¬í•™\n",
      "âœ… ì €ì¥ ì™„ë£Œ: [ì˜¤ëŠ˜ì˜ ì£¼ìš”ì¼ì •]ì •ì¹˜(5ì›”22ì¼ ëª©ìš”ì¼)\n",
      "âœ… ì €ì¥ ì™„ë£Œ: ì£¼ì œí† ë¡ í•˜ëŠ” ê¹€ëŒ€ì¢… ì„¸ì¢…ëŒ€ ê²½ì˜í•™ë¶€ êµìˆ˜\n",
      "âœ… ì €ì¥ ì™„ë£Œ: ì„œìš¸ì‹œë¦½ëŒ€, â€˜2025 ììœ¨ì£¼í–‰ ë¡œë´‡ë ˆì´ìŠ¤ 1ì°¨ ëŒ€íšŒâ€™ ëŒ€ìƒ\n",
      "âœ… ì €ì¥ ì™„ë£Œ: 2025 ê¸€ë¡œë²Œ ê¸ˆìœµë¹„ì „ í¬ëŸ¼ ì£¼ì œí† ë¡ \n",
      "âœ… ì €ì¥ ì™„ë£Œ: [2025 ê¸ˆìœµë¹„ì „í¬ëŸ¼] ì°¸ì„í•´ ì¶•í•˜í•´ ì£¼ì‹  ë¶„ë“¤\n",
      "âœ… ì €ì¥ ì™„ë£Œ: [2025 ê¸ˆìœµë¹„ì „í¬ëŸ¼-í† ë¡ ] ê¹€ëŒ€ì¢… ì„¸ì¢…ëŒ€ êµìˆ˜ \"ìƒë²• ê°œì •ì•ˆ, í•´ì™¸ íˆ¬ê¸°ìë³¸ì—ê²Œ ê²½ì˜ê¶Œ ë°˜ë‚©í•˜ëŠ” ê²ƒ\"\n",
      "âœ… ì €ì¥ ì™„ë£Œ: í”¼ì‹±? ìˆ˜ì‚¬ê¸°ê´€ ê²½ê³ ?â€¦í—·ê°ˆë¦¬ëŠ” ë¬¸ì í†µë³´ ì–´ì©Œì£ ?\n",
      "âœ… ì €ì¥ ì™„ë£Œ: æ•…ì´í¬ì¤‘ ì‘ê°€ íšŒê³ ì „ ì„¸ì¢…ë®¤ì§€ì—„ê°¤ëŸ¬ë¦¬ì„œ ì—´ë ¤\n",
      "âœ… ì €ì¥ ì™„ë£Œ: ë°°Â·ë°˜ë„ì²´Â·ë°¥ì„ ì§“ëŠ” ì‚¬ëŒë“¤\n",
      "âŒ í¬ë¡¤ë§ ì¤‘ ì˜ˆì™¸ ë°œìƒ: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "âœ… ì €ì¥ ì™„ë£Œ: ì„¸ì¢…ëŒ€, â€˜ì„œìš¸ë§ˆì´ì¹¼ë¦¬ì§€â€™ ìš´ì˜ê¸°ê´€ ì„ ì •\n",
      "âœ… ì €ì¥ ì™„ë£Œ: ì„¸ì¢…ì‚¬ì´ë²„ëŒ€, êµ­ë‚´ ì²« 'ë©”íƒ€ë²„ìŠ¤ ìº í¼ìŠ¤'â€¦êµ°ì¸Â·ê²½ì°°Â·ì†Œë°©ê´€ ì…í•™ê¸ˆ ë©´ì œ\n",
      "âœ… ì €ì¥ ì™„ë£Œ: ì„œìš¸ì‹œ, ì„œìš¸ì†Œì¬ 35ê°œ ëŒ€í•™ì— 765ì–µ ì§€ì›\n",
      "âœ… ì €ì¥ ì™„ë£Œ: ë³´ê±´ì˜ë£Œì •ë³´ê´€ë¦¬ì‚¬ íƒ„ìƒ 40ì£¼ë…„â€¦ê±´ê°•ì •ë³´ ì•ˆì „ ê´€ë¦¬ 'ì² ì €'\n",
      "âœ… ì €ì¥ ì™„ë£Œ: ì„œìš¸ì‹œ, â€˜RISEâ€™ ì‚¬ì—…ìˆ˜í–‰ 35ê°œ ëŒ€í•™ ìµœì¢… ì„ ì •â€¦â€œì§€ì—­Â·ëŒ€í•™ ë™ë°˜ ì„±ì¥â€\n",
      "âœ… ì €ì¥ ì™„ë£Œ: ë³´í›ˆë¶€, ì¬ë‚œì•ˆì „ì²´í—˜êµìœ¡ ì§„í–‰â€¦ë³¸ë¶€ ì§ì› 100ì—¬ëª… ì°¸ê°€\n",
      "âœ… ì €ì¥ ì™„ë£Œ: í•œêµ­êµ­ì œê²½ì˜í•™íšŒ ì¶˜ê³„í•™ìˆ ëŒ€íšŒ, ì˜¤ëŠ” 23ì¼ ê°œìµœâ€¦â€˜K-ê¸€ë¡œë²Œ ê²½ì˜ì˜ ê³¼ê±°, í˜„ì¬ ê·¸ë¦¬ê³  ë¯¸ë˜â€™ ì£¼ì œë¡œ\n",
      "âœ… ì €ì¥ ì™„ë£Œ: 'ì§€ë¸Œë¦¬ í™”í’' ì±—GPTê°€ ì €ì‘ê¶Œ ìœ„ë°˜?â€¦\"éŸ“ì„  ë¶€ì •ê²½ìŸë°©ì§€ë²• ìœ„ë°˜ ì†Œì§€\"\n",
      "âœ… ì €ì¥ ì™„ë£Œ: ì„œìš¸ ë¼ì´ì¦ˆ ì‚¬ì—…ì— 35ê°œ ëŒ€í•™ ì„ ì •â€¦êµ­ë¹„Â·ì‹œë¹„ 765ì–µ ì§€ì›\n",
      "âœ… ì €ì¥ ì™„ë£Œ: \n",
      "âœ… ì €ì¥ ì™„ë£Œ: ì„¸ì¢…ëŒ€, '2025 ìƒë°˜ê¸° ë©´ì ‘ ì—­ëŸ‰ ê°•í™” í”„ë¡œê·¸ë¨' ìš´ì˜\n",
      "âœ… ì €ì¥ ì™„ë£Œ: \"ì±—GPT 'ì§€ë¸Œë¦¬'í’ ì´ë¯¸ì§€, éŸ“ ë¶€ì •ê²½ìŸë°©ì§€ë²• ìœ„ë²• ê°€ëŠ¥ì„±\"\n",
      "âœ… ì €ì¥ ì™„ë£Œ: ë°œëª…ì„ í†µí•œ ì°½ì˜Â·í˜ì‹ ì˜ ì‹¤í˜„ ë³´ì—¬ì£¼ê³  ë¯¸ë˜ ì˜ˆì¸¡ê¹Œì§€\n",
      "âœ… ì €ì¥ ì™„ë£Œ: ì¬ì™¸ë™í¬ ì–¸ë¡ ì¸, êµ­ì œì‹¬í¬ì§€ì—„ì„œ ê¸€ë¡œë²Œ K-ì–¸ë¡  ë°œì „ê³¼ì œ ë…¼ì˜\n",
      "âœ… ì €ì¥ ì™„ë£Œ: [ì‚¬ê³ ] ë°ì¼ë¦¬ì•ˆ '2025 ê¸€ë¡œë²Œ ê¸ˆìœµë¹„ì „ í¬ëŸ¼', 5ì›”21ì¼ ì—¬ì˜ë„ CCMMë¹Œë”© ê°œìµœ\n",
      "âœ… ì €ì¥ ì™„ë£Œ: ì„¸ì¢…ëŒ€ ë¯¸ë˜êµìœ¡ì›, '2025 ì„œìš¸ë§ˆì´ì¹¼ë¦¬ì§€' ìš´ì˜ê¸°ê´€ ì„ ì •\n",
      "âœ… ì €ì¥ ì™„ë£Œ: â€˜ì œ6íšŒ ì„¬ì˜ ë‚ â€™ í™ë³´ëŒ€ì‚¬ì— ìœ„í•˜ì¤€, í•˜í˜„ìš°, ì•ˆì„±í›ˆ, íŠ¸ë¦¬í”Œì—ìŠ¤, ì˜¤ì„¸ë“, ì •ì§€ì„  ìœ„ì´‰\n",
      "âœ… ì €ì¥ ì™„ë£Œ: \"ì™¸êµ­ê³„ ì·¨ì—…, í˜„ì¥ì—ì„œ ê¸¸ì„ ì°¾ë‹¤\" ì„¸ì¢…ëŒ€, GTF ë°•ëŒíšŒ ì°¸ê°€\n",
      "âœ… ì €ì¥ ì™„ë£Œ: \"ì˜ì—…ì´ìµë¥  25%ë¼ë‹ˆ\" í™©ì œì£¼ ë“±ê·¹ ì‚¼ì–‘ì‹í’ˆâ€¦\"ë¶ˆë‹­ ì„±ê³µ, ìš´ ì•„ë‹ˆë‹¤\"\n",
      "âœ… ì €ì¥ ì™„ë£Œ: [ì˜¤ëŠ˜ì˜ ì •ì¹˜ì¼ì •] 5ì›” 19ì¼(ì›”)\n",
      "âœ… ì €ì¥ ì™„ë£Œ: ìš©ì¸íŠ¹ë¡€ì‹œ, 'ì œ2íšŒ ëŒ€í•œë¯¼êµ­ ëŒ€í•™ì—°ê·¹ì œ' ë³¸ì„  ì§„ì¶œ 12ê°œ ëŒ€í•™íŒ€ ìµœì¢… ì„ ë°œ\n",
      "âœ… ì €ì¥ ì™„ë£Œ: ìš©ì¸ì‹œ, 'ì œ2íšŒ ëŒ€í•œë¯¼êµ­ ëŒ€í•™ì—°ê·¹ì œ' ë³¸ì„  ì§„ì¶œíŒ€ ì„ ë°œ\n",
      "âœ… ì €ì¥ ì™„ë£Œ: í•œêµ­ì„ 'ë² ì´ì§•ì— ê°€ì¥ ê°€ê¹Œì´ ìˆëŠ” í•­ê³µëª¨í•¨'ìœ¼ë¡œ ì¸ì‹í•˜ëŠ” íŠ¸ëŸ¼í”„ í–‰ì •ë¶€\n",
      "âœ… ì €ì¥ ì™„ë£Œ: ìš©ì¸ì‹œ 'ì œ2íšŒ ëŒ€í•œë¯¼êµ­ ëŒ€í•™ì—°ê·¹ì œ' ë³¸ì„  ì§„ì¶œ 12íŒ€ í™•ì •\n",
      "âœ… ì €ì¥ ì™„ë£Œ: [ì˜¤ëŠ˜ì˜ ì£¼ìš”ì¼ì •]ì‚¬íšŒ(5ì›”19ì¼ ì›”ìš”ì¼)\n",
      "âœ… ì €ì¥ ì™„ë£Œ: [ì˜¤ëŠ˜ì˜ ì£¼ìš”ì¼ì •]ì •ì¹˜(5ì›”19ì¼ ì›”ìš”ì¼)\n",
      "âœ… ì €ì¥ ì™„ë£Œ: [ì˜¤ëŠ˜ì˜ ì£¼ìš”ì¼ì •]ì‚¬íšŒ(5ì›”18ì¼ ì¼ìš”ì¼)\n",
      "âœ… ì €ì¥ ì™„ë£Œ: ë¯¼ì£¼ ì„ ëŒ€ìœ„ í˜¸ì‚¬ì¹´ ìœ ì§€ \"ê¹€ë¬¸ìˆ˜ ë‰´ë¼ì´íŠ¸ ë§ì–¸ì€ 'ë§¤êµ­'\"\n",
      "âœ… ì €ì¥ ì™„ë£Œ: [ì˜¤ëŠ˜ì˜ ì£¼ìš”ì¼ì •]ì‚¬íšŒ(5ì›”17ì¼ í† ìš”ì¼)\n",
      "âœ… ì €ì¥ ì™„ë£Œ: ì‹œë¯¼ë‹¨ì²´ \"ë¯¼ì£¼ì£¼ì˜ ìœ„ê¸° ë¶ˆí‰ë“±ì— ìˆì–´...í•´ê²°ì±…ì€ ë³µì§€êµ­ê°€\"\n",
      "âœ… ì €ì¥ ì™„ë£Œ: ì¤‘êµ¬, 2026í•™ë…„ë„ ëŒ€í•™ë³„ ì…í•™ì„¤ëª…íšŒ ê°œìµœâ€¦17ê°œ ëŒ€í•™ ì°¸ì—¬\n",
      "âœ… ì €ì¥ ì™„ë£Œ: [ì˜¤ëŠ˜ì˜ ì£¼ìš”ì¼ì •]ì‚¬íšŒ(5ì›”16ì¼ ê¸ˆìš”ì¼)\n",
      "âœ… ì €ì¥ ì™„ë£Œ: ê°€ì¥ ë¶€ì‹¤í•œ ì‚¬ë¦½ëŒ€í•™ ì–´ë””?â€¦ì‚¬êµë ¨, 38ê°œ ëŒ€í•™ ì§„ë‹¨ í‰ê°€ ê³µê°œ\n",
      "âœ… ì €ì¥ ì™„ë£Œ: ì„¸ì¢…ëŒ€ ìœ„ì •ë¯¼ êµìˆ˜, ì„¸ë¼ë¯¹íŒ”ë ˆìŠ¤í™€ì—ì„œ ì´ˆì²­ ë…ì°½íšŒ ê°œìµœ\n",
      "âœ… ì €ì¥ ì™„ë£Œ: ì„¸ì¢…ëŒ€ í•™ìˆ ì •ë³´ì›, 'ì œ3íšŒ í•™ì •í¬ëŸ¼' ê°œìµœ\n",
      "âœ… ì €ì¥ ì™„ë£Œ: \"ë²•ì¸ í‰ê°€ ì—†ì´ ì‚¬ë¦½ëŒ€ ë°œì „ ì—†ë‹¤\"â€¦38ê°œ ëŒ€í•™ ì§„ë‹¨ ê²°ê³¼(ì¢…í•©)\n",
      "âœ… ì €ì¥ ì™„ë£Œ: \n",
      "âœ… ì €ì¥ ì™„ë£Œ: \"AI ì‹œëŒ€, ë¬´ì—‡ì´ ì‚´ì•„ë‚¨ì„ê¹Œ\" ì„¸ì¢…ëŒ€ í•™ì •í¬ëŸ¼, ì§ì—…ì˜ ë¯¸ë˜ ì¡°ë§\n",
      "âœ… ì €ì¥ ì™„ë£Œ: í•œêµ­ê³µì¸íšŒê³„ì‚¬íšŒ, ì˜¤ëŠ” 21ì¼ 'ì œ18íšŒ ì§€ì†ê°€ëŠ¥ì„±ì¸ì¦í¬ëŸ¼' ê°œìµœ\n",
      "âœ… ì €ì¥ ì™„ë£Œ: í•œêµ­ê³µì¸íšŒê³„ì‚¬íšŒ, ì˜¤ëŠ” 21ì¼ â€˜ì§€ì†ê°€ëŠ¥ì„±ì¸ì¦í¬ëŸ¼â€™ ê°œìµœ\n",
      "âœ… ì €ì¥ ì™„ë£Œ: ê¹€ë¯¼ì„, í˜¸ì‚¬ì¹´ ìœ ì§€ êµìˆ˜ì™€ ìœ ì„¸â€¦â€œê¹€ë¬¸ìˆ˜, ì™œê³¡ëœ ì—­ì‚¬ê´€â€\n",
      "âœ… ì €ì¥ ì™„ë£Œ: ì„¸ì¢…ëŒ€ ë¬¼ë¦¬ì²œë¬¸í•™ê³¼ êµìˆ˜ì§„, '2025 ë¸Œë ˆì´í¬ìŠ¤ë£¨ìƒ' ê³µë™ ìˆ˜ìƒ\n",
      "âœ… ì €ì¥ ì™„ë£Œ: í•œêµ­ì¬ë¬´ê´€ë¦¬í•™íšŒ â€œêµ­ë¯¼ì—°ê¸ˆ, ì‚¬ì—…ë³´êµ­ ì´ë£¨ì–´ì§ˆ ìˆ˜ ìˆëŠ” ë°©í–¥ìœ¼ë¡œ ì˜ê²°ê¶Œ í–‰ì‚¬í•´ì•¼â€\n",
      "âœ… ì €ì¥ ì™„ë£Œ: ì„œìš¸ ì¤‘êµ¬, 2026í•™ë…„ë„ ëŒ€í•™ë³„ ì…í•™ì„¤ëª…íšŒ ê°œìµœ\n",
      "âœ… ì €ì¥ ì™„ë£Œ: ë¯¼ì£¼, ê¹€ë¬¸ìˆ˜ì— \"5Â·18 ì •ì‹  í—Œë²•ìˆ˜ë¡ ê³µì•½í•´ì•¼\"â€¦í˜¸ì‚¬ì¹´ êµìˆ˜ ì˜ì…ë„\n",
      "âœ… ì €ì¥ ì™„ë£Œ: ì„¸ì¢…ëŒ€, 'ë°”ì´ì˜¤Â·ì œì•½ HPLC êµìœ¡ í”„ë¡œê·¸ë¨' ìš´ì˜\n",
      "âœ… ì €ì¥ ì™„ë£Œ: [ì˜¤ëŠ˜ì˜ ì£¼ìš”ì¼ì •]ì‚¬íšŒ(5ì›”15ì¼ ëª©ìš”ì¼)\n",
      "âœ… ì €ì¥ ì™„ë£Œ: ê¹€ë¯¼ì„, í˜¸ì‚¬ì¹´ ìœ ì§€ êµìˆ˜ì™€ ìœ ì„¸â€¦\"ê¹€ë¬¸ìˆ˜, ì™œê³¡ëœ ì—­ì‚¬ê´€\"\n",
      "âœ… ì €ì¥ ì™„ë£Œ: ì„¸ì¢…ëŒ€ ë¬¼ë¦¬ì²œë¬¸í•™ê³¼ êµìˆ˜ì§„, 'ë¸Œë ˆì´í¬ìŠ¤ë£¨ìƒ' ê³µë™ ìˆ˜ìƒ\n",
      "âœ… ì €ì¥ ì™„ë£Œ: ì„¸ì¢…ëŒ€, 'ì»´í“¨í„°í™œìš©ëŠ¥ë ¥ 1ê¸‰ ëŒ€ë¹„ë°˜' ìš´ì˜\n",
      "âœ… ì €ì¥ ì™„ë£Œ: ì„œìš¸ ì¤‘êµ¬ 'ëŒ€í•™ë³„ ì…í•™ì„¤ëª…íšŒ' 3ì°¨ë¡€ ê°œìµœâ€¦17ê°œ ëŒ€í•™ ì°¸ì—¬\n",
      "\n",
      "âœ… ì €ì¥ ìš”ì•½\n",
      "ğŸ“° ì—°í•©ë‰´ìŠ¤ ê¸°ì‚¬ ì´ 10ê±´ Supabase test í…Œì´ë¸”ì— ì €ì¥ ì™„ë£Œ\n",
      "ğŸ“° ë‰´ì‹œìŠ¤ ê¸°ì‚¬ ì´ 33ê±´ Supabase test í…Œì´ë¸”ì— ì €ì¥ ì™„ë£Œ\n",
      "ğŸ“° ë¨¸ë‹ˆíˆ¬ë°ì´ ê¸°ì‚¬ ì´ 8ê±´ Supabase test í…Œì´ë¸”ì— ì €ì¥ ì™„ë£Œ\n",
      "ğŸ“° í—¤ëŸ´ë“œê²½ì œ ê¸°ì‚¬ ì´ 8ê±´ Supabase test í…Œì´ë¸”ì— ì €ì¥ ì™„ë£Œ\n",
      "ğŸ“° ì„œìš¸ê²½ì œ ê¸°ì‚¬ ì´ 7ê±´ Supabase test í…Œì´ë¸”ì— ì €ì¥ ì™„ë£Œ\n",
      "ğŸ“° ë‰´ìŠ¤í•Œ ê¸°ì‚¬ ì´ 8ê±´ Supabase test í…Œì´ë¸”ì— ì €ì¥ ì™„ë£Œ\n",
      "ğŸ“° ë°ì¼ë¦¬ì•ˆ ê¸°ì‚¬ ì´ 12ê±´ Supabase test í…Œì´ë¸”ì— ì €ì¥ ì™„ë£Œ\n",
      "ğŸ“° ë§¤ì¼ê²½ì œ ê¸°ì‚¬ ì´ 6ê±´ Supabase test í…Œì´ë¸”ì— ì €ì¥ ì™„ë£Œ\n",
      "ğŸ“° ì•„ì‹œì•„ê²½ì œ ê¸°ì‚¬ ì´ 7ê±´ Supabase test í…Œì´ë¸”ì— ì €ì¥ ì™„ë£Œ\n",
      "ğŸ“° ë…¸ì»·ë‰´ìŠ¤ ê¸°ì‚¬ ì´ 3ê±´ Supabase test í…Œì´ë¸”ì— ì €ì¥ ì™„ë£Œ\n",
      "ğŸ“° ì´ë°ì¼ë¦¬ë‰´ìŠ¤ ê¸°ì‚¬ ì´ 12ê±´ Supabase test í…Œì´ë¸”ì— ì €ì¥ ì™„ë£Œ\n",
      "ğŸ“° ê²½ì¸ì¼ë³´ ê¸°ì‚¬ ì´ 1ê±´ Supabase test í…Œì´ë¸”ì— ì €ì¥ ì™„ë£Œ\n",
      "ğŸ“° ì„œìš¸ì‹ ë¬¸ ê¸°ì‚¬ ì´ 4ê±´ Supabase test í…Œì´ë¸”ì— ì €ì¥ ì™„ë£Œ\n",
      "\n",
      "âœ… ì €ì¥ ìš”ì•½ì„ 'ì„¸ì¢…ëŒ€_news_save_summary.txt' íŒŒì¼ë¡œ ì €ì¥í–ˆìŠµë‹ˆë‹¤.\n",
      "ì´ ì‹¤í–‰ ì‹œê°„: 22.60ì´ˆ\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import urllib.parse\n",
    "from urllib.parse import urlparse\n",
    "from bs4 import BeautifulSoup\n",
    "from supabase import create_client, Client\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import time\n",
    "\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "# âœ… Supabase ì„¤ì •\n",
    "SUPABASE_URL = \"https://ypyujiaoeaqykbqetjef.supabase.co\"\n",
    "SUPABASE_KEY = \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6InlweXVqaWFvZWFxeWticWV0amVmIiwicm9sZSI6ImFub24iLCJpYXQiOjE3NDY1NDUyNTQsImV4cCI6MjA2MjEyMTI1NH0.RuR9l89gxCcMkSzO053EHluQ0ers-piN4SUjZ-LtWjU\"\n",
    "supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)\n",
    "\n",
    "\n",
    "## -----------------ì–¸ë¡ ì‚¬ í¬ë¡¤ëŸ¬ ì½”ë“œ ì‹œì‘!!!! ì—¬ê¸°ë¶€í„° ì•ˆ ê±´ë“œë ¤ë„ ë¼ìš”--------------------\n",
    "def crawl_yonhap_news(url):\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        return None\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    title_tag = soup.find('h1', class_='tit01')\n",
    "    title = title_tag.get_text(strip=True) if title_tag else \"ì œëª© ì—†ìŒ\"\n",
    "\n",
    "    body_tag = soup.find('div', class_='story-news article')\n",
    "    if body_tag:\n",
    "        paragraphs = body_tag.find_all('p')\n",
    "        body_lines = []\n",
    "        for p in paragraphs:\n",
    "            if 'txt-copyright' in p.get('class', []): continue\n",
    "            text = p.get_text(strip=True)\n",
    "            if text:\n",
    "                body_lines.append(text)\n",
    "        body = '\\n'.join(body_lines)\n",
    "    else:\n",
    "        body = \"ë³¸ë¬¸ ì—†ìŒ\"\n",
    "\n",
    "    return {\"title\": title, \"url\": url, \"body\": body, \"media\": \"ì—°í•©ë‰´ìŠ¤\"}\n",
    "\n",
    "\n",
    "def crawl_newsis_news(url):\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        return None\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    title_tag = soup.select_one('h1.tit.title_area')\n",
    "    title = title_tag.get_text(strip=True) if title_tag else \"ì œëª© ì—†ìŒ\"\n",
    "\n",
    "    body_tag = soup.find('div', class_='viewer')\n",
    "    if body_tag:\n",
    "        raw_text = body_tag.get_text(separator=\"\\n\").strip()\n",
    "        body_lines = [line.strip() for line in raw_text.split(\"\\n\") if line.strip()]\n",
    "        body = '\\n'.join(body_lines)\n",
    "    else:\n",
    "        body = \"ë³¸ë¬¸ ì—†ìŒ\"\n",
    "\n",
    "    return {\"title\": title, \"url\": url, \"body\": body, \"media\": \"ë‰´ì‹œìŠ¤\"}\n",
    "\n",
    "\n",
    "def crawl_mt_news(url):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0'\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        print(\"âŒ ê¸°ì‚¬ ìš”ì²­ ì‹¤íŒ¨:\", response.status_code)\n",
    "        return None\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    title_tag = soup.find('h1', class_=['subject'])\n",
    "    title = title_tag.get_text(strip=True) if title_tag else \"ì œëª© ì—†ìŒ\"\n",
    "\n",
    "    body_tag = soup.find('div', class_='view_text')\n",
    "\n",
    "    if body_tag:\n",
    "        raw_text = body_tag.get_text(separator=\"\\n\").strip()\n",
    "        body_lines = [line.strip() for line in raw_text.split(\"\\n\") if line.strip()]\n",
    "        body = '\\n'.join(body_lines)\n",
    "    else:\n",
    "        body = \"ë³¸ë¬¸ ì—†ìŒ\"\n",
    "\n",
    "    return {\n",
    "        \"title\": title,\n",
    "        \"url\": url,\n",
    "        \"body\": body,\n",
    "        \"media\": \"ë¨¸ë‹ˆíˆ¬ë°ì´\"\n",
    "    }\n",
    "\n",
    "\n",
    "def crawl_heraldcorp_news(url):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0'\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        print(\"âŒ ê¸°ì‚¬ ìš”ì²­ ì‹¤íŒ¨:\", response.status_code)\n",
    "        return None\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    title = soup.select_one('div.news_title > h1')\n",
    "    title = title.get_text(strip=True) if title else \"ì œëª© ì—†ìŒ\"\n",
    "\n",
    "    body_tag = soup.find('article', id='articleText')\n",
    "\n",
    "    if body_tag:\n",
    "        raw_text = \"\\n\".join([p.get_text(strip=True) for p in body_tag.find_all('p')]).strip()\n",
    "        body_lines = [line.strip() for line in raw_text.split(\"\\n\") if line.strip()]\n",
    "        body = '\\n'.join(body_lines)\n",
    "    else:\n",
    "        body = \"ë³¸ë¬¸ ì—†ìŒ\"\n",
    "\n",
    "    return {\n",
    "        \"title\": title,\n",
    "        \"url\": url,\n",
    "        \"body\": body,\n",
    "        \"media\": \"í—¤ëŸ´ë“œê²½ì œ\"\n",
    "    }\n",
    "\n",
    "\n",
    "def crawl_sedaily_news(url):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0'\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        print(\"âŒ ê¸°ì‚¬ ìš”ì²­ ì‹¤íŒ¨:\", response.status_code)\n",
    "        return None\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    title_tag = soup.find('h1', class_=['art_tit'])\n",
    "    title = title_tag.get_text(strip=True) if title_tag else \"ì œëª© ì—†ìŒ\"\n",
    "\n",
    "    body_tag = soup.find('div', class_='article_view')\n",
    "\n",
    "    if body_tag:\n",
    "        for fig in body_tag.find_all('figure', class_='art_photo'):\n",
    "            fig.decompose()\n",
    "\n",
    "        for br in body_tag.find_all('br'):\n",
    "            br.replace_with('\\n')\n",
    "\n",
    "        raw_text = body_tag.get_text(separator=\"\\n\").strip()\n",
    "        body_lines = [line.strip() for line in raw_text.split(\"\\n\") if line.strip()]\n",
    "        body = '\\n'.join(body_lines)\n",
    "    else:\n",
    "        body = \"ë³¸ë¬¸ ì—†ìŒ\"\n",
    "\n",
    "    return {\n",
    "        \"title\": title,\n",
    "        \"url\": url,\n",
    "        \"body\": body,\n",
    "        \"media\": \"ì„œìš¸ê²½ì œ\"\n",
    "    }\n",
    "\n",
    "\n",
    "def crawl_newspim_news(url):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0'\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        print(\"âŒ ê¸°ì‚¬ ìš”ì²­ ì‹¤íŒ¨:\", response.status_code)\n",
    "        return None\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    title_tag = soup.find('h2')\n",
    "    title = title_tag.get_text(strip=True) if title_tag else \"ì œëª© ì—†ìŒ\"\n",
    "\n",
    "    body_tag = soup.find('div', class_='contents', itemprop='articleBody')\n",
    "\n",
    "    if body_tag:\n",
    "        raw_text = \"\\n\".join([p.get_text(strip=True) for p in body_tag.find_all('p')]).strip()\n",
    "        body_lines = [line.strip() for line in raw_text.split(\"\\n\") if line.strip()]\n",
    "        body = ''.join(body_lines)\n",
    "    else:\n",
    "        body = \"ë³¸ë¬¸ ì—†ìŒ\"\n",
    "\n",
    "    return {\n",
    "        \"title\": title,\n",
    "        \"url\": url,\n",
    "        \"body\": body,\n",
    "        \"media\": \"ë‰´ìŠ¤í•Œ\"\n",
    "    }\n",
    "\n",
    "\n",
    "def crawl_dailian_news(url):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0'\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        print(\"âŒ ê¸°ì‚¬ ìš”ì²­ ì‹¤íŒ¨:\", response.status_code)\n",
    "        return None\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    title_tag = soup.find('h1', class_=['title'])\n",
    "    title = title_tag.get_text(strip=True) if title_tag else \"ì œëª© ì—†ìŒ\"\n",
    "\n",
    "    body_tag = soup.find('div', class_='article')\n",
    "\n",
    "    if body_tag:\n",
    "        raw_text = \"\\n\".join([p.get_text(strip=True) for p in body_tag.find_all('p')]).strip()\n",
    "        body_lines = [line.strip() for line in raw_text.split(\"\\n\") if line.strip()]\n",
    "        body = ''.join(body_lines)\n",
    "    else:\n",
    "        body = \"ë³¸ë¬¸ ì—†ìŒ\"\n",
    "\n",
    "    return {\n",
    "        \"title\": title,\n",
    "        \"url\": url,\n",
    "        \"body\": body,\n",
    "        \"media\": \"ë°ì¼ë¦¬ì•ˆ\"\n",
    "    }\n",
    "\n",
    "\n",
    "def crawl_mk_news(url):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0'\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        print(\"âŒ ê¸°ì‚¬ ìš”ì²­ ì‹¤íŒ¨:\", response.status_code)\n",
    "        return None\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    title_tag = soup.find('h2', class_=['news_ttl'])\n",
    "    title = title_tag.get_text(strip=True) if title_tag else \"ì œëª© ì—†ìŒ\"\n",
    "    \n",
    "    body_tag = soup.find('div', class_='news_cnt_detail_wrap', itemprop='articleBody')\n",
    "\n",
    "    if body_tag:\n",
    "        p_texts = [p.get_text(strip=True) for p in body_tag.find_all('p')]\n",
    "        p_texts = [text for text in p_texts if text]\n",
    "\n",
    "        if p_texts:\n",
    "            body = ''.join(p_texts)\n",
    "        else:\n",
    "            br_texts = [str(t).strip() for t in body_tag.children if t and str(t).strip() and not getattr(t, 'name', None)]\n",
    "            br_texts = [text for text in br_texts if text]\n",
    "            if br_texts:\n",
    "                body = ''.join(br_texts)\n",
    "            else:\n",
    "                body = \"ë³¸ë¬¸ ì—†ìŒ\"\n",
    "    else:\n",
    "        body = \"ë³¸ë¬¸ ì—†ìŒ\"\n",
    "        \n",
    "    return {\n",
    "        \"title\": title,\n",
    "        \"url\": url,\n",
    "        \"body\": body,\n",
    "        \"media\": \"ë§¤ì¼ê²½ì œ\"\n",
    "    }\n",
    "\n",
    "\n",
    "def crawl_asiae_news(url):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0'\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        print(\"âŒ ê¸°ì‚¬ ìš”ì²­ ì‹¤íŒ¨:\", response.status_code)\n",
    "        return None\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    title_tag = soup.find('h1')\n",
    "    title = title_tag.get_text(strip=True) if title_tag else \"ì œëª© ì—†ìŒ\"\n",
    "    \n",
    "    body_tag = soup.find('div', class_='article')\n",
    "\n",
    "    if body_tag:\n",
    "        p_tags = [p for p in body_tag.find_all('p') if 'txt_prohibition' not in p.get('class', [])]\n",
    "\n",
    "        raw_text = \"\\n\".join([p.get_text(strip=True) for p in p_tags]).strip()\n",
    "\n",
    "        body_lines = [line.strip() for line in raw_text.split(\"\\n\") if line.strip()]\n",
    "\n",
    "        body = ''.join(body_lines)\n",
    "    else:\n",
    "        body = \"ë³¸ë¬¸ ì—†ìŒ\"\n",
    "\n",
    "    return {\n",
    "        \"title\": title,\n",
    "        \"url\": url,\n",
    "        \"body\": body,\n",
    "        \"media\": \"ì•„ì‹œì•„ê²½ì œ\"\n",
    "    }\n",
    "    \n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from supabase import create_client, Client\n",
    "\n",
    "#Supabase ì„¤ì •\n",
    "SUPABASE_URL = \"https://ypyujiaoeaqykbqetjef.supabase.co\"\n",
    "SUPABASE_KEY = \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6InlweXVqaWFvZWFxeWticWV0amVmIiwicm9sZSI6ImFub24iLCJpYXQiOjE3NDY1NDUyNTQsImV4cCI6MjA2MjEyMTI1NH0.RuR9l89gxCcMkSzO053EHluQ0ers-piN4SUjZ-LtWjU\"\n",
    "supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)\n",
    "\n",
    "#ë…¸ì»·ë‰´ìŠ¤ í¬ë¡¤ë§ í•¨ìˆ˜\n",
    "def crawl_nocut_news(url):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0'\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        print(\"âŒ ê¸°ì‚¬ ìš”ì²­ ì‹¤íŒ¨:\", response.status_code)\n",
    "        return None\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # ì œëª© ì¶”ì¶œ\n",
    "    title_tag = soup.find('h2')\n",
    "    title = title_tag.get_text(strip=True) if title_tag else \"ì œëª© ì—†ìŒ\"\n",
    "    \n",
    "    body_tag = soup.find('div', id='pnlContent')  # id ì†ì„±ìœ¼ë¡œ ë³¸ë¬¸ divë¥¼ ì°¾ìŒ\n",
    "\n",
    "    if body_tag:\n",
    "        for br in body_tag.find_all(\"br\"):\n",
    "            br.replace_with(\"\")  # br íƒœê·¸ ì‚­ì œ, ì¤„ë°”ê¿ˆ ì—†ì´ ì´ì–´ë¶™ì„\n",
    "\n",
    "        raw_text = body_tag.get_text(strip=True)\n",
    "        # ë¶ˆí•„ìš”í•œ ë¹ˆ ì¤„ ì œê±° ë° ê³µë°± ì •ë¦¬\n",
    "        body_lines = [line.strip() for line in raw_text.split(\"\\n\") if line.strip()]\n",
    "        body = \"\\n\".join(body_lines)\n",
    "    else:\n",
    "        body = \"ë³¸ë¬¸ ì—†ìŒ\"\n",
    "\n",
    "    return {\n",
    "        \"title\": title,\n",
    "        \"url\": url,\n",
    "        \"body\": body,\n",
    "        \"media\": \"ë…¸ì»·ë‰´ìŠ¤\"\n",
    "    }\n",
    "    \n",
    "def crawl_edaily_news(url):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0'\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        print(\"âŒ ê¸°ì‚¬ ìš”ì²­ ì‹¤íŒ¨:\", response.status_code)\n",
    "        return None\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # ì œëª© ì¶”ì¶œ\n",
    "    title_tag = soup.find('h1')\n",
    "    title = title_tag.get_text(strip=True) if title_tag else \"ì œëª© ì—†ìŒ\"\n",
    "    \n",
    "    body_tag = soup.find('div', class_='news_body', itemprop='articleBody')\n",
    "\n",
    "    if body_tag:\n",
    "    # ë¶€ìˆ˜ ìš”ì†Œ ì œê±°\n",
    "        for tag_to_remove in body_tag.find_all(['table', 'div'], class_=['gg_textshow']):\n",
    "            tag_to_remove.decompose()  # íƒœê·¸ ìì²´ ì‚­ì œ\n",
    "\n",
    "    # <br> íƒœê·¸ëŠ” ì¤„ë°”ê¿ˆ ë¬¸ìë¡œ ë³€í™˜\n",
    "        for br in body_tag.find_all(\"br\"):\n",
    "            br.replace_with(\"\\n\")\n",
    "\n",
    "        raw_text = body_tag.get_text(separator=\"\\n\", strip=True)\n",
    "\n",
    "    # ë¹ˆ ì¤„ ì œê±° ë° ê³µë°± ì •ë¦¬\n",
    "        body_lines = [line.strip() for line in raw_text.split(\"\\n\") if line.strip()]\n",
    "        body = \"\\n\".join(body_lines)\n",
    "        \n",
    "        body = body.replace('\\n', '').replace('\\r', '')\n",
    "    else:\n",
    "        body = \"ë³¸ë¬¸ ì—†ìŒ\"\n",
    "\n",
    "    return {\n",
    "        \"title\": title,\n",
    "        \"url\": url,\n",
    "        \"body\": body,\n",
    "        \"media\": \"ì´ë°ì¼ë¦¬\"\n",
    "    }\n",
    "    \n",
    "#ê²½ì¸ì¼ë³´ í¬ë¡¤ë§ í•¨ìˆ˜\n",
    "def crawl_kyeongin_news(url):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0'\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        print(\"âŒ ê¸°ì‚¬ ìš”ì²­ ì‹¤íŒ¨:\", response.status_code)\n",
    "        return None\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # ì œëª© ì¶”ì¶œ\n",
    "    title_tag = soup.find('h1')\n",
    "    title = title_tag.get_text(strip=True) if title_tag else \"ì œëª© ì—†ìŒ\"\n",
    "    \n",
    "    body_tag = soup.find('div', class_='article-body')  # í˜¹ì€ id='article-body'\n",
    "\n",
    "    if body_tag:\n",
    "        # ê´‘ê³ ìš© div ë“± ë¶ˆí•„ìš”í•œ ìš”ì†Œ ì œê±°: idê°€ 'svcad_'ë¡œ ì‹œì‘í•˜ëŠ” div ì œê±°\n",
    "        for ad_div in body_tag.find_all('div'):\n",
    "            if ad_div.get('id') and ad_div['id'].startswith('svcad_'):\n",
    "                ad_div.decompose()\n",
    "\n",
    "        # table, íŠ¹ì • í´ë˜ìŠ¤ div ì œê±° (í•„ìš”ì‹œ ì¶”ê°€)\n",
    "        for tag_to_remove in body_tag.find_all(['table', 'div'], class_=['gg_textshow']):\n",
    "            tag_to_remove.decompose()\n",
    "\n",
    "        # <br> íƒœê·¸ë¥¼ ì¤„ë°”ê¿ˆìœ¼ë¡œ ë³€í™˜\n",
    "        for br in body_tag.find_all('br'):\n",
    "            br.replace_with('\\n')\n",
    "\n",
    "        raw_text = body_tag.get_text(separator='\\n', strip=True)\n",
    "\n",
    "        # ë¹ˆ ì¤„ ì œê±° ë° ê³µë°± ì •ë¦¬\n",
    "        body_lines = [line.strip() for line in raw_text.split(\"\\n\") if line.strip()]\n",
    "        body = \"\\n\".join(body_lines)\n",
    "        \n",
    "        body = body.replace('\\n', '').replace('\\r', '')\n",
    "\n",
    "    else:\n",
    "        body = \"ë³¸ë¬¸ ì—†ìŒ\"\n",
    "   \n",
    "    return {\n",
    "        \"title\": title,\n",
    "        \"url\": url,\n",
    "        \"body\": body,\n",
    "        \"media\": \"ê²½ì¸ì¼ë³´\"\n",
    "    }\n",
    "\n",
    "def crawl_seoul_news(url):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0'\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, headers=headers)\n",
    "\n",
    "    # ì¸ì½”ë”© ê°•ì œ ì§€ì • (utf-8 ë˜ëŠ” euc-kr ë‘˜ ì¤‘ í•˜ë‚˜ ì‹œë„)\n",
    "    response.encoding = 'utf-8'  # ë˜ëŠ” 'euc-kr'\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        print(\"âŒ ê¸°ì‚¬ ìš”ì²­ ì‹¤íŒ¨:\", response.status_code)\n",
    "        return None\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # ì œëª© ì¶”ì¶œ\n",
    "    title_tag = soup.find('h1')\n",
    "    title = title_tag.get_text(strip=True) if title_tag else \"ì œëª© ì—†ìŒ\"\n",
    "    \n",
    "    body_tag = soup.find('div', class_='viewContent body18 color700')\n",
    "\n",
    "    if body_tag:\n",
    "        # ê´‘ê³  div ì œê±° (ì˜ˆ: idê°€ svcad_ë¡œ ì‹œì‘í•˜ëŠ” div)\n",
    "        for ad_div in body_tag.find_all('div'):\n",
    "            if ad_div.get('id') and ad_div['id'].startswith('svcad_'):\n",
    "                ad_div.decompose()\n",
    "\n",
    "        # ë¶ˆí•„ìš”í•œ íƒœê·¸ ì œê±° (í•„ìš” ì‹œ ì¶”ê°€)\n",
    "        for tag_to_remove in body_tag.find_all(['table', 'div'], class_=['gg_textshow']):\n",
    "            tag_to_remove.decompose()\n",
    "\n",
    "        # <br> íƒœê·¸ë¥¼ ì¤„ë°”ê¿ˆ ë¬¸ìë¡œ ëŒ€ì²´\n",
    "        for br in body_tag.find_all('br'):\n",
    "            br.replace_with('\\n')\n",
    "\n",
    "        # í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° ë¹ˆ ì¤„ ì œê±°\n",
    "        raw_text = body_tag.get_text(separator='\\n', strip=True)\n",
    "        body_lines = [line.strip() for line in raw_text.split('\\n') if line.strip()]\n",
    "        body = '\\n'.join(body_lines)\n",
    "        body = body.replace('\\n', '').replace('\\r', '')\n",
    "\n",
    "    else:\n",
    "        body = \"ë³¸ë¬¸ ì—†ìŒ\"\n",
    "\n",
    "    return {\n",
    "        \"title\": title,\n",
    "        \"url\": url,\n",
    "        \"body\": body,\n",
    "        \"media\": \"ì„œìš¸ì‹ ë¬¸\"\n",
    "    }\n",
    "    \n",
    "def crawl_fn_news(url):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0'\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, headers=headers)\n",
    "\n",
    "# ì¸ì½”ë”© ê°•ì œ ì§€ì • (utf-8 ë˜ëŠ” euc-kr ë‘˜ ì¤‘ í•˜ë‚˜ ì‹œë„)\n",
    "    response.encoding = 'utf-8'  # ë˜ëŠ” 'euc-kr'\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        print(\"âŒ ê¸°ì‚¬ ìš”ì²­ ì‹¤íŒ¨:\", response.status_code)\n",
    "        return None\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # ì œëª© ì¶”ì¶œ\n",
    "    title_tag = soup.find('h1')\n",
    "    title = title_tag.get_text(strip=True) if title_tag else \"ì œëª© ì—†ìŒ\"\n",
    "    \n",
    "    body_tag = soup.find('div', class_='cont_view', id='article_content')\n",
    "\n",
    "    if body_tag:\n",
    "        # ê´‘ê³  div ì œê±° (í•„ìš” ì‹œ ì¡°ê±´ ì¶”ê°€)\n",
    "        for ad_div in body_tag.find_all('div'):\n",
    "            if ad_div.get('id') and ad_div['id'].startswith('svcad_'):\n",
    "                ad_div.decompose()\n",
    "\n",
    "        # ë¶€ìˆ˜ ìš”ì†Œ ì œê±° (í•„ìš”í•˜ë©´ ë” ì¶”ê°€ ê°€ëŠ¥)\n",
    "        for tag_to_remove in body_tag.find_all(['table', 'div'], class_=['gg_textshow']):\n",
    "            tag_to_remove.decompose()\n",
    "\n",
    "        # <br> íƒœê·¸ë¥¼ ì¤„ë°”ê¿ˆ ë¬¸ìë¡œ ëŒ€ì²´\n",
    "        for br in body_tag.find_all('br'):\n",
    "            br.replace_with('\\n')\n",
    "\n",
    "        # í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° ë¹ˆ ì¤„ ì œê±°\n",
    "        raw_text = body_tag.get_text(separator='\\n', strip=True)\n",
    "        body_lines = [line.strip() for line in raw_text.split('\\n') if line.strip()]\n",
    "        body = '\\n'.join(body_lines)\n",
    "        body = body.replace('\\n', '').replace('\\r', '')\n",
    "\n",
    "    else:\n",
    "        body = \"ë³¸ë¬¸ ì—†ìŒ\"\n",
    "\n",
    "    return {\n",
    "        \"title\": title,\n",
    "        \"url\": url,\n",
    "        \"body\": body,\n",
    "        \"media\": \"íŒŒì´ë‚¸ì…œë‰´ìŠ¤\"\n",
    "    }\n",
    "\n",
    "## -----------------ì–¸ë¡ ì‚¬ í¬ë¡¤ëŸ¬ ì½”ë“œ ë!!!! ì—¬ê¸°ê¹Œì§€ ì•ˆ ê±´ë“œë ¤ë„ ë¼ìš”--------------------\n",
    "\n",
    "\n",
    "# âœ… Supabase ì €ì¥ í•¨ìˆ˜ (ìˆ˜ì •: 'test' í…Œì´ë¸”, keyword ì»¬ëŸ¼ ì¶”ê°€)\n",
    "def save_to_supabase(data, keyword, log_path=\"save_log.txt\"):\n",
    "    try:\n",
    "        # urlê³¼ keywordê°€ ê°™ì€ ë°ì´í„°ê°€ ì´ë¯¸ ìˆëŠ”ì§€ í™•ì¸\n",
    "        existing = supabase.table(\"test\").select(\"id\").eq(\"url\", data[\"url\"]).eq(\"keyword\", keyword).execute()\n",
    "        if existing.data:\n",
    "            msg = f\"âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: {data['url']}\"\n",
    "            print(msg)\n",
    "            with open(log_path, \"a\", encoding=\"utf-8\") as f:\n",
    "                f.write(msg + \"\\n\")\n",
    "            return False\n",
    "\n",
    "        data_with_keyword = data.copy()\n",
    "        data_with_keyword[\"keyword\"] = keyword\n",
    "\n",
    "        supabase.table(\"test\").insert([data_with_keyword]).execute()\n",
    "        msg = f\"âœ… ì €ì¥ ì™„ë£Œ: {data['title']}\"\n",
    "        print(msg)\n",
    "        with open(log_path, \"a\", encoding=\"utf-8\") as f:\n",
    "            f.write(msg + \"\\n\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        msg = f\"âŒ ì €ì¥ ì‹¤íŒ¨: {e}\"\n",
    "        print(msg)\n",
    "        with open(log_path, \"a\", encoding=\"utf-8\") as f:\n",
    "            f.write(msg + \"\\n\")\n",
    "        return False\n",
    "\n",
    "\n",
    "# ì–¸ë¡ ì‚¬ ë„ë©”ì¸ â†’ í¬ë¡¤ë§ í•¨ìˆ˜ ë§¤í•‘\n",
    "CRAWLER_FUNCTION_MAP = {\n",
    "    \"www.yna.co.kr\": crawl_yonhap_news,\n",
    "    \"www.newsis.com\": crawl_newsis_news,\n",
    "    \"news.mt.co.kr\" : crawl_mt_news,\n",
    "    \"biz.heraldcorp.com\" : crawl_heraldcorp_news,\n",
    "    \"www.sedaily.com\" : crawl_sedaily_news,\n",
    "    \"www.newspim.com\" : crawl_newspim_news,\n",
    "    \"www.dailian.co.kr\" : crawl_dailian_news,\n",
    "    \"www.mk.co.kr\" : crawl_mk_news,\n",
    "    \"view.asiae.co.kr\" : crawl_asiae_news,\n",
    "    \"www.nocutnews.co.kr\" : crawl_nocut_news,\n",
    "    \"www.edaily.co.kr\" : crawl_edaily_news,\n",
    "    \"www.kyeongin.com\" : crawl_kyeongin_news,\n",
    "    \"www.seoul.co.kr\" : crawl_seoul_news,\n",
    "    \"www.fnnews.com\" : crawl_fn_news,\n",
    "}\n",
    "\n",
    "# ì–¸ë¡ ì‚¬ ë„ë©”ì¸ â†’ ì–¸ë¡ ì‚¬ ì´ë¦„ ë§¤í•‘\n",
    "MEDIA_NAME_MAP = {\n",
    "    \"www.yna.co.kr\": \"ì—°í•©ë‰´ìŠ¤\",\n",
    "    \"www.newsis.com\": \"ë‰´ì‹œìŠ¤\",\n",
    "    \"news.mt.co.kr\" : \"ë¨¸ë‹ˆíˆ¬ë°ì´\",\n",
    "    \"biz.heraldcorp.com\" : \"í—¤ëŸ´ë“œê²½ì œ\",\n",
    "    \"www.sedaily.com\" : \"ì„œìš¸ê²½ì œ\",\n",
    "    \"www.newspim.com\" : \"ë‰´ìŠ¤í•Œ\",\n",
    "    \"www.dailian.co.kr\" : \"ë°ì¼ë¦¬ì•ˆ\",\n",
    "    \"www.mk.co.kr\" : \"ë§¤ì¼ê²½ì œ\",\n",
    "    \"view.asiae.co.kr\" : \"ì•„ì‹œì•„ê²½ì œ\",\n",
    "    \"www.nocutnews.co.kr\" : \"ë…¸ì»·ë‰´ìŠ¤\",\n",
    "    \"www.edaily.co.kr\" : \"ì´ë°ì¼ë¦¬ë‰´ìŠ¤\",\n",
    "    \"www.kyeongin.com\" : \"ê²½ì¸ì¼ë³´\",\n",
    "    \"www.seoul.co.kr\" : \"ì„œìš¸ì‹ ë¬¸\",\n",
    "    \"www.fnnews.com\" : \"íŒŒì´ë‚¸ì…œë‰´ìŠ¤\",\n",
    "\n",
    "    \n",
    "}\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "def save_articles_from_naver_parallel(query, max_workers=10):  # ë³‘ë ¬ì²˜ë¦¬ ì‹œë„\n",
    "    client_id = \"_TznE38btYhyzWYsq9XK\"\n",
    "    client_secret = \"06UYVlSHF9\"\n",
    "\n",
    "    encoded_query = urllib.parse.quote(query)\n",
    "    headers = {\n",
    "        \"X-Naver-Client-Id\": client_id,\n",
    "        \"X-Naver-Client-Secret\": client_secret\n",
    "    }\n",
    "\n",
    "    display = 100\n",
    "    saved_count_by_domain = {domain: 0 for domain in CRAWLER_FUNCTION_MAP.keys()}\n",
    "\n",
    "    for start in range(1, 1000 + 1, display):\n",
    "        url = f\"https://openapi.naver.com/v1/search/news.json?query={encoded_query}&display={display}&start={start}&sort=date\"\n",
    "        response = requests.get(url, headers=headers)\n",
    "\n",
    "        if response.status_code != 200:\n",
    "            print(f\"âŒ ìš”ì²­ ì‹¤íŒ¨ at start={start}: {response.status_code}\")\n",
    "            continue\n",
    "\n",
    "        data = response.json()\n",
    "        items = data.get(\"items\", [])\n",
    "        if not items:\n",
    "            break\n",
    "\n",
    "        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "            futures = []\n",
    "\n",
    "            for item in items:\n",
    "                originallink = item.get(\"originallink\", \"\")\n",
    "                domain = urlparse(originallink).netloc\n",
    "\n",
    "                if domain in CRAWLER_FUNCTION_MAP:\n",
    "                    futures.append(executor.submit(CRAWLER_FUNCTION_MAP[domain], originallink))\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "            for future in as_completed(futures):\n",
    "                try:\n",
    "                    article = future.result()\n",
    "                except Exception as e:\n",
    "                    print(f\"âŒ í¬ë¡¤ë§ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e}\")\n",
    "                    continue\n",
    "\n",
    "                if article:\n",
    "                    success = save_to_supabase(article, query)\n",
    "                    if success:\n",
    "                        domain = urlparse(article[\"url\"]).netloc\n",
    "                        saved_count_by_domain[domain] += 1\n",
    "\n",
    "        if len(items) < display:\n",
    "            break\n",
    "\n",
    "    # 1) ì¶œë ¥\n",
    "    print(\"\\nâœ… ì €ì¥ ìš”ì•½\")\n",
    "    for domain, count in saved_count_by_domain.items():\n",
    "        media = MEDIA_NAME_MAP.get(domain, domain)\n",
    "        print(f\"ğŸ“° {media} ê¸°ì‚¬ ì´ {count}ê±´ Supabase test í…Œì´ë¸”ì— ì €ì¥ ì™„ë£Œ\")\n",
    "\n",
    "    # 2) í…ìŠ¤íŠ¸ íŒŒì¼ë¡œ ì €ì¥\n",
    "    filename = f\"{query}_news_save_summary.txt\"\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(f\"ê²€ìƒ‰ì–´: {query}\\n\\n\")\n",
    "        f.write(\"ì–¸ë¡ ì‚¬ë³„ ì €ì¥ ê±´ìˆ˜ ìš”ì•½:\\n\")\n",
    "        for domain, count in saved_count_by_domain.items():\n",
    "            media = MEDIA_NAME_MAP.get(domain, domain)\n",
    "            f.write(f\"{media}: {count}ê±´\\n\")\n",
    "\n",
    "    print(f\"\\nâœ… ì €ì¥ ìš”ì•½ì„ '{filename}' íŒŒì¼ë¡œ ì €ì¥í–ˆìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "# main ì‹¤í–‰ë¶€ (inputìœ¼ë¡œ ê²€ìƒ‰ì–´ ë°›ìŒ)  \n",
    "if __name__ == \"__main__\":\n",
    "    search_keyword = input(\"ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•˜ì„¸ìš”: \").strip()\n",
    "    if search_keyword:\n",
    "        start_time = time.time()\n",
    "        save_articles_from_naver_parallel(search_keyword)\n",
    "        end_time = time.time()\n",
    "        print(f\"ì´ ì‹¤í–‰ ì‹œê°„: {end_time - start_time:.2f}ì´ˆ\")\n",
    "    else:\n",
    "        print(\"ê²€ìƒ‰ì–´ê°€ ì…ë ¥ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "webenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
