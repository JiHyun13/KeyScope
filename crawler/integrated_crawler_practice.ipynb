{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4a2edb1",
   "metadata": {},
   "source": [
    "# **í¬ë¡¤ëŸ¬ 1ê°œë§Œ ì ìš©í•´ì„œ ì €ì¥**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "aeeb5cda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ì €ì¥ ì™„ë£Œ: ç¾ ê³µë¬´ì›ë“¤ë„ ë‹¹í•˜ëŠ” AI ì‚¬ê¸°â€¦FBIì„œ ìŒì„±ë©”ì‹œì§€ ì£¼ì˜ ê²½ë³´\n",
      "âœ… ì €ì¥ ì™„ë£Œ: \"ì—¬ìë¼ì„œ ì£½ì—ˆë‹¤\" ê°•ë‚¨ì—­ ì‚´ì¸ì‚¬ê±´ 9ì£¼ê¸°â€¦'ì—¬í˜ ê·œíƒ„' ì‹œìœ„ ê³³ê³³ì„œ\n",
      "âœ… ì €ì¥ ì™„ë£Œ: ì†Œê°œíŒ… ì—¬ì„±ì´ ì‹ ì²´ ì ‘ì´‰ ê±°ì ˆí•˜ì 'ë°±ì´ˆí¬'í•œ 20ëŒ€ë‚¨\n",
      "âœ… ì €ì¥ ì™„ë£Œ: ì´ì¬ëª… \"ì—¬ì„± ë°›ëŠ” ì°¨ë³„ ê°œì„  ë…¸ë ¥í•´ì•¼â€¦ë‚¨ë…€ êµ¬ë¶„í•´ ê°ˆë“±í•˜ëŠ” ê²ƒ ì˜³ì§€ ì•Šì•„\"\n",
      "âœ… ì €ì¥ ì™„ë£Œ: ìœ„ì¥ì „ì… ì´ì¬ëª…Â·ìš¸ë³´ ê¹€ë¬¸ìˆ˜â€¦'ìˆ˜ë°±ë§Œ í´ë¦­' ë¶€ë¥´ëŠ” ë§¤ìš´ë§› 'ê°€ì§œë‰´ìŠ¤ ë°ˆ'[ê°€ì§œ íŒì¹˜ëŠ” SNSì •ì¹˜â‘ ]\n",
      "âœ… ì €ì¥ ì™„ë£Œ: ë¶€ì‚°êµìœ¡ì²­, êµì‚¬ 350ëª… ëŒ€ìƒ â€˜ë””ì§€í„¸ ì„±í­ë ¥ ì˜ˆë°© ì—°ìˆ˜â€™â€¦ë”¥í˜ì´í¬ ëŒ€ì‘\n",
      "âœ… ì €ì¥ ì™„ë£Œ: ï§¡ëŒ€í–‰ \"ëŒ€í†µë ¹ ì„ ê±°, êµ­ë¯¼ í†µí•© ì´ë£¨ëŠ” ì—­ì‚¬ì  ì „í™˜ì  ë¼ì•¼\"\n",
      "âœ… ì €ì¥ ì™„ë£Œ: ì˜¤ëŠ” 12ì¼ë¶€í„° 6Â·3ëŒ€ì„  ê³µì‹ì„ ê±°ìš´ë™â€¦\"ì¼ë°˜ ìœ ê¶Œìë„ ì„ ê±°ìš´ë™ ê°€ëŠ¥\"\n",
      "âœ… ì €ì¥ ì™„ë£Œ: ê´‘ì£¼ê²½ì°°, ì¹´ì¹´ì˜¤Tì™€ ë³´ì´ìŠ¤í”¼ì‹±Â·ë”¥í˜ì´í¬ ì˜ˆë°© í™ë³´\n",
      "âœ… ì €ì¥ ì™„ë£Œ: â€˜ëŒ€ì„  ëŒ€ë¹„â€™ ì „êµ­ ê²½ì°° ì§€íœ˜ë¶€ í™”ìƒíšŒì˜â€¦â€œìš°ë°œìƒí™© ì² ì € ëŒ€ë¹„â€\n",
      "âœ… ì €ì¥ ì™„ë£Œ: ë¼ì˜¨ì‹œíì–´, ì£¼ì‹ ì•¡ë©´ë³‘í•© í›„ ê±°ë˜ ì¬ê°œâ€¦ëª¨ë°”ì¼ì‹ ë¶„ì¦Â·ì–‘ìë‚´ì„±ì•”í˜¸Â·AIë³´ì•ˆ ì„±ì¥ê¸°ë°˜ ê°•í™”\n",
      "âœ… ì €ì¥ ì™„ë£Œ: \"ìœ ì „ì´ì•¼ë§ë¡œ ê°€ì¥ ê°•ë ¥í•œ AI\"â€¦ì˜ˆìœ ë§¨ì–¼êµ´ ì˜ìƒìœ¼ë¡œ 'í•©ì„±' ë…¼ë€ ì ì¬ìš´ ì‹ ë¶€\n",
      "âœ… ì €ì¥ ì™„ë£Œ: ì—¬ê°€ë¶€, 10.9ì–µì› ì¶”ê²½â€¦ë””ì§€í„¸ì„±ë²”ì£„ í”¼í•´ì ì§€ì›\n",
      "\n",
      "ğŸ“° ê¸°ì‚¬ ì´ 13ê±´ Supabaseì— ì €ì¥ ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import urllib.parse\n",
    "from urllib.parse import urlparse\n",
    "from bs4 import BeautifulSoup\n",
    "from supabase import create_client, Client\n",
    "\n",
    "# âœ… Supabase ì„¤ì •\n",
    "SUPABASE_URL = \"https://ypyujiaoeaqykbqetjef.supabase.co\"\n",
    "SUPABASE_KEY = \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6InlweXVqaWFvZWFxeWticWV0amVmIiwicm9sZSI6ImFub24iLCJpYXQiOjE3NDY1NDUyNTQsImV4cCI6MjA2MjEyMTI1NH0.RuR9l89gxCcMkSzO053EHluQ0ers-piN4SUjZ-LtWjU\"\n",
    "supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)\n",
    "\n",
    "# âœ… ì•„ì‹œì•„ê²½ì œ í¬ë¡¤ë§ í•¨ìˆ˜\n",
    "def crawl_asiae_news(url):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0'\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        print(\"âŒ ê¸°ì‚¬ ìš”ì²­ ì‹¤íŒ¨:\", response.status_code)\n",
    "        return None\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # ì œëª© ì¶”ì¶œ\n",
    "    title_tag = soup.find('h1')  # ê·¸ëƒ¥ ì²« ë²ˆì§¸ h1 íƒœê·¸ ì°¾ê¸°\n",
    "    title = title_tag.get_text(strip=True) if title_tag else \"ì œëª© ì—†ìŒ\"\n",
    "    \n",
    "    body_tag = soup.find('div', class_='article')\n",
    "\n",
    "    if body_tag:\n",
    "        # classê°€ txt_prohibitionì¸ p íƒœê·¸ëŠ” ì œì™¸í•˜ê³  í…ìŠ¤íŠ¸ ì¶”ì¶œ\n",
    "        p_tags = [p for p in body_tag.find_all('p') if 'txt_prohibition' not in p.get('class', [])]\n",
    "\n",
    "        raw_text = \"\\n\".join([p.get_text(strip=True) for p in p_tags]).strip()\n",
    "\n",
    "        # ê° ì¤„ ê³µë°± ì œê±° ë° ë¹ˆ ì¤„ ì œê±°\n",
    "        body_lines = [line.strip() for line in raw_text.split(\"\\n\") if line.strip()]\n",
    "\n",
    "        body = ''.join(body_lines)\n",
    "    else:\n",
    "        body = \"ë³¸ë¬¸ ì—†ìŒ\"\n",
    "\n",
    "\n",
    "    return {\n",
    "        \"title\": title,\n",
    "        \"url\": url,\n",
    "        \"body\": body,\n",
    "        \"media\": \"ì•„ì‹œì•„ê²½ì œ\"\n",
    "    }\n",
    "\n",
    "# âœ… Supabaseì— ì €ì¥\n",
    "def save_to_supabase(data):\n",
    "    try:\n",
    "        # ì¤‘ë³µ í™•ì¸ (url ê¸°ì¤€)\n",
    "        existing = supabase.table(\"articles\").select(\"id\").eq(\"url\", data[\"url\"]).execute()\n",
    "        if existing.data:\n",
    "            print(\"âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬:\", data[\"url\"])\n",
    "            return\n",
    "        response = supabase.table(\"articles\").insert([data]).execute()\n",
    "        print(\"âœ… ì €ì¥ ì™„ë£Œ:\", data[\"title\"])\n",
    "    except Exception as e:\n",
    "        print(\"âŒ ì €ì¥ ì‹¤íŒ¨:\", e)\n",
    "\n",
    "# âœ… ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ í›„ ì—°í•©ë‰´ìŠ¤ ê¸°ì‚¬ë§Œ ì €ì¥\n",
    "def save_articles_from_naver(query):\n",
    "    client_id = \"_TznE38btYhyzWYsq9XK\"\n",
    "    client_secret = \"06UYVlSHF9\"\n",
    "\n",
    "    encoded_query = urllib.parse.quote(query)\n",
    "    headers = {\n",
    "        \"X-Naver-Client-Id\": client_id,\n",
    "        \"X-Naver-Client-Secret\": client_secret\n",
    "    }\n",
    "\n",
    "    display = 100\n",
    "    total_saved = 0\n",
    "\n",
    "    for start in range(1, 1000 + 1, display):\n",
    "        url = f\"https://openapi.naver.com/v1/search/news.json?query={encoded_query}&display={display}&start={start}&sort=date\"\n",
    "        response = requests.get(url, headers=headers)\n",
    "\n",
    "        if response.status_code != 200:\n",
    "            print(f\"âŒ ìš”ì²­ ì‹¤íŒ¨ at start={start}: {response.status_code}\")\n",
    "            continue\n",
    "\n",
    "        data = response.json()\n",
    "        items = data.get(\"items\", [])\n",
    "        if not items:\n",
    "            break\n",
    "\n",
    "        for item in items:\n",
    "            originallink = item.get(\"originallink\", \"\")\n",
    "            domain = urlparse(originallink).netloc\n",
    "            if domain == \"view.asiae.co.kr\": # ìˆ˜ì • âœ…âœ…âœ…âœ…âœ…âœ…\n",
    "                article = crawl_asiae_news(originallink) # âœ…âœ…âœ…âœ…âœ…âœ…\n",
    "                if article:\n",
    "                    save_to_supabase(article)\n",
    "                    total_saved += 1\n",
    "\n",
    "        if len(items) < display:\n",
    "            break\n",
    "\n",
    "    print(f\"\\nğŸ“° ê¸°ì‚¬ ì´ {total_saved}ê±´ Supabaseì— ì €ì¥ ì™„ë£Œ\")\n",
    "\n",
    "# âœ… ì‹¤í–‰\n",
    "save_articles_from_naver(\"ë”¥í˜ì´í¬\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4241ecb8",
   "metadata": {},
   "source": [
    "## **í†µí•© í¬ë¡¤ëŸ¬ ì½”ë“œ**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537bb5c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.news1.kr/politics/assembly/5785957\n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.newsis.com/view/NISX20250516_0003178759\n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.newsis.com/view/NISX20250517_0003179429\n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.newsis.com/view/NISX20250516_0003178893\n",
      "âœ… ì €ì¥ ì™„ë£Œ: ë””ì§€í„¸ ì•„ë…¸ë¯¸, ì•Œê³ í¬ëŸ¬ì‹œ... AI ë¬¸ëª… ì‹œëŒ€ì— ëŒ€í•œ ì‚¬íšŒí•™ìì˜ ê²½ê³ \n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.chosun.com/politics/election2025/2025/05/16/3KMOI3VZRZGW7OJQ4IZ42RPSJY/?utm_source=naver&utm_medium=referral&utm_campaign=naver-news\n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.yna.co.kr/view/AKR20250516115200001?input=1195m\n",
      "âœ… ì €ì¥ ì™„ë£Œ: ì´ì¬ëª…, ì—¬ì„±ê³µì•½ \"êµì œí­ë ¥Â·ìŠ¤í† í‚¹Â·ë””ì§€í„¸ì„±ë²”ì£„ ê°•ë ¥ ëŒ€ì‘í•  ê²ƒ\"\n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.newsis.com/view/NISX20250516_0003178868\n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.news1.kr/politics/assembly/5785352\n",
      "âœ… ì €ì¥ ì™„ë£Œ: [ì†ë³´]ì´ì¬ëª… \"ë”¥í˜ì´í¬ ë“± ë””ì§€í„¸ ì„±ë²”ì£„ ì§‘ì¤‘ ëª¨ë‹ˆí„°ë§\"\n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.newsis.com/view/NISX20250516_0003178806\n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.newsis.com/view/NISX20250516_0003178097\n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.chosun.com/national/court_law/2025/05/15/3T2BMR6CZ5ACVCGK6HHG6WAZIY/?utm_source=naver&utm_medium=referral&utm_campaign=naver-news\n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.newsis.com/view/NISX20250515_0003177499\n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.news1.kr/it-science/general-it/5784220\n",
      "âœ… ì €ì¥ ì™„ë£Œ: \"AI í™•ì‚°ì— í†µì‹ ì„œë¹„ìŠ¤ í–‰íƒœ ë³€í™”...ì´ìš©ìë³´í˜¸ ìƒˆ ì ‘ê·¼ ë°©ì‹ í•„ìš”\"\n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.yna.co.kr/view/AKR20250515098500056?input=1195m\n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.yna.co.kr/view/AKR20250515061200001?input=1195m\n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.newsis.com/view/NISX20250515_0003176700\n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.news1.kr/local/jeju/5783604\n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.news1.kr/politics/assembly/5783580\n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.news1.kr/society/incident-accident/5783095\n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.yna.co.kr/view/AKR20250514101700505?input=1195m\n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.chosun.com/national/education/2025/05/15/KXEGRV2C4JAUJGSDG3WU2HXHMM/?utm_source=naver&utm_medium=referral&utm_campaign=naver-news\n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.news1.kr/it-science/cc-newmedia/5782936\n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.yna.co.kr/view/AKR20250514138700017?input=1195m\n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.newsis.com/view/NISX20250514_0003175833\n",
      "âœ… ì €ì¥ ì™„ë£Œ: í•˜ë£¨ ë§Œì— 270ê±´ ëŠ˜ì—ˆë‹¤â€¦ 'ë¶ˆë²• ë”¥í˜ì´í¬'ì— ì„ ê´€ìœ„Â·ê²½ì°° ë¹„ìƒ\n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.newsis.com/view/NISX20250514_0003175121\n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.yna.co.kr/view/AKR20250514039200017?input=1195m\n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.newsis.com/view/NISX20250514_0003174700\n",
      "âœ… ì €ì¥ ì™„ë£Œ: \"ë‹¤ìŒ ëŒ€í†µë ¹ì€ ë‹¤ìŒ(Daum)ê³¼ í•¨ê»˜\"â€¦ì œ21ëŒ€ ëŒ€ì„  íŠ¹ì§‘ í˜ì´ì§€\n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.news1.kr/it-science/general-it/5782070\n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.newsis.com/view/NISX20250514_0003174607\n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.news1.kr/society/incident-accident/5781886\n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.newsis.com/view/NISX20250513_0003173845\n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.yna.co.kr/view/AKR20250513114500017?input=1195m\n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.yna.co.kr/view/AKR20250513047600530?input=1195m\n",
      "âœ… ì €ì¥ ì™„ë£Œ: ì‘ë…„ êµê¶Œì¹¨í•´ 4200ì—¬ ê±´â€¦'ì •ë‹¹í•œ ìƒí™œì§€ë„ ë¶ˆì‘'ì´ 32%\n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.newsis.com/view/NISX20250513_0003173430\n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.news1.kr/society/education/5781170\n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.yna.co.kr/view/AKR20250513041200009?input=1195m\n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.yna.co.kr/view/AKR20250512074900004?input=1195m\n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.newsis.com/view/NISX20250512_0003172026\n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.news1.kr/society/incident-accident/5779858\n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.newsis.com/view/NISX20250512_0003171989\n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.news1.kr/politics/general-politics/5779752\n",
      "âœ… ì €ì¥ ì™„ë£Œ: ì´ì£¼í˜¸ ê¶Œí•œëŒ€í–‰ \"êµ­ë¯¼í†µí•© ìœ„í•´ ê³µì • ì„ ê±° ì¤‘ìš”..ìœ„ë²•ì‹œ ë¬´ê´€ìš©\"\n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.yna.co.kr/view/AKR20250512056800530?input=1195m\n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.newsis.com/view/NISX20250512_0003171697\n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.news1.kr/local/daegu-gyeongbuk/5778854\n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.news1.kr/politics/assembly/5777732\n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.yna.co.kr/view/AKR20250509066400052?input=1195m\n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.yna.co.kr/view/AKR20250509050300017?input=1195m\n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.news1.kr/it-science/general-it/5777457\n",
      "âœ… ì €ì¥ ì™„ë£Œ: ê°•ê²½ìœ¤ ê¸°ì, ê¹€ì„¸ì˜ ê³ ì†Œâ€¦\"æ•…ê¹€ìƒˆë¡  ì œë³´ì ì°¾ì•„ê°„ ì  ì—†ì–´\"\n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.newsis.com/view/NISX20250508_0003167665\n",
      "âœ… ì €ì¥ ì™„ë£Œ: \"íˆ¬ì ì„±ê³µ, 40ì–µ ì§‘ ìƒ€ë‹¤\" 120ì–µ ëœ¯ì–´ê°„ ê·¸ë…€ ì •ì²´ì— 'ê¹œì§'â€¦ëŒ€ì²˜ ì–´ë–»ê²Œ\n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.news1.kr/local/gwangju-jeonnam/5776222\n",
      "âœ… ì €ì¥ ì™„ë£Œ: ê²½ì°°, ëŒ€ì„  íˆ¬í‘œì¼ì— 16ë§Œ8000ëª… íˆ¬ì…â€¦\"ìš°ë°œìƒí™© ì² ì € ëŒ€ë¹„\"\n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.newsis.com/view/NISX20250508_0003167022\n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.news1.kr/society/incident-accident/5775389\n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.yna.co.kr/view/AKR20250508047200004?input=1195m\n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.news1.kr/society/general-society/5775503\n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.newsis.com/view/NISX20250507_0003166634\n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.yna.co.kr/view/AKR20250507090300505?input=1195m\n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.newsis.com/view/NISX20250507_0003165527\n",
      "âœ… ì €ì¥ ì™„ë£Œ: ë¼ì˜¨ì‹œíì–´ 5ëŒ€1 ì•¡ë©´ë³‘í•© í›„ ê±°ë˜ ì¬ê°œ... \"êµ­ë‚´ì™¸ ì„±ì¥ê¸°ë°˜ ê°•í™”\"\n",
      "âœ… ì €ì¥ ì™„ë£Œ: ì‹œì‘ë„ ëª»í•œ ëŒ€í•œë¯¼êµ­ AIì‚°ì—…, ë°œëª©ë¶€í„° ì¡ëŠ” 'ì´ê²ƒ'\n",
      "âœ… ì €ì¥ ì™„ë£Œ: ì±—GPT 3ë…„ ëëŠ”ë° í•œêµ­ì€?â€¦'ë” ê°•í•œ AIê·œì œ' ë§Œë“¤ê¸° ë°”ë¹´ë‹¤\n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.news1.kr/society/incident-accident/5773697\n",
      "âœ… ì €ì¥ ì™„ë£Œ: ì–´ì œì˜ í”¼í•´ìê°€ ì˜¤ëŠ˜ì˜ ê°€í•´ìë¡œâ€¦ì‚¬ì´ë²„ ì„±í­ë ¥ ëŠªì— ë¹ ì§„ 10ëŒ€ë“¤\n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.newsis.com/view/NISX20250504_0003164362\n",
      "âœ… ì €ì¥ ì™„ë£Œ: 'AI ìœ¤ì„ì—´Â·ì´ì¬ëª…'ì´ ëŒ€ì„ íŒ í”ë“œë‚˜â€¦'ë¶ˆë²• ë”¥í˜ì´í¬' ì£¼ì˜ë³´\n",
      "âœ… ì €ì¥ ì™„ë£Œ: 'ì¿µì¿µ' ì‹¬ì¥ë°•ë™ê¹Œì§€ í›”ì¹œ ë”¥í˜ì´í¬â€¦\"êµ¬ë¶„ ë” ì–´ë µë„¤\"\n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.newsis.com/view/NISX20250502_0003162438\n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.yna.co.kr/view/AKR20250503034400530?input=1195m\n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.news1.kr/finance/blockchain-fintech/5771501\n",
      "âœ… ì €ì¥ ì™„ë£Œ: 'ë”¥í˜ì´í¬Â·ë¶ˆë²• ë¦¬ë² ì´íŠ¸' ë“± ê²½ì°°ê´€ 11ëª…, íŠ¹ì§„\n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.news1.kr/society/incident-accident/5772097\n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.yna.co.kr/view/AKR20250501112800004?input=1195m\n",
      "âœ… ì €ì¥ ì™„ë£Œ: \"ì´ˆë“±ìƒ ìë…€ ì„±êµìœ¡ ì–´ì©Œì§€\"â€¦ë¶€ëª¨ë“¤ ì‹ ì²­ ëª°ë¦¬ë”ë‹ˆ 'ì¡°ê¸° ë§ˆê°'\n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.yna.co.kr/view/AKR20250502070000530?input=1195m\n",
      "âœ… ì €ì¥ ì™„ë£Œ: ì—¬ê°€ë¶€, ì¶”ê²½ 10.9ì–µ...ë””ì§€í„¸ì„±ë²”ì£„ í”¼í•´ì ì§€ì›\n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.news1.kr/society/general-society/5771963\n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.newsis.com/view/NISX20250502_0003162646\n",
      "\n",
      "âœ… ì €ì¥ ìš”ì•½\n",
      "ğŸ“° ì—°í•©ë‰´ìŠ¤ ê¸°ì‚¬ ì´ 0ê±´ Supabaseì— ì €ì¥ ì™„ë£Œ\n",
      "ğŸ“° ì¡°ì„ ì¼ë³´ ê¸°ì‚¬ ì´ 0ê±´ Supabaseì— ì €ì¥ ì™„ë£Œ\n",
      "ğŸ“° ë‰´ì‹œìŠ¤ ê¸°ì‚¬ ì´ 0ê±´ Supabaseì— ì €ì¥ ì™„ë£Œ\n",
      "ğŸ“° ë‰´ìŠ¤1 ê¸°ì‚¬ ì´ 0ê±´ Supabaseì— ì €ì¥ ì™„ë£Œ\n",
      "ğŸ“° ë¨¸ë‹ˆíˆ¬ë°ì´ ê¸°ì‚¬ ì´ 20ê±´ Supabaseì— ì €ì¥ ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import urllib.parse\n",
    "from urllib.parse import urlparse\n",
    "from bs4 import BeautifulSoup\n",
    "from supabase import create_client, Client\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import time\n",
    "\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "# âœ… Supabase ì„¤ì •\n",
    "SUPABASE_URL = \"https://ypyujiaoeaqykbqetjef.supabase.co\"\n",
    "SUPABASE_KEY = \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6InlweXVqaWFvZWFxeWticWV0amVmIiwicm9sZSI6ImFub24iLCJpYXQiOjE3NDY1NDUyNTQsImV4cCI6MjA2MjEyMTI1NH0.RuR9l89gxCcMkSzO053EHluQ0ers-piN4SUjZ-LtWjU\"\n",
    "supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)\n",
    "\n",
    "# âœ… ì—°í•©ë‰´ìŠ¤ í¬ë¡¤ë§ í•¨ìˆ˜\n",
    "def crawl_yonhap_news(url):\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        return None\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    title_tag = soup.find('h1', class_='tit01')\n",
    "    title = title_tag.get_text(strip=True) if title_tag else \"ì œëª© ì—†ìŒ\"\n",
    "\n",
    "    body_tag = soup.find('div', class_='story-news article')\n",
    "    if body_tag:\n",
    "        paragraphs = body_tag.find_all('p')\n",
    "        body_lines = []\n",
    "        for p in paragraphs:\n",
    "            if 'txt-copyright' in p.get('class', []): continue\n",
    "            text = p.get_text(strip=True)\n",
    "            if text:\n",
    "                body_lines.append(text)\n",
    "        body = '\\n'.join(body_lines)\n",
    "    else:\n",
    "        body = \"ë³¸ë¬¸ ì—†ìŒ\"\n",
    "\n",
    "    return {\"title\": title, \"url\": url, \"body\": body, \"media\": \"ì—°í•©ë‰´ìŠ¤\"}\n",
    "\n",
    "# âœ… ì¡°ì„ ì¼ë³´ í¬ë¡¤ë§ í•¨ìˆ˜\n",
    "def crawl_chosun_news(url):\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument('--headless')\n",
    "    options.add_argument('--no-sandbox')\n",
    "    options.add_argument('--disable-dev-shm-usage')\n",
    "\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "    driver.get(url)\n",
    "    time.sleep(3)\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    driver.quit()\n",
    "\n",
    "    title_tag = soup.find('h1')\n",
    "    title = title_tag.get_text(strip=True) if title_tag else \"ì œëª© ì—†ìŒ\"\n",
    "\n",
    "    body_paragraphs = soup.select('p.article-body__content.article-body__content-text')\n",
    "    body_lines = []\n",
    "    for p in body_paragraphs:\n",
    "        text = p.get_text(strip=True)\n",
    "        if not text:\n",
    "            continue\n",
    "        if any(keyword in text for keyword in ['ê¸°ì', 'ë¬´ë‹¨ ì „ì¬', 'êµ¬ë…', 'Copyright']):\n",
    "            continue\n",
    "        body_lines.append(text)\n",
    "    body = '\\n'.join(body_lines) if body_lines else \"ë³¸ë¬¸ ì—†ìŒ\"\n",
    "\n",
    "    return {\"title\": title, \"url\": url, \"body\": body, \"media\": \"ì¡°ì„ ì¼ë³´\"}\n",
    "\n",
    "# âœ… ë‰´ì‹œìŠ¤ í¬ë¡¤ë§ í•¨ìˆ˜\n",
    "def crawl_newsis_news(url):\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        return None\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    title_tag = soup.select_one('h1.tit.title_area')\n",
    "    title = title_tag.get_text(strip=True) if title_tag else \"ì œëª© ì—†ìŒ\"\n",
    "\n",
    "    body_tag = soup.find('div', class_='viewer')\n",
    "    if body_tag:\n",
    "        raw_text = body_tag.get_text(separator=\"\\n\").strip()\n",
    "        body_lines = [line.strip() for line in raw_text.split(\"\\n\") if line.strip()]\n",
    "        body = '\\n'.join(body_lines)\n",
    "    else:\n",
    "        body = \"ë³¸ë¬¸ ì—†ìŒ\"\n",
    "\n",
    "    return {\"title\": title, \"url\": url, \"body\": body, \"media\": \"ë‰´ì‹œìŠ¤\"}\n",
    "\n",
    "\n",
    "def crawl_news1_news(url):\n",
    "    # âœ… ì…€ë ˆë‹ˆì›€ ì˜µì…˜ ì„¤ì •\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument('--headless')  # ì°½ ì—†ì´ ì‹¤í–‰\n",
    "    options.add_argument('--no-sandbox')\n",
    "    options.add_argument('--disable-dev-shm-usage')\n",
    "\n",
    "    # âœ… ë“œë¼ì´ë²„ ì‹¤í–‰\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "    driver.get(url)\n",
    "\n",
    "    # âœ… ëª…ì‹œì  ëŒ€ê¸°: ë³¸ë¬¸ì´ ë¡œë”©ë  ë•Œê¹Œì§€ ìµœëŒ€ 10ì´ˆ ëŒ€ê¸°\n",
    "    try:\n",
    "        WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.ID, \"articleBodyContent\"))\n",
    "        )\n",
    "    except:\n",
    "        print(\"â° ë¡œë”© ì‹¤íŒ¨ ë˜ëŠ” íƒ€ì„ì•„ì›ƒ ë°œìƒ\")\n",
    "        driver.quit()\n",
    "        return None\n",
    "\n",
    "    # âœ… í˜ì´ì§€ íŒŒì‹±\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    driver.quit()\n",
    "\n",
    "    # âœ… ì œëª© ì¶”ì¶œ\n",
    "    title_tag = soup.select_one('h1.article-h2-header-title.mb-40')\n",
    "    title = title_tag.get_text(strip=True) if title_tag else \"ì œëª© ì—†ìŒ\"\n",
    "\n",
    "    # âœ… ë³¸ë¬¸ ì¶”ì¶œ\n",
    "    body_tag = soup.find('div', id='articleBodyContent')\n",
    "    if body_tag:\n",
    "        paragraphs = body_tag.find_all('p')\n",
    "        body_lines = [p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True)]\n",
    "        body = '\\n'.join(body_lines)\n",
    "    else:\n",
    "        body = \"ë³¸ë¬¸ ì—†ìŒ\"\n",
    "\n",
    "    return {\n",
    "        \"title\": title,\n",
    "        \"url\": url,\n",
    "        \"body\": body,\n",
    "        \"media\": \"ë‰´ìŠ¤1\"\n",
    "    }\n",
    "    \n",
    "# âœ… ë¨¸ë‹ˆíˆ¬ë°ì´ ë‰´ìŠ¤ í¬ë¡¤ë§ í•¨ìˆ˜\n",
    "def crawl_mt_news(url):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0'\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        print(\"âŒ ê¸°ì‚¬ ìš”ì²­ ì‹¤íŒ¨:\", response.status_code)\n",
    "        return None\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # ì œëª© ì¶”ì¶œ\n",
    "    title_tag = soup.find('h1', class_=['subject'])\n",
    "    title = title_tag.get_text(strip=True) if title_tag else \"ì œëª© ì—†ìŒ\"\n",
    "\n",
    "    # ë³¸ë¬¸ ì¶”ì¶œ (p íƒœê·¸ ì¤‘ì—ì„œ ì €ì‘ê¶Œ ì •ë³´ ì œì™¸)\n",
    "    # 'view_text' í´ë˜ìŠ¤ë¥¼ ê°€ì§„ divë¥¼ ì°¾ìŒ (ë³¸ë¬¸ ì „ì²´ ì˜ì—­)\n",
    "    body_tag = soup.find('div', class_='view_text')\n",
    "\n",
    "    if body_tag:\n",
    "    # ë³¸ë¬¸ í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ (ì¤„ë°”ê¿ˆ ê¸°ì¤€ ë¶„ë¦¬)\n",
    "        raw_text = body_tag.get_text(separator=\"\\n\").strip()\n",
    "\n",
    "    # ê° ì¤„ì˜ ê³µë°± ì œê±° ë° ë¹ˆ ì¤„ ì œê±°\n",
    "        body_lines = [line.strip() for line in raw_text.split(\"\\n\") if line.strip()]\n",
    "\n",
    "    # ë‹¤ì‹œ ì¤„ë°”ê¿ˆ ë¬¸ìë¡œ í•©ì¹¨\n",
    "        body = '\\n'.join(body_lines)\n",
    "    else:\n",
    "        body = \"ë³¸ë¬¸ ì—†ìŒ\"\n",
    "\n",
    "    return {\n",
    "        \"title\": title,\n",
    "        \"url\": url,\n",
    "        \"body\": body,\n",
    "        \"media\": \"ë¨¸ë‹ˆíˆ¬ë°ì´\"\n",
    "    }\n",
    "\n",
    "# âœ… í—¤ëŸ´ë“œê²½ì œ í¬ë¡¤ë§ í•¨ìˆ˜\n",
    "def crawl_heraldcorp_news(url):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0'\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        print(\"âŒ ê¸°ì‚¬ ìš”ì²­ ì‹¤íŒ¨:\", response.status_code)\n",
    "        return None\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # ì œëª© ì¶”ì¶œ\n",
    "    title = soup.select_one('div.news_title > h1')\n",
    "    title = title.get_text(strip=True) if title else \"ì œëª© ì—†ìŒ\"\n",
    "\n",
    "    # ë³¸ë¬¸ ì¶”ì¶œ (p íƒœê·¸ ì¤‘ì—ì„œ ì €ì‘ê¶Œ ì •ë³´ ì œì™¸)\n",
    "    # 'view_text' í´ë˜ìŠ¤ë¥¼ ê°€ì§„ divë¥¼ ì°¾ìŒ (ë³¸ë¬¸ ì „ì²´ ì˜ì—­)\n",
    "    body_tag = soup.find('article', id='articleText')\n",
    "\n",
    "    if body_tag:\n",
    "        # article ì•ˆì˜ ëª¨ë“  p íƒœê·¸ í…ìŠ¤íŠ¸ë¥¼ ì¤„ë°”ê¿ˆ ê¸°ì¤€ìœ¼ë¡œ í•©ì¹¨\n",
    "        raw_text = \"\\n\".join([p.get_text(strip=True) for p in body_tag.find_all('p')]).strip()\n",
    "\n",
    "        # ê° ì¤„ì˜ ê³µë°± ì œê±° ë° ë¹ˆ ì¤„ ì œê±°\n",
    "        body_lines = [line.strip() for line in raw_text.split(\"\\n\") if line.strip()]\n",
    "\n",
    "        # ë‹¤ì‹œ ì¤„ë°”ê¿ˆ ë¬¸ìë¡œ í•©ì¹¨\n",
    "        body = '\\n'.join(body_lines)\n",
    "    else:\n",
    "        body = \"ë³¸ë¬¸ ì—†ìŒ\"\n",
    "\n",
    "    return {\n",
    "        \"title\": title,\n",
    "        \"url\": url,\n",
    "        \"body\": body,\n",
    "        \"media\": \"í—¤ëŸ´ë“œê²½ì œ\"\n",
    "    }\n",
    "\n",
    "\n",
    "# âœ… ì„œìš¸ê²½ì œ í¬ë¡¤ë§ í•¨ìˆ˜\n",
    "def crawl_sedaily_news(url):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0'\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        print(\"âŒ ê¸°ì‚¬ ìš”ì²­ ì‹¤íŒ¨:\", response.status_code)\n",
    "        return None\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # ì œëª© ì¶”ì¶œ\n",
    "    title_tag = soup.find('h1', class_=['art_tit'])\n",
    "    title = title_tag.get_text(strip=True) if title_tag else \"ì œëª© ì—†ìŒ\"\n",
    "\n",
    "    # ë³¸ë¬¸ ì¶”ì¶œ (p íƒœê·¸ ì¤‘ì—ì„œ ì €ì‘ê¶Œ ì •ë³´ ì œì™¸)\n",
    "    # 'view_text' í´ë˜ìŠ¤ë¥¼ ê°€ì§„ divë¥¼ ì°¾ìŒ (ë³¸ë¬¸ ì „ì²´ ì˜ì—­)\n",
    "    body_tag = soup.find('div', class_='article_view')\n",
    "\n",
    "    if body_tag:\n",
    "    # figure íƒœê·¸ ì œê±° (caption í¬í•¨)\n",
    "        for fig in body_tag.find_all('figure', class_='art_photo'):\n",
    "            fig.decompose()  # í•´ë‹¹ íƒœê·¸ ë° í•˜ìœ„ ë‚´ìš© ì™„ì „ ì‚­ì œ\n",
    "\n",
    "    # <br> íƒœê·¸ë¥¼ '\\n'ìœ¼ë¡œ ë³€í™˜\n",
    "        for br in body_tag.find_all('br'):\n",
    "            br.replace_with('\\n')\n",
    "\n",
    "    # í…ìŠ¤íŠ¸ ì¶”ì¶œ í›„ strip\n",
    "        raw_text = body_tag.get_text(separator=\"\\n\").strip()\n",
    "\n",
    "    # ê³µë°± ì¤„ ì œê±°\n",
    "        body_lines = [line.strip() for line in raw_text.split(\"\\n\") if line.strip()]\n",
    "\n",
    "    # ë‹¤ì‹œ ì¤„ë°”ê¿ˆìœ¼ë¡œ í•©ì¹¨\n",
    "        body = '\\n'.join(body_lines)\n",
    "    else:\n",
    "        body = \"ë³¸ë¬¸ ì—†ìŒ\"\n",
    "\n",
    "    return {\n",
    "        \"title\": title,\n",
    "        \"url\": url,\n",
    "        \"body\": body,\n",
    "        \"media\": \"ì„œìš¸ê²½ì œ\"\n",
    "    }\n",
    "\n",
    "# âœ… ë‰´ìŠ¤í•Œ í¬ë¡¤ë§ í•¨ìˆ˜\n",
    "def crawl_newspim_news(url):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0'\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        print(\"âŒ ê¸°ì‚¬ ìš”ì²­ ì‹¤íŒ¨:\", response.status_code)\n",
    "        return None\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # ì œëª© ì¶”ì¶œ\n",
    "    title_tag = soup.find('h2')\n",
    "    title = title_tag.get_text(strip=True) if title_tag else \"ì œëª© ì—†ìŒ\"\n",
    "\n",
    "    body_tag = soup.find('div', class_='contents', itemprop='articleBody')\n",
    "\n",
    "    if body_tag:\n",
    "        # articleBody ë‚´ ëª¨ë“  p íƒœê·¸ í…ìŠ¤íŠ¸ë¥¼ ì¤„ë°”ê¿ˆ ê¸°ì¤€ìœ¼ë¡œ í•©ì¹¨\n",
    "        raw_text = \"\\n\".join([p.get_text(strip=True) for p in body_tag.find_all('p')]).strip()\n",
    "\n",
    "        # ê° ì¤„ ê³µë°± ì œê±° ë° ë¹ˆ ì¤„ ì œê±°\n",
    "        body_lines = [line.strip() for line in raw_text.split(\"\\n\") if line.strip()]\n",
    "\n",
    "        body = ''.join(body_lines)\n",
    "\n",
    "    else:\n",
    "        body = \"ë³¸ë¬¸ ì—†ìŒ\"\n",
    "\n",
    "    return {\n",
    "        \"title\": title,\n",
    "        \"url\": url,\n",
    "        \"body\": body,\n",
    "        \"media\": \"ë‰´ìŠ¤í•Œ\"\n",
    "    }\n",
    "\n",
    "\n",
    "# âœ… ë°ì¼ë¦¬ì•ˆ í¬ë¡¤ë§ í•¨ìˆ˜\n",
    "def crawl_dailian_news(url):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0'\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        print(\"âŒ ê¸°ì‚¬ ìš”ì²­ ì‹¤íŒ¨:\", response.status_code)\n",
    "        return None\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # ì œëª© ì¶”ì¶œ\n",
    "    title_tag = soup.find('h1', class_=['title'])\n",
    "    title = title_tag.get_text(strip=True) if title_tag else \"ì œëª© ì—†ìŒ\"\n",
    "\n",
    "    body_tag = soup.find('div', class_='article')\n",
    "\n",
    "    if body_tag:\n",
    "        # articleBody ë‚´ ëª¨ë“  p íƒœê·¸ í…ìŠ¤íŠ¸ë¥¼ ì¤„ë°”ê¿ˆ ê¸°ì¤€ìœ¼ë¡œ í•©ì¹¨\n",
    "        raw_text = \"\\n\".join([p.get_text(strip=True) for p in body_tag.find_all('p')]).strip()\n",
    "\n",
    "        # ê° ì¤„ ê³µë°± ì œê±° ë° ë¹ˆ ì¤„ ì œê±°\n",
    "        body_lines = [line.strip() for line in raw_text.split(\"\\n\") if line.strip()]\n",
    "\n",
    "        body = ''.join(body_lines)\n",
    "\n",
    "    else:\n",
    "        body = \"ë³¸ë¬¸ ì—†ìŒ\"\n",
    "\n",
    "    return {\n",
    "        \"title\": title,\n",
    "        \"url\": url,\n",
    "        \"body\": body,\n",
    "        \"media\": \"ë°ì¼ë¦¬ì•ˆ\"\n",
    "    }\n",
    "\n",
    "# âœ… ë§¤ì¼ê²½ì œ í¬ë¡¤ë§ í•¨ìˆ˜\n",
    "def crawl_mk_news(url):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0'\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        print(\"âŒ ê¸°ì‚¬ ìš”ì²­ ì‹¤íŒ¨:\", response.status_code)\n",
    "        return None\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # ì œëª© ì¶”ì¶œ\n",
    "    title_tag = soup.find('h2', class_=['news_ttl'])\n",
    "    title = title_tag.get_text(strip=True) if title_tag else \"ì œëª© ì—†ìŒ\"\n",
    "    \n",
    "    body_tag = soup.find('div', class_='news_cnt_detail_wrap', itemprop='articleBody')\n",
    "\n",
    "    if body_tag:\n",
    "        p_texts = [p.get_text(strip=True) for p in body_tag.find_all('p')]\n",
    "        p_texts = [text for text in p_texts if text]\n",
    "\n",
    "        if p_texts:\n",
    "            body = ''.join(p_texts)\n",
    "        else:\n",
    "            # p íƒœê·¸ ì—†ê±°ë‚˜ ë¹ˆ ê²½ìš° br ê¸°ì¤€ í…ìŠ¤íŠ¸ ë…¸ë“œ ì¶”ì¶œ\n",
    "            br_texts = [str(t).strip() for t in body_tag.children if t and str(t).strip() and not getattr(t, 'name', None)]\n",
    "            br_texts = [text for text in br_texts if text]\n",
    "            if br_texts:\n",
    "                body = ''.join(br_texts)\n",
    "            else:\n",
    "                body = \"ë³¸ë¬¸ ì—†ìŒ\"\n",
    "    else:\n",
    "        body = \"ë³¸ë¬¸ ì—†ìŒ\"\n",
    "        \n",
    "    return {\n",
    "        \"title\": title,\n",
    "        \"url\": url,\n",
    "        \"body\": body,\n",
    "        \"media\": \"ë§¤ì¼ê²½ì œ\"\n",
    "    }\n",
    "\n",
    "# âœ… ì•„ì‹œì•„ê²½ì œ í¬ë¡¤ë§ í•¨ìˆ˜\n",
    "def crawl_asiae_news(url):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0'\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        print(\"âŒ ê¸°ì‚¬ ìš”ì²­ ì‹¤íŒ¨:\", response.status_code)\n",
    "        return None\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # ì œëª© ì¶”ì¶œ\n",
    "    title_tag = soup.find('h1')  # ê·¸ëƒ¥ ì²« ë²ˆì§¸ h1 íƒœê·¸ ì°¾ê¸°\n",
    "    title = title_tag.get_text(strip=True) if title_tag else \"ì œëª© ì—†ìŒ\"\n",
    "    \n",
    "    body_tag = soup.find('div', class_='article')\n",
    "\n",
    "    if body_tag:\n",
    "        # classê°€ txt_prohibitionì¸ p íƒœê·¸ëŠ” ì œì™¸í•˜ê³  í…ìŠ¤íŠ¸ ì¶”ì¶œ\n",
    "        p_tags = [p for p in body_tag.find_all('p') if 'txt_prohibition' not in p.get('class', [])]\n",
    "\n",
    "        raw_text = \"\\n\".join([p.get_text(strip=True) for p in p_tags]).strip()\n",
    "\n",
    "        # ê° ì¤„ ê³µë°± ì œê±° ë° ë¹ˆ ì¤„ ì œê±°\n",
    "        body_lines = [line.strip() for line in raw_text.split(\"\\n\") if line.strip()]\n",
    "\n",
    "        body = ''.join(body_lines)\n",
    "    else:\n",
    "        body = \"ë³¸ë¬¸ ì—†ìŒ\"\n",
    "\n",
    "\n",
    "    return {\n",
    "        \"title\": title,\n",
    "        \"url\": url,\n",
    "        \"body\": body,\n",
    "        \"media\": \"ì•„ì‹œì•„ê²½ì œ\"\n",
    "    }\n",
    "\n",
    "\n",
    "# âœ… Supabase ì €ì¥ í•¨ìˆ˜\n",
    "def save_to_supabase(data):\n",
    "    try:\n",
    "        existing = supabase.table(\"articles\").select(\"id\").eq(\"url\", data[\"url\"]).execute()\n",
    "        if existing.data:\n",
    "            print(\"âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬:\", data[\"url\"])\n",
    "            return False\n",
    "        supabase.table(\"articles\").insert([data]).execute()\n",
    "        print(\"âœ… ì €ì¥ ì™„ë£Œ:\", data['title'])\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(\"âŒ ì €ì¥ ì‹¤íŒ¨:\", e)\n",
    "        return False\n",
    "\n",
    "# âœ… ì–¸ë¡ ì‚¬ ë„ë©”ì¸ â†’ í¬ë¡¤ë§ í•¨ìˆ˜ ë§¤í•‘\n",
    "CRAWLER_FUNCTION_MAP = {\n",
    "    \"www.yna.co.kr\": crawl_yonhap_news,\n",
    "    \"www.chosun.com\": crawl_chosun_news,\n",
    "    \"www.newsis.com\": crawl_newsis_news,\n",
    "    \"www.news1.kr\" : crawl_news1_news,\n",
    "    \"news.mt.co.kr\" : crawl_mt_news,\n",
    "    \"biz.heraldcorp.com\" : crawl_heraldcorp_news,\n",
    "    \"www.sedaily.com\" : crawl_sedaily_news,\n",
    "    \"www.newspim.com\" : crawl_newspim_news,\n",
    "    \"www.dailian.co.kr\" : crawl_dailian_news,\n",
    "    \"www.mk.co.kr\" : crawl_mk_news,\n",
    "    \"view.asiae.co.kr\" : crawl_asiae_news,\n",
    "    \"www.khan.co.kr\" : crawl_khan_news,\n",
    "    \n",
    "    \n",
    "}\n",
    "\n",
    "# âœ… ì–¸ë¡ ì‚¬ ë„ë©”ì¸ â†’ ì–¸ë¡ ì‚¬ ì´ë¦„ ë§¤í•‘\n",
    "MEDIA_NAME_MAP = {\n",
    "    \"www.yna.co.kr\": \"ì—°í•©ë‰´ìŠ¤\",\n",
    "    \"www.chosun.com\": \"ì¡°ì„ ì¼ë³´\",\n",
    "    \"www.newsis.com\": \"ë‰´ì‹œìŠ¤\",\n",
    "    \"www.news1.kr\" : \"ë‰´ìŠ¤1\",\n",
    "    \"news.mt.co.kr\" : \"ë¨¸ë‹ˆíˆ¬ë°ì´\",\n",
    "    \"biz.heraldcorp.com\" : \"í—¤ëŸ´ë“œê²½ì œ\",\n",
    "    \"www.sedaily.com\" : \"ì„œìš¸ê²½ì œ\",\n",
    "    \"www.newspim.com\" : \"ë‰´ìŠ¤í•Œ\",\n",
    "    \"www.dailian.co.kr\" : \"ë°ì¼ë¦¬ì•ˆ\",\n",
    "    \"www.mk.co.kr\" : \"ë§¤ì¼ê²½ì œ\",\n",
    "    \"view.asiae.co.kr\" : \"ì•„ì‹œì•„ê²½ì œ\",\n",
    "    \"www.khan.co.kr\" : \"ê²½í–¥ì‹ ë¬¸\",\n",
    "    \n",
    "    \n",
    "}\n",
    "\n",
    "# âœ… ë„¤ì´ë²„ ë‰´ìŠ¤ ìˆ˜ì§‘ ë° ì €ì¥\n",
    "def save_articles_from_naver(query):\n",
    "    client_id = \"_TznE38btYhyzWYsq9XK\"\n",
    "    client_secret = \"06UYVlSHF9\"\n",
    "\n",
    "    encoded_query = urllib.parse.quote(query)\n",
    "    headers = {\n",
    "        \"X-Naver-Client-Id\": client_id,\n",
    "        \"X-Naver-Client-Secret\": client_secret\n",
    "    }\n",
    "\n",
    "    display = 100\n",
    "    saved_count_by_domain = {domain: 0 for domain in CRAWLER_FUNCTION_MAP.keys()}\n",
    "\n",
    "    for start in range(1, 1000 + 1, display):\n",
    "        url = f\"https://openapi.naver.com/v1/search/news.json?query={encoded_query}&display={display}&start={start}&sort=date\"\n",
    "        response = requests.get(url, headers=headers)\n",
    "\n",
    "        if response.status_code != 200:\n",
    "            print(f\"âŒ ìš”ì²­ ì‹¤íŒ¨ at start={start}: {response.status_code}\")\n",
    "            continue\n",
    "\n",
    "        data = response.json()\n",
    "        items = data.get(\"items\", [])\n",
    "        if not items:\n",
    "            break\n",
    "\n",
    "        for item in items:\n",
    "            originallink = item.get(\"originallink\", \"\")\n",
    "            domain = urlparse(originallink).netloc\n",
    "\n",
    "            if domain in CRAWLER_FUNCTION_MAP:\n",
    "                article = CRAWLER_FUNCTION_MAP[domain](originallink)\n",
    "                if article:\n",
    "                    success = save_to_supabase(article)\n",
    "                    if success:\n",
    "                        saved_count_by_domain[domain] += 1\n",
    "\n",
    "        if len(items) < display:\n",
    "            break\n",
    "\n",
    "    # âœ… ìµœì¢… ê²°ê³¼ ì¶œë ¥\n",
    "    print(\"\\nâœ… ì €ì¥ ìš”ì•½\")\n",
    "    for domain, count in saved_count_by_domain.items():\n",
    "        media = MEDIA_NAME_MAP.get(domain, domain)\n",
    "        print(f\"ğŸ“° {media} ê¸°ì‚¬ ì´ {count}ê±´ Supabaseì— ì €ì¥ ì™„ë£Œ\")\n",
    "\n",
    "# âœ… ì‹¤í–‰\n",
    "save_articles_from_naver(\"ë”¥í˜ì´í¬\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87579bb",
   "metadata": {},
   "source": [
    "## **í¬ë¡¤ëŸ¬ ê²½ëŸ‰í™”**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b86729e",
   "metadata": {},
   "source": [
    "ë™ì  í˜ì´ì§€ ì–¸ë¡ ì‚¬ë§Œ ë¹¼ê³  ì œì¼ ë§ì´ ê²€ìƒ‰ë˜ëŠ” ì–¸ë¡ ì‚¬ ëª‡ê°œë§Œ ë„£ì–´ë³´ê¸°."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334453cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ì €ì¥ ì™„ë£Œ: ì„¸ì¢…ëŒ€ ë¯¸ë˜êµìœ¡ì›, '2025 ì„œìš¸ë§ˆì´ì¹¼ë¦¬ì§€' ìš´ì˜ê¸°ê´€ ì„ ì •\n",
      "âœ… ì €ì¥ ì™„ë£Œ: [ì‚¬ê³ ] ë°ì¼ë¦¬ì•ˆ '2025 ê¸€ë¡œë²Œ ê¸ˆìœµë¹„ì „ í¬ëŸ¼', 5ì›”21ì¼ ì—¬ì˜ë„ CCMMë¹Œë”© ê°œìµœ\n",
      "âœ… ì €ì¥ ì™„ë£Œ: \"ì™¸êµ­ê³„ ì·¨ì—…, í˜„ì¥ì—ì„œ ê¸¸ì„ ì°¾ë‹¤\" ì„¸ì¢…ëŒ€, GTF ë°•ëŒíšŒ ì°¸ê°€\n",
      "âœ… ì €ì¥ ì™„ë£Œ: â€˜ì œ6íšŒ ì„¬ì˜ ë‚ â€™ í™ë³´ëŒ€ì‚¬ì— ìœ„í•˜ì¤€, í•˜í˜„ìš°, ì•ˆì„±í›ˆ, íŠ¸ë¦¬í”Œì—ìŠ¤, ì˜¤ì„¸ë“, ì •ì§€ì„  ìœ„ì´‰\n",
      "âœ… ì €ì¥ ì™„ë£Œ: í•œêµ­ì„ 'ë² ì´ì§•ì— ê°€ì¥ ê°€ê¹Œì´ ìˆëŠ” í•­ê³µëª¨í•¨'ìœ¼ë¡œ ì¸ì‹í•˜ëŠ” íŠ¸ëŸ¼í”„ í–‰ì •ë¶€\n",
      "âœ… ì €ì¥ ì™„ë£Œ: [ì˜¤ëŠ˜ì˜ ì£¼ìš”ì¼ì •]ì‚¬íšŒ(5ì›”19ì¼ ì›”ìš”ì¼)\n",
      "âœ… ì €ì¥ ì™„ë£Œ: [ì˜¤ëŠ˜ì˜ ì •ì¹˜ì¼ì •] 5ì›” 19ì¼(ì›”)\n",
      "âœ… ì €ì¥ ì™„ë£Œ: [ì˜¤ëŠ˜ì˜ ì£¼ìš”ì¼ì •]ì •ì¹˜(5ì›”19ì¼ ì›”ìš”ì¼)\n",
      "âœ… ì €ì¥ ì™„ë£Œ: ìš©ì¸íŠ¹ë¡€ì‹œ, 'ì œ2íšŒ ëŒ€í•œë¯¼êµ­ ëŒ€í•™ì—°ê·¹ì œ' ë³¸ì„  ì§„ì¶œ 12ê°œ ëŒ€í•™íŒ€ ìµœì¢… ì„ ë°œ\n",
      "âœ… ì €ì¥ ì™„ë£Œ: ìš©ì¸ì‹œ, 'ì œ2íšŒ ëŒ€í•œë¯¼êµ­ ëŒ€í•™ì—°ê·¹ì œ' ë³¸ì„  ì§„ì¶œíŒ€ ì„ ë°œ\n",
      "âœ… ì €ì¥ ì™„ë£Œ: [ì˜¤ëŠ˜ì˜ ì£¼ìš”ì¼ì •]ì‚¬íšŒ(5ì›”18ì¼ ì¼ìš”ì¼)\n",
      "âœ… ì €ì¥ ì™„ë£Œ: ë¯¼ì£¼ ì„ ëŒ€ìœ„ í˜¸ì‚¬ì¹´ ìœ ì§€ \"ê¹€ë¬¸ìˆ˜ ë‰´ë¼ì´íŠ¸ ë§ì–¸ì€ 'ë§¤êµ­'\"\n",
      "âœ… ì €ì¥ ì™„ë£Œ: [ì˜¤ëŠ˜ì˜ ì£¼ìš”ì¼ì •]ì‚¬íšŒ(5ì›”17ì¼ í† ìš”ì¼)\n",
      "âœ… ì €ì¥ ì™„ë£Œ: ì„¸ì¢…ëŒ€ ìœ„ì •ë¯¼ êµìˆ˜, ì„¸ë¼ë¯¹íŒ”ë ˆìŠ¤í™€ì—ì„œ ì´ˆì²­ ë…ì°½íšŒ ê°œìµœ\n",
      "âœ… ì €ì¥ ì™„ë£Œ: [ì˜¤ëŠ˜ì˜ ì£¼ìš”ì¼ì •]ì‚¬íšŒ(5ì›”16ì¼ ê¸ˆìš”ì¼)\n",
      "âœ… ì €ì¥ ì™„ë£Œ: \"ë²•ì¸ í‰ê°€ ì—†ì´ ì‚¬ë¦½ëŒ€ ë°œì „ ì—†ë‹¤\"â€¦38ê°œ ëŒ€í•™ ì§„ë‹¨ ê²°ê³¼(ì¢…í•©)\n",
      "âœ… ì €ì¥ ì™„ë£Œ: ì„¸ì¢…ëŒ€ í•™ìˆ ì •ë³´ì›, 'ì œ3íšŒ í•™ì •í¬ëŸ¼' ê°œìµœ\n",
      "âœ… ì €ì¥ ì™„ë£Œ: \"AI ì‹œëŒ€, ë¬´ì—‡ì´ ì‚´ì•„ë‚¨ì„ê¹Œ\" ì„¸ì¢…ëŒ€ í•™ì •í¬ëŸ¼, ì§ì—…ì˜ ë¯¸ë˜ ì¡°ë§\n",
      "âœ… ì €ì¥ ì™„ë£Œ: ì‹œë¯¼ë‹¨ì²´ \"ë¯¼ì£¼ì£¼ì˜ ìœ„ê¸° ë¶ˆí‰ë“±ì— ìˆì–´...í•´ê²°ì±…ì€ ë³µì§€êµ­ê°€\"\n",
      "âœ… ì €ì¥ ì™„ë£Œ: ì¤‘êµ¬, 2026í•™ë…„ë„ ëŒ€í•™ë³„ ì…í•™ì„¤ëª…íšŒ ê°œìµœâ€¦17ê°œ ëŒ€í•™ ì°¸ì—¬\n",
      "âœ… ì €ì¥ ì™„ë£Œ: í•œêµ­ê³µì¸íšŒê³„ì‚¬íšŒ, ì˜¤ëŠ” 21ì¼ 'ì œ18íšŒ ì§€ì†ê°€ëŠ¥ì„±ì¸ì¦í¬ëŸ¼' ê°œìµœ\n",
      "âœ… ì €ì¥ ì™„ë£Œ: ê°€ì¥ ë¶€ì‹¤í•œ ì‚¬ë¦½ëŒ€í•™ ì–´ë””?â€¦ì‚¬êµë ¨, 38ê°œ ëŒ€í•™ ì§„ë‹¨ í‰ê°€ ê³µê°œ\n",
      "âœ… ì €ì¥ ì™„ë£Œ: ì„¸ì¢…ëŒ€, 'ë°”ì´ì˜¤Â·ì œì•½ HPLC êµìœ¡ í”„ë¡œê·¸ë¨' ìš´ì˜\n",
      "âœ… ì €ì¥ ì™„ë£Œ: ë¯¼ì£¼, ê¹€ë¬¸ìˆ˜ì— \"5Â·18 ì •ì‹  í—Œë²•ìˆ˜ë¡ ê³µì•½í•´ì•¼\"â€¦í˜¸ì‚¬ì¹´ êµìˆ˜ ì˜ì…ë„\n",
      "âœ… ì €ì¥ ì™„ë£Œ: ì„œìš¸ ì¤‘êµ¬, 2026í•™ë…„ë„ ëŒ€í•™ë³„ ì…í•™ì„¤ëª…íšŒ ê°œìµœ\n",
      "âœ… ì €ì¥ ì™„ë£Œ: ì„œìš¸ ì¤‘êµ¬ 'ëŒ€í•™ë³„ ì…í•™ì„¤ëª…íšŒ' 3ì°¨ë¡€ ê°œìµœâ€¦17ê°œ ëŒ€í•™ ì°¸ì—¬\n",
      "âœ… ì €ì¥ ì™„ë£Œ: í•œêµ­ì¬ë¬´ê´€ë¦¬í•™íšŒ â€œêµ­ë¯¼ì—°ê¸ˆ, ì‚¬ì—…ë³´êµ­ ì´ë£¨ì–´ì§ˆ ìˆ˜ ìˆëŠ” ë°©í–¥ìœ¼ë¡œ ì˜ê²°ê¶Œ í–‰ì‚¬í•´ì•¼â€\n",
      "âœ… ì €ì¥ ì™„ë£Œ: [ì˜¤ëŠ˜ì˜ ì£¼ìš”ì¼ì •]ì‚¬íšŒ(5ì›”15ì¼ ëª©ìš”ì¼)\n",
      "âœ… ì €ì¥ ì™„ë£Œ: ê¹€ë¯¼ì„, í˜¸ì‚¬ì¹´ ìœ ì§€ êµìˆ˜ì™€ ìœ ì„¸â€¦\"ê¹€ë¬¸ìˆ˜, ì™œê³¡ëœ ì—­ì‚¬ê´€\"\n",
      "âœ… ì €ì¥ ì™„ë£Œ: ì„¸ì¢…ëŒ€, 'ì»´í“¨í„°í™œìš©ëŠ¥ë ¥ 1ê¸‰ ëŒ€ë¹„ë°˜' ìš´ì˜\n",
      "âœ… ì €ì¥ ì™„ë£Œ: ì„¸ì¢…ëŒ€ ë¬¼ë¦¬ì²œë¬¸í•™ê³¼ êµìˆ˜ì§„, 'ë¸Œë ˆì´í¬ìŠ¤ë£¨ìƒ' ê³µë™ ìˆ˜ìƒ\n",
      "âœ… ì €ì¥ ì™„ë£Œ: â€œì™¸ë¶€ì¸ ì°¨ë³„â€ vs â€œë“±ë¡ê¸ˆ ë‚´ë“ ì§€â€â€¦ëŒ€í•™ ì¶•ì œ ë“¤ì´ë‹¥ì¹œ â€˜í™ˆë§ˆâ€™ì— ê²°êµ­\n",
      "âœ… ì €ì¥ ì™„ë£Œ: [ì˜¤ëŠ˜ì˜ ì£¼ìš”ì¼ì •]ì‚¬íšŒ(5ì›”14ì¼ ìˆ˜ìš”ì¼)\n",
      "âœ… ì €ì¥ ì™„ë£Œ: ì„¸ì¢…ëŒ€, ì§€ì—­ ì²­ë…„ ëŒ€ìƒ 'ì²­ë…„ì·¨ì—…ì§€ì›ì •ì±…' íŠ¹ê°• ì—´ì–´\n",
      "âœ… ì €ì¥ ì™„ë£Œ: ì„¸ì¢…ëŒ€ ê¹€ëŒ€ì¢… êµìˆ˜, í•œì„¸ëŒ€ íŠ¹ê°• ì§„í–‰\n",
      "âœ… ì €ì¥ ì™„ë£Œ: \"ëª°ë¼ì„œ ëª» ì“°ëŠ” ì·¨ì—…ì •ì±… ì—†ê²Œ\" ì„¸ì¢…ëŒ€, ì²­ë…„ ì·¨ì—…ì§€ì› íŠ¹ê°•\n",
      "âœ… ì €ì¥ ì™„ë£Œ: ì¬í•™ìƒë“¤ ë¶ˆë§Œ í­ì£¼â€¦ëŒ€í•™ ì¶•ì œ â€˜ë¯¼íë…¼ë€â€™ ì´ ë…€ì„ë“¤ì˜ ì •ì²´\n",
      "âœ… ì €ì¥ ì™„ë£Œ: [ì‚¬ê³ ] ê¸€ë¡œë²Œ ê²½ìŸë ¥ ê°•í™”ë¥¼ ìœ„í•œ ë²•ì œÂ·ê¸ˆìœµ í˜ì‹  ì „ëµì„ ëª¨ìƒ‰í•©ë‹ˆë‹¤\n",
      "âœ… ì €ì¥ ì™„ë£Œ: OECD \"ë‚´ë…„ éŸ“ 1%ëŒ€ ì €ì„±ì¥\"â€¦KDI, ê²½ê¸°ì¹¨ì²´ 'ë¹¨ê°„ë¶ˆ' [Pickì½”ë…¸ë¯¸]\n",
      "âœ… ì €ì¥ ì™„ë£Œ: [ì˜¤ëŠ˜ì˜ ì£¼ìš”ì¼ì •]ì‚¬íšŒ(5ì›”13ì¼ í™”ìš”ì¼)\n",
      "âœ… ì €ì¥ ì™„ë£Œ: ëŒ€í•™ ì¶•ì œ ë“¤ì´ë‹¥ì¹œ ëŒ€í¬ì¹´ë©”ë¼ë“¤â€¦'í™ˆë§ˆì¡´'ê¹Œì§€ ë“±ì¥\n",
      "âœ… ì €ì¥ ì™„ë£Œ: ì„¸ì¢…ëŒ€ ì¥ì„ í¬ êµìˆ˜, íŠ¹ë³„ ê¸°íš ë°œë ˆ ê³µì—° 'ëŸ¬ë¸ŒìŠ¤í† ë¦¬' ê°œìµœ\n",
      "âœ… ì €ì¥ ì™„ë£Œ: KDI, ê²½ê¸°ì¹¨ì²´ ê²½ê³ â€¦OECDë„ \"ë‚´ë…„ éŸ“ 1%ëŒ€ ì €ì„±ì¥\"\n",
      "âœ… ì €ì¥ ì™„ë£Œ: ì„¸ì¢…ëŒ€, íŠ¹ë³„ ê¸°íš ë°œë ˆ ê³µì—° 'ëŸ¬ë¸ŒìŠ¤í† ë¦¬' ê°œìµœ\n",
      "âœ… ì €ì¥ ì™„ë£Œ: â€˜ì£¼ê±° ì„ëŒ€ ì†”ë£¨ì…˜â€™ ë¸”ë£¨ê·¸ë¼ìš´ë“œ, 14Â·23ì¼ ì‚¬ì—…ì„¤ëª…íšŒ ê°œìµœ\n",
      "âœ… ì €ì¥ ì™„ë£Œ: ì„¸ì¢…ëŒ€, 'ë‚˜ëˆ”ë´‰ì‚¬ë‹¨ ë¡œíƒ€ë¦¬ ìœ„ì„±í´ëŸ½' ì¶œë²”\n",
      "âœ… ì €ì¥ ì™„ë£Œ: ì œì£¼, ê¸€ë¡œë²Œ êµìœ¡Â·ì—°êµ¬ í—ˆë¸Œë¡œ ë„ì•½â€¦\"ë¯¸Â·ì¼ ëŒ€í•™ê³¼ êµë¥˜\"\n",
      "âœ… ì €ì¥ ì™„ë£Œ: [ê³µì—°ì†Œì‹] ì•„ë²¨ ì½°ë¥´í…Ÿ, êµ­ë¦½ì‹¬í¬ë‹ˆì˜¤ì¼€ìŠ¤íŠ¸ë¼ ì•„ì¹´ë°ë¯¸ì™€ í˜‘ì—°\n",
      "âœ… ì €ì¥ ì™„ë£Œ: 'ë‹¨ê¸° ì²´ë¥˜ì ê³µëµ' ë¸”ë£¨ê·¸ë¼ìš´ë“œ, ì„ëŒ€ì‚¬ì—…ì ëŒ€ìƒ ì‚¬ì—…ì„¤ëª…íšŒ ê°œìµœ\n",
      "âœ… ì €ì¥ ì™„ë£Œ: ì„¸ì¢…ëŒ€, ì¡¸ì—…ìƒ ëŒ€ìƒ íŠ¹í™” í”„ë¡œê·¸ë¨ ìš´ì˜\n",
      "âœ… ì €ì¥ ì™„ë£Œ: â€˜ë‹¨ê¸° ì„ëŒ€ ì„œë¹„ìŠ¤â€™ ë¸”ë£¨ê·¸ë¼ìš´ë“œ, êµ­ë‚´ ì„ëŒ€ì‚¬ì—…ì ëŒ€ìƒ ì‚¬ì—…ì„¤ëª…íšŒ ê°œìµœ\n",
      "âœ… ì €ì¥ ì™„ë£Œ: [ì˜¤ëŠ˜ì˜ ì£¼ìš”ì¼ì •]ì‚¬íšŒ(5ì›”12ì¼ ì›”ìš”ì¼)\n",
      "âœ… ì €ì¥ ì™„ë£Œ: í•œêµ­í˜„ëŒ€ë¬´ìš©í˜‘íšŒ, 'ì œ32íšŒ ì‹ ì¸ë°ë·”ì „' ì‹ ì¸ìƒì— ì•ˆë¬´ê°€ ê¹€ì˜ì›…\n",
      "âœ… ì €ì¥ ì™„ë£Œ: [ì˜¤ëŠ˜ì˜ ì£¼ìš”ì¼ì •]ì‚¬íšŒ(5ì›”11ì¼ ì¼ìš”ì¼)\n",
      "âœ… ì €ì¥ ì™„ë£Œ: ì„¸ì¢…ëŒ€ë¡œì„œ ì§‘íšŒí•˜ëŠ” ììœ í†µì¼ë‹¹\n",
      "âœ… ì €ì¥ ì™„ë£Œ: ì„¸ì¢…ëŒ€ë¡œì„œ ì§‘íšŒí•˜ëŠ” ììœ í†µì¼ë‹¹\n",
      "âœ… ì €ì¥ ì™„ë£Œ: ì„¸ì¢…ëŒ€ë¡œì„œ ì§‘íšŒí•˜ëŠ” ììœ í†µì¼ë‹¹\n",
      "âœ… ì €ì¥ ì™„ë£Œ: ì„¸ì¢…ëŒ€ë¡œì„œ ì§‘íšŒí•˜ëŠ” ììœ í†µì¼ë‹¹\n",
      "âœ… ì €ì¥ ì™„ë£Œ: ì„¸ì¢…ëŒ€ ê´€ê´‘ëŒ€í•™ì›, í•œÂ·ì¼Â·í™ ê³µë™ ì „ì‹œì‚°ì—… ì„±ê³¼êµë¥˜íšŒ\n",
      "âœ… ì €ì¥ ì™„ë£Œ: [ì˜¤ëŠ˜ì˜ ì£¼ìš”ì¼ì •]ì‚¬íšŒ(5ì›”10ì¼ í† ìš”ì¼)\n",
      "âœ… ì €ì¥ ì™„ë£Œ: [ì—°í•©ë‰´ìŠ¤ ì´ ì‹œê° í—¤ë“œë¼ì¸] - 18:00\n",
      "âœ… ì €ì¥ ì™„ë£Œ: ì„¸ì¢…ëŒ€ ê´€ê´‘ëŒ€í•™ì›, í•œÂ·ì¼Â·í™ 3êµ­ ì‚°í•™ì—° í˜‘ë ¥ í”„ë¡œê·¸ë¨ ì§„í–‰\n",
      "âœ… ì €ì¥ ì™„ë£Œ: ì§€ë‚œí•´ í•´ì™¸ ì…ì–‘ì•„ 58ëª…â€¦ 5ë…„ ì „ ëŒ€ë¹„ 82%â†“\n",
      "âœ… ì €ì¥ ì™„ë£Œ: ì„¸ì¢…ëŒ€, 'ìœ ë¼ì‹œì•„ íš¡ë‹¨ í­ì—¼-ê°€ë­„ ì—´ì°¨' ê°•í™” ì›ì¸ ê·œëª…\n",
      "âœ… ì €ì¥ ì™„ë£Œ: ì´ê¸°ì¼ ë³µì§€ë¶€ ì°¨ê´€ \"7ì›” ì…ì–‘ ì²´ê³„ ì™„ë¹„â€¦ì±…ì„ì„±Â·ì „ë¬¸ì„± ê°•í™”\"\n",
      "âœ… ì €ì¥ ì™„ë£Œ: í•´ì™¸ ì…ì–‘, 5ë…„ë§Œì— 317â†’58ëª…...\"êµ­ë‚´ì…ì–‘ ìš°ì„ í•´ì•¼\"\n",
      "âœ… ì €ì¥ ì™„ë£Œ: ì§€ë‚œí•´ ì…ì–‘ëœ ì•„ë™ 212ëª…â€¦í•´ì™¸ì…ì–‘ 58ëª… ì¤‘ 71%ê°€ ë¯¸êµ­ìœ¼ë¡œ\n",
      "âœ… ì €ì¥ ì™„ë£Œ: ì•„ë™ê¶Œë¦¬ë³´ì¥ì›, ì…ì–‘ì˜ ë‚  ê¸°ë…í–‰ì‚¬â€¦ê°€ì¡±ì¤‘ì‹¬ ì¶•ì œ ë¶„ìœ„ê¸° ì¡°ì„±\n",
      "âœ… ì €ì¥ ì™„ë£Œ: í•´ì™¸ì…ì–‘ì¸ì—°ëŒ€ì„œ 12ë…„ê°„ ë´‰ì‚¬â€¦ìœ ì¬í›ˆ ì˜ˆë³´ì‚¬ì¥ ë“± 16ëª… í¬ìƒ\n",
      "âœ… ì €ì¥ ì™„ë£Œ: ì´ê´‘í˜• ì§€ì¬ìœ„ ìœ„ì›ì¥ â€œí•œêµ­í˜• ì¦ê±°ìˆ˜ì§‘ì œë„ ë„ì… ì„œë‘˜ëŸ¬ì•¼â€\n",
      "âœ… ì €ì¥ ì™„ë£Œ: ì´ê´‘í˜• ì§€ì¬ìœ„ ìœ„ì›ì¥ â€œí•œêµ­í˜• ì¦ê±°ìˆ˜ì§‘ì œë„ ë„ì… ì„œë‘˜ëŸ¬ì•¼â€\n",
      "âœ… ì €ì¥ ì™„ë£Œ: \"íŠ¹í—ˆë¶„ìŸ, í•œêµ­í˜• ì¦ê±°ìˆ˜ì§‘ ì œë„ ì‹œê¸‰\"â€¦ì§€ì¬ìœ„, IPì •ì±…í¬ëŸ¼ ê°œìµœ\n",
      "âœ… ì €ì¥ ì™„ë£Œ: \"ì§ë¬´ë°œëª…ë³´ìƒ ì„¸ì œ, ê¸°íƒ€ì†Œë“ ë° ë¶„ë¦¬ê³¼ì„¸ ë°©í–¥ìœ¼ë¡œ ê°œì„ í•´ì•¼\"\n",
      "âœ… ì €ì¥ ì™„ë£Œ: [ì˜¤ëŠ˜ì˜ ì£¼ìš”ì¼ì •]ì‚¬íšŒ(5ì›”9ì¼ ê¸ˆìš”ì¼)\n",
      "âœ… ì €ì¥ ì™„ë£Œ: ë³´ëŒê·¸ë£¹, í•µì‹¬ì¸ì¬ ëŒ€ê±° ì˜ì…â€¦\"ë¯¸ë˜ì„±ì¥ë™ë ¥ í™•ë³´\"\n",
      "âœ… ì €ì¥ ì™„ë£Œ: ìƒì˜, ì•”ì°¸Â·í•œêµ­GMê³¼ â€˜ë°”ì´ ì•„ë©”ë¦¬ì¹´â€™ í˜‘ë ¥...ç¾ ì°¨ëŸ‰ íŒë§¤ í™•ëŒ€\n",
      "âœ… ì €ì¥ ì™„ë£Œ: ë³´ëŒê·¸ë£¹, CJÂ·í˜„ëŒ€ë°±í™”ì  ë“±ì„œ í•µì‹¬ ì„ì› ì˜ì…í•´ ì„±ì¥ë™ë ¥ í™•ë³´\n",
      "âœ… ì €ì¥ ì™„ë£Œ: ë³´ëŒê·¸ë£¹, í•µì‹¬ì¸ì¬ ì™¸ë¶€ì„œ ì˜ì…â€¦\"ë¯¸ë˜ ì„±ì¥ë™ë ¥ í™•ë³´\"\n",
      "âœ… ì €ì¥ ì™„ë£Œ: ì„¸ì¢…ëŒ€, ëŒ€ê¸°ì—… í˜„ì§ìì™€ í•¨ê»˜í•˜ëŠ” ë©˜í† ë§ íŠ¹ê°• ê°œìµœ\n",
      "âœ… ì €ì¥ ì™„ë£Œ: ë³´ëŒê·¸ë£¹, í•µì‹¬ì¸ì¬ ì˜ì…í•´ ë¯¸ë˜ì„±ì¥ë™ë ¥ í™•ë³´\n",
      "âœ… ì €ì¥ ì™„ë£Œ: ì„¸ì¢…ëŒ€, ì™¸êµ­ì¸ ìœ í•™ìƒ ìœ„í•œ êµ­ë‚´ ìµœëŒ€ ì·¨Â·ì°½ì—… ë°•ëŒíšŒ ê°œìµœ\n",
      "âœ… ì €ì¥ ì™„ë£Œ: ì„¸ì¢…ëŒ€, ì™¸êµ­ì¸ ìœ í•™ìƒ ëŒ€ìƒ êµ­ë‚´ ìµœëŒ€ ì·¨ì°½ì—… ë°•ëŒíšŒ ê°œìµœ\n",
      "âœ… ì €ì¥ ì™„ë£Œ: ì„±ì£¼êµ°, ì„¸ì¢…ëŒ€ì™•ìíƒœì‹¤ íƒœë´‰ì•ˆ í–‰ì°¨ ì¬í˜„ í–‰ì‚¬ ì§„í–‰\n",
      "âœ… ì €ì¥ ì™„ë£Œ: ì„±ì£¼êµ°, ì„¸ì¢…ëŒ€ì™•ì íƒœì‹¤ íƒœë´‰ì•ˆ ì˜ì‹ ì¬í˜„\n",
      "âœ… ì €ì¥ ì™„ë£Œ: [ì˜¤ëŠ˜ì˜ ì£¼ìš”ì¼ì •]ì‚¬íšŒ(5ì›”8ì¼ ëª©ìš”ì¼)\n",
      "âœ… ì €ì¥ ì™„ë£Œ: [ì˜¤ëŠ˜ì˜ ì •ì¹˜ì¼ì •] 5ì›” 8ì¼(ëª©)\n",
      "âœ… ì €ì¥ ì™„ë£Œ: [ìƒ·!] \"ìš¸í™” ì¹˜ë¯¸ëŠ”ë°\"â€¦ë§¤ìš´ë§›ìœ¼ë¡œ ë‹¬ë˜ë³¼ê¹Œ\n",
      "âœ… ì €ì¥ ì™„ë£Œ: [ì˜¤ëŠ˜ì˜ ì£¼ìš”ì¼ì •]ì •ì¹˜(5ì›”8ì¼ ëª©ìš”ì¼)\n",
      "âœ… ì €ì¥ ì™„ë£Œ: ì„¸ì¢…ë®¤ì§€ì—„ê°¤ëŸ¬ë¦¬ 2ê´€, ì„ì§„ì„± ì‘ê°€ ì´ˆëŒ€ì „ ê°œìµœ\n",
      "âœ… ì €ì¥ ì™„ë£Œ: [ì˜¤ëŠ˜ì˜ ì£¼ìš”ì¼ì •]ì‚¬íšŒ(5ì›”7ì¼ ìˆ˜ìš”ì¼)\n",
      "âœ… ì €ì¥ ì™„ë£Œ: \"ìœ ëŸ½ ë°œë ˆ ìˆœë¡€ 40ë…„ â€¦ ì‹œëŒ€ ë”°ë¼ ëª¸ì§“ë„ ë³€í™”ë¬´ìŒ\"\n",
      "\n",
      "âœ… ì €ì¥ ìš”ì•½\n",
      "ğŸ“° ì—°í•©ë‰´ìŠ¤ ê¸°ì‚¬ ì´ 8ê±´ Supabase test í…Œì´ë¸”ì— ì €ì¥ ì™„ë£Œ\n",
      "ğŸ“° ë‰´ì‹œìŠ¤ ê¸°ì‚¬ ì´ 46ê±´ Supabase test í…Œì´ë¸”ì— ì €ì¥ ì™„ë£Œ\n",
      "ğŸ“° ë¨¸ë‹ˆíˆ¬ë°ì´ ê¸°ì‚¬ ì´ 7ê±´ Supabase test í…Œì´ë¸”ì— ì €ì¥ ì™„ë£Œ\n",
      "ğŸ“° í—¤ëŸ´ë“œê²½ì œ ê¸°ì‚¬ ì´ 5ê±´ Supabase test í…Œì´ë¸”ì— ì €ì¥ ì™„ë£Œ\n",
      "ğŸ“° ì„œìš¸ê²½ì œ ê¸°ì‚¬ ì´ 5ê±´ Supabase test í…Œì´ë¸”ì— ì €ì¥ ì™„ë£Œ\n",
      "ğŸ“° ë‰´ìŠ¤í•Œ ê¸°ì‚¬ ì´ 6ê±´ Supabase test í…Œì´ë¸”ì— ì €ì¥ ì™„ë£Œ\n",
      "ğŸ“° ë°ì¼ë¦¬ì•ˆ ê¸°ì‚¬ ì´ 7ê±´ Supabase test í…Œì´ë¸”ì— ì €ì¥ ì™„ë£Œ\n",
      "ğŸ“° ë§¤ì¼ê²½ì œ ê¸°ì‚¬ ì´ 3ê±´ Supabase test í…Œì´ë¸”ì— ì €ì¥ ì™„ë£Œ\n",
      "ğŸ“° ì•„ì‹œì•„ê²½ì œ ê¸°ì‚¬ ì´ 4ê±´ Supabase test í…Œì´ë¸”ì— ì €ì¥ ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import urllib.parse\n",
    "from urllib.parse import urlparse\n",
    "from bs4 import BeautifulSoup\n",
    "from supabase import create_client, Client\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import time\n",
    "\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "# âœ… Supabase ì„¤ì •\n",
    "SUPABASE_URL = \"https://ypyujiaoeaqykbqetjef.supabase.co\"\n",
    "SUPABASE_KEY = \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6InlweXVqaWFvZWFxeWticWV0amVmIiwicm9sZSI6ImFub24iLCJpYXQiOjE3NDY1NDUyNTQsImV4cCI6MjA2MjEyMTI1NH0.RuR9l89gxCcMkSzO053EHluQ0ers-piN4SUjZ-LtWjU\"\n",
    "supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)\n",
    "\n",
    "\n",
    "## -----------------ì–¸ë¡ ì‚¬ í¬ë¡¤ëŸ¬ ì½”ë“œ ì‹œì‘!!!! ì—¬ê¸°ë¶€í„° ì•ˆ ê±´ë“œë ¤ë„ ë¼ìš”--------------------\n",
    "def crawl_yonhap_news(url):\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        return None\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    title_tag = soup.find('h1', class_='tit01')\n",
    "    title = title_tag.get_text(strip=True) if title_tag else \"ì œëª© ì—†ìŒ\"\n",
    "\n",
    "    body_tag = soup.find('div', class_='story-news article')\n",
    "    if body_tag:\n",
    "        paragraphs = body_tag.find_all('p')\n",
    "        body_lines = []\n",
    "        for p in paragraphs:\n",
    "            if 'txt-copyright' in p.get('class', []): continue\n",
    "            text = p.get_text(strip=True)\n",
    "            if text:\n",
    "                body_lines.append(text)\n",
    "        body = '\\n'.join(body_lines)\n",
    "    else:\n",
    "        body = \"ë³¸ë¬¸ ì—†ìŒ\"\n",
    "\n",
    "    return {\"title\": title, \"url\": url, \"body\": body, \"media\": \"ì—°í•©ë‰´ìŠ¤\"}\n",
    "\n",
    "\n",
    "def crawl_newsis_news(url):\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        return None\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    title_tag = soup.select_one('h1.tit.title_area')\n",
    "    title = title_tag.get_text(strip=True) if title_tag else \"ì œëª© ì—†ìŒ\"\n",
    "\n",
    "    body_tag = soup.find('div', class_='viewer')\n",
    "    if body_tag:\n",
    "        raw_text = body_tag.get_text(separator=\"\\n\").strip()\n",
    "        body_lines = [line.strip() for line in raw_text.split(\"\\n\") if line.strip()]\n",
    "        body = '\\n'.join(body_lines)\n",
    "    else:\n",
    "        body = \"ë³¸ë¬¸ ì—†ìŒ\"\n",
    "\n",
    "    return {\"title\": title, \"url\": url, \"body\": body, \"media\": \"ë‰´ì‹œìŠ¤\"}\n",
    "\n",
    "\n",
    "def crawl_mt_news(url):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0'\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        print(\"âŒ ê¸°ì‚¬ ìš”ì²­ ì‹¤íŒ¨:\", response.status_code)\n",
    "        return None\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    title_tag = soup.find('h1', class_=['subject'])\n",
    "    title = title_tag.get_text(strip=True) if title_tag else \"ì œëª© ì—†ìŒ\"\n",
    "\n",
    "    body_tag = soup.find('div', class_='view_text')\n",
    "\n",
    "    if body_tag:\n",
    "        raw_text = body_tag.get_text(separator=\"\\n\").strip()\n",
    "        body_lines = [line.strip() for line in raw_text.split(\"\\n\") if line.strip()]\n",
    "        body = '\\n'.join(body_lines)\n",
    "    else:\n",
    "        body = \"ë³¸ë¬¸ ì—†ìŒ\"\n",
    "\n",
    "    return {\n",
    "        \"title\": title,\n",
    "        \"url\": url,\n",
    "        \"body\": body,\n",
    "        \"media\": \"ë¨¸ë‹ˆíˆ¬ë°ì´\"\n",
    "    }\n",
    "\n",
    "\n",
    "def crawl_heraldcorp_news(url):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0'\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        print(\"âŒ ê¸°ì‚¬ ìš”ì²­ ì‹¤íŒ¨:\", response.status_code)\n",
    "        return None\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    title = soup.select_one('div.news_title > h1')\n",
    "    title = title.get_text(strip=True) if title else \"ì œëª© ì—†ìŒ\"\n",
    "\n",
    "    body_tag = soup.find('article', id='articleText')\n",
    "\n",
    "    if body_tag:\n",
    "        raw_text = \"\\n\".join([p.get_text(strip=True) for p in body_tag.find_all('p')]).strip()\n",
    "        body_lines = [line.strip() for line in raw_text.split(\"\\n\") if line.strip()]\n",
    "        body = '\\n'.join(body_lines)\n",
    "    else:\n",
    "        body = \"ë³¸ë¬¸ ì—†ìŒ\"\n",
    "\n",
    "    return {\n",
    "        \"title\": title,\n",
    "        \"url\": url,\n",
    "        \"body\": body,\n",
    "        \"media\": \"í—¤ëŸ´ë“œê²½ì œ\"\n",
    "    }\n",
    "\n",
    "\n",
    "def crawl_sedaily_news(url):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0'\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        print(\"âŒ ê¸°ì‚¬ ìš”ì²­ ì‹¤íŒ¨:\", response.status_code)\n",
    "        return None\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    title_tag = soup.find('h1', class_=['art_tit'])\n",
    "    title = title_tag.get_text(strip=True) if title_tag else \"ì œëª© ì—†ìŒ\"\n",
    "\n",
    "    body_tag = soup.find('div', class_='article_view')\n",
    "\n",
    "    if body_tag:\n",
    "        for fig in body_tag.find_all('figure', class_='art_photo'):\n",
    "            fig.decompose()\n",
    "\n",
    "        for br in body_tag.find_all('br'):\n",
    "            br.replace_with('\\n')\n",
    "\n",
    "        raw_text = body_tag.get_text(separator=\"\\n\").strip()\n",
    "        body_lines = [line.strip() for line in raw_text.split(\"\\n\") if line.strip()]\n",
    "        body = '\\n'.join(body_lines)\n",
    "    else:\n",
    "        body = \"ë³¸ë¬¸ ì—†ìŒ\"\n",
    "\n",
    "    return {\n",
    "        \"title\": title,\n",
    "        \"url\": url,\n",
    "        \"body\": body,\n",
    "        \"media\": \"ì„œìš¸ê²½ì œ\"\n",
    "    }\n",
    "\n",
    "\n",
    "def crawl_newspim_news(url):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0'\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        print(\"âŒ ê¸°ì‚¬ ìš”ì²­ ì‹¤íŒ¨:\", response.status_code)\n",
    "        return None\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    title_tag = soup.find('h2')\n",
    "    title = title_tag.get_text(strip=True) if title_tag else \"ì œëª© ì—†ìŒ\"\n",
    "\n",
    "    body_tag = soup.find('div', class_='contents', itemprop='articleBody')\n",
    "\n",
    "    if body_tag:\n",
    "        raw_text = \"\\n\".join([p.get_text(strip=True) for p in body_tag.find_all('p')]).strip()\n",
    "        body_lines = [line.strip() for line in raw_text.split(\"\\n\") if line.strip()]\n",
    "        body = ''.join(body_lines)\n",
    "    else:\n",
    "        body = \"ë³¸ë¬¸ ì—†ìŒ\"\n",
    "\n",
    "    return {\n",
    "        \"title\": title,\n",
    "        \"url\": url,\n",
    "        \"body\": body,\n",
    "        \"media\": \"ë‰´ìŠ¤í•Œ\"\n",
    "    }\n",
    "\n",
    "\n",
    "def crawl_dailian_news(url):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0'\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        print(\"âŒ ê¸°ì‚¬ ìš”ì²­ ì‹¤íŒ¨:\", response.status_code)\n",
    "        return None\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    title_tag = soup.find('h1', class_=['title'])\n",
    "    title = title_tag.get_text(strip=True) if title_tag else \"ì œëª© ì—†ìŒ\"\n",
    "\n",
    "    body_tag = soup.find('div', class_='article')\n",
    "\n",
    "    if body_tag:\n",
    "        raw_text = \"\\n\".join([p.get_text(strip=True) for p in body_tag.find_all('p')]).strip()\n",
    "        body_lines = [line.strip() for line in raw_text.split(\"\\n\") if line.strip()]\n",
    "        body = ''.join(body_lines)\n",
    "    else:\n",
    "        body = \"ë³¸ë¬¸ ì—†ìŒ\"\n",
    "\n",
    "    return {\n",
    "        \"title\": title,\n",
    "        \"url\": url,\n",
    "        \"body\": body,\n",
    "        \"media\": \"ë°ì¼ë¦¬ì•ˆ\"\n",
    "    }\n",
    "\n",
    "\n",
    "def crawl_mk_news(url):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0'\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        print(\"âŒ ê¸°ì‚¬ ìš”ì²­ ì‹¤íŒ¨:\", response.status_code)\n",
    "        return None\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    title_tag = soup.find('h2', class_=['news_ttl'])\n",
    "    title = title_tag.get_text(strip=True) if title_tag else \"ì œëª© ì—†ìŒ\"\n",
    "    \n",
    "    body_tag = soup.find('div', class_='news_cnt_detail_wrap', itemprop='articleBody')\n",
    "\n",
    "    if body_tag:\n",
    "        p_texts = [p.get_text(strip=True) for p in body_tag.find_all('p')]\n",
    "        p_texts = [text for text in p_texts if text]\n",
    "\n",
    "        if p_texts:\n",
    "            body = ''.join(p_texts)\n",
    "        else:\n",
    "            br_texts = [str(t).strip() for t in body_tag.children if t and str(t).strip() and not getattr(t, 'name', None)]\n",
    "            br_texts = [text for text in br_texts if text]\n",
    "            if br_texts:\n",
    "                body = ''.join(br_texts)\n",
    "            else:\n",
    "                body = \"ë³¸ë¬¸ ì—†ìŒ\"\n",
    "    else:\n",
    "        body = \"ë³¸ë¬¸ ì—†ìŒ\"\n",
    "        \n",
    "    return {\n",
    "        \"title\": title,\n",
    "        \"url\": url,\n",
    "        \"body\": body,\n",
    "        \"media\": \"ë§¤ì¼ê²½ì œ\"\n",
    "    }\n",
    "\n",
    "\n",
    "def crawl_asiae_news(url):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0'\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        print(\"âŒ ê¸°ì‚¬ ìš”ì²­ ì‹¤íŒ¨:\", response.status_code)\n",
    "        return None\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    title_tag = soup.find('h1')\n",
    "    title = title_tag.get_text(strip=True) if title_tag else \"ì œëª© ì—†ìŒ\"\n",
    "    \n",
    "    body_tag = soup.find('div', class_='article')\n",
    "\n",
    "    if body_tag:\n",
    "        p_tags = [p for p in body_tag.find_all('p') if 'txt_prohibition' not in p.get('class', [])]\n",
    "\n",
    "        raw_text = \"\\n\".join([p.get_text(strip=True) for p in p_tags]).strip()\n",
    "\n",
    "        body_lines = [line.strip() for line in raw_text.split(\"\\n\") if line.strip()]\n",
    "\n",
    "        body = ''.join(body_lines)\n",
    "    else:\n",
    "        body = \"ë³¸ë¬¸ ì—†ìŒ\"\n",
    "\n",
    "    return {\n",
    "        \"title\": title,\n",
    "        \"url\": url,\n",
    "        \"body\": body,\n",
    "        \"media\": \"ì•„ì‹œì•„ê²½ì œ\"\n",
    "    }\n",
    "\n",
    "## -----------------ì–¸ë¡ ì‚¬ í¬ë¡¤ëŸ¬ ì½”ë“œ ë!!!! ì—¬ê¸°ê¹Œì§€ ì•ˆ ê±´ë“œë ¤ë„ ë¼ìš”--------------------\n",
    "\n",
    "\n",
    "# âœ… Supabase ì €ì¥ í•¨ìˆ˜ (ìˆ˜ì •: 'test' í…Œì´ë¸”, keyword ì»¬ëŸ¼ ì¶”ê°€)\n",
    "def save_to_supabase(data, keyword):\n",
    "    try:\n",
    "        # urlê³¼ keywordê°€ ê°™ì€ ë°ì´í„°ê°€ ì´ë¯¸ ìˆëŠ”ì§€ í™•ì¸\n",
    "        existing = supabase.table(\"test\").select(\"id\").eq(\"url\", data[\"url\"]).eq(\"keyword\", keyword).execute()\n",
    "        if existing.data:\n",
    "            print(\"âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬:\", data[\"url\"])\n",
    "            return False\n",
    "\n",
    "        data_with_keyword = data.copy()\n",
    "        data_with_keyword[\"keyword\"] = keyword\n",
    "\n",
    "        supabase.table(\"test\").insert([data_with_keyword]).execute()\n",
    "        print(\"âœ… ì €ì¥ ì™„ë£Œ:\", data[\"title\"])\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(\"âŒ ì €ì¥ ì‹¤íŒ¨:\", e)\n",
    "        return False\n",
    "\n",
    "\n",
    "# ì–¸ë¡ ì‚¬ ë„ë©”ì¸ â†’ í¬ë¡¤ë§ í•¨ìˆ˜ ë§¤í•‘\n",
    "CRAWLER_FUNCTION_MAP = {\n",
    "    \"www.yna.co.kr\": crawl_yonhap_news,\n",
    "    \"www.newsis.com\": crawl_newsis_news,\n",
    "    \"news.mt.co.kr\" : crawl_mt_news,\n",
    "    \"biz.heraldcorp.com\" : crawl_heraldcorp_news,\n",
    "    \"www.sedaily.com\" : crawl_sedaily_news,\n",
    "    \"www.newspim.com\" : crawl_newspim_news,\n",
    "    \"www.dailian.co.kr\" : crawl_dailian_news,\n",
    "    \"www.mk.co.kr\" : crawl_mk_news,\n",
    "    \"view.asiae.co.kr\" : crawl_asiae_news,\n",
    "}\n",
    "\n",
    "# ì–¸ë¡ ì‚¬ ë„ë©”ì¸ â†’ ì–¸ë¡ ì‚¬ ì´ë¦„ ë§¤í•‘\n",
    "MEDIA_NAME_MAP = {\n",
    "    \"www.yna.co.kr\": \"ì—°í•©ë‰´ìŠ¤\",\n",
    "    \"www.newsis.com\": \"ë‰´ì‹œìŠ¤\",\n",
    "    \"news.mt.co.kr\" : \"ë¨¸ë‹ˆíˆ¬ë°ì´\",\n",
    "    \"biz.heraldcorp.com\" : \"í—¤ëŸ´ë“œê²½ì œ\",\n",
    "    \"www.sedaily.com\" : \"ì„œìš¸ê²½ì œ\",\n",
    "    \"www.newspim.com\" : \"ë‰´ìŠ¤í•Œ\",\n",
    "    \"www.dailian.co.kr\" : \"ë°ì¼ë¦¬ì•ˆ\",\n",
    "    \"www.mk.co.kr\" : \"ë§¤ì¼ê²½ì œ\",\n",
    "    \"view.asiae.co.kr\" : \"ì•„ì‹œì•„ê²½ì œ\",\n",
    "}\n",
    "\n",
    "# ë„¤ì´ë²„ ë‰´ìŠ¤ ìˆ˜ì§‘ ë° ì €ì¥ (ìˆ˜ì •: keyword ì¸ì ì‚¬ìš©)\n",
    "def save_articles_from_naver(query):\n",
    "    client_id = \"_TznE38btYhyzWYsq9XK\"\n",
    "    client_secret = \"06UYVlSHF9\"\n",
    "\n",
    "    encoded_query = urllib.parse.quote(query)\n",
    "    headers = {\n",
    "        \"X-Naver-Client-Id\": client_id,\n",
    "        \"X-Naver-Client-Secret\": client_secret\n",
    "    }\n",
    "\n",
    "    display = 100\n",
    "    saved_count_by_domain = {domain: 0 for domain in CRAWLER_FUNCTION_MAP.keys()}\n",
    "\n",
    "    for start in range(1, 1000 + 1, display):\n",
    "        url = f\"https://openapi.naver.com/v1/search/news.json?query={encoded_query}&display={display}&start={start}&sort=date\"\n",
    "        response = requests.get(url, headers=headers)\n",
    "\n",
    "        if response.status_code != 200:\n",
    "            print(f\"âŒ ìš”ì²­ ì‹¤íŒ¨ at start={start}: {response.status_code}\")\n",
    "            continue\n",
    "\n",
    "        data = response.json()\n",
    "        items = data.get(\"items\", [])\n",
    "        if not items:\n",
    "            break\n",
    "\n",
    "        for item in items:\n",
    "            originallink = item.get(\"originallink\", \"\")\n",
    "            domain = urlparse(originallink).netloc\n",
    "\n",
    "            if domain in CRAWLER_FUNCTION_MAP:\n",
    "                article = CRAWLER_FUNCTION_MAP[domain](originallink)\n",
    "                if article:\n",
    "                    success = save_to_supabase(article, query)\n",
    "                    if success:\n",
    "                        saved_count_by_domain[domain] += 1\n",
    "\n",
    "        if len(items) < display:\n",
    "            break\n",
    "\n",
    "    print(\"\\nâœ… ì €ì¥ ìš”ì•½\")\n",
    "    for domain, count in saved_count_by_domain.items():\n",
    "        media = MEDIA_NAME_MAP.get(domain, domain)\n",
    "        print(f\"ğŸ“° {media} ê¸°ì‚¬ ì´ {count}ê±´ Supabase test í…Œì´ë¸”ì— ì €ì¥ ì™„ë£Œ\")\n",
    "\n",
    "\n",
    "# main ì‹¤í–‰ë¶€ (inputìœ¼ë¡œ ê²€ìƒ‰ì–´ ë°›ìŒ)\n",
    "if __name__ == \"__main__\":\n",
    "    search_keyword = input(\"ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•˜ì„¸ìš”: \").strip()\n",
    "    if search_keyword:\n",
    "        save_articles_from_naver(search_keyword)\n",
    "    else:\n",
    "        print(\"ê²€ìƒ‰ì–´ê°€ ì…ë ¥ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
