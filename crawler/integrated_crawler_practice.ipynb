{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4a2edb1",
   "metadata": {},
   "source": [
    "# **크롤러 1개만 적용해서 저장**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "aeeb5cda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 저장 완료: 美 공무원들도 당하는 AI 사기…FBI서 음성메시지 주의 경보\n",
      "✅ 저장 완료: \"여자라서 죽었다\" 강남역 살인사건 9주기…'여혐 규탄' 시위 곳곳서\n",
      "✅ 저장 완료: 소개팅 여성이 신체 접촉 거절하자 '백초크'한 20대남\n",
      "✅ 저장 완료: 이재명 \"여성 받는 차별 개선 노력해야…남녀 구분해 갈등하는 것 옳지 않아\"\n",
      "✅ 저장 완료: 위장전입 이재명·울보 김문수…'수백만 클릭' 부르는 매운맛 '가짜뉴스 밈'[가짜 판치는 SNS정치①]\n",
      "✅ 저장 완료: 부산교육청, 교사 350명 대상 ‘디지털 성폭력 예방 연수’…딥페이크 대응\n",
      "✅ 저장 완료: 李대행 \"대통령 선거, 국민 통합 이루는 역사적 전환점 돼야\"\n",
      "✅ 저장 완료: 오는 12일부터 6·3대선 공식선거운동…\"일반 유권자도 선거운동 가능\"\n",
      "✅ 저장 완료: 광주경찰, 카카오T와 보이스피싱·딥페이크 예방 홍보\n",
      "✅ 저장 완료: ‘대선 대비’ 전국 경찰 지휘부 화상회의…“우발상황 철저 대비”\n",
      "✅ 저장 완료: 라온시큐어, 주식 액면병합 후 거래 재개…모바일신분증·양자내성암호·AI보안 성장기반 강화\n",
      "✅ 저장 완료: \"유전이야말로 가장 강력한 AI\"…예쁜 맨얼굴 영상으로 '합성' 논란 잠재운 신부\n",
      "✅ 저장 완료: 여가부, 10.9억원 추경…디지털성범죄 피해자 지원\n",
      "\n",
      "📰 기사 총 13건 Supabase에 저장 완료\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import urllib.parse\n",
    "from urllib.parse import urlparse\n",
    "from bs4 import BeautifulSoup\n",
    "from supabase import create_client, Client\n",
    "\n",
    "# ✅ Supabase 설정\n",
    "SUPABASE_URL = \"https://ypyujiaoeaqykbqetjef.supabase.co\"\n",
    "SUPABASE_KEY = \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6InlweXVqaWFvZWFxeWticWV0amVmIiwicm9sZSI6ImFub24iLCJpYXQiOjE3NDY1NDUyNTQsImV4cCI6MjA2MjEyMTI1NH0.RuR9l89gxCcMkSzO053EHluQ0ers-piN4SUjZ-LtWjU\"\n",
    "supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)\n",
    "\n",
    "# ✅ 아시아경제 크롤링 함수\n",
    "def crawl_asiae_news(url):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0'\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        print(\"❌ 기사 요청 실패:\", response.status_code)\n",
    "        return None\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # 제목 추출\n",
    "    title_tag = soup.find('h1')  # 그냥 첫 번째 h1 태그 찾기\n",
    "    title = title_tag.get_text(strip=True) if title_tag else \"제목 없음\"\n",
    "    \n",
    "    body_tag = soup.find('div', class_='article')\n",
    "\n",
    "    if body_tag:\n",
    "        # class가 txt_prohibition인 p 태그는 제외하고 텍스트 추출\n",
    "        p_tags = [p for p in body_tag.find_all('p') if 'txt_prohibition' not in p.get('class', [])]\n",
    "\n",
    "        raw_text = \"\\n\".join([p.get_text(strip=True) for p in p_tags]).strip()\n",
    "\n",
    "        # 각 줄 공백 제거 및 빈 줄 제거\n",
    "        body_lines = [line.strip() for line in raw_text.split(\"\\n\") if line.strip()]\n",
    "\n",
    "        body = ''.join(body_lines)\n",
    "    else:\n",
    "        body = \"본문 없음\"\n",
    "\n",
    "\n",
    "    return {\n",
    "        \"title\": title,\n",
    "        \"url\": url,\n",
    "        \"body\": body,\n",
    "        \"media\": \"아시아경제\"\n",
    "    }\n",
    "\n",
    "# ✅ Supabase에 저장\n",
    "def save_to_supabase(data):\n",
    "    try:\n",
    "        # 중복 확인 (url 기준)\n",
    "        existing = supabase.table(\"articles\").select(\"id\").eq(\"url\", data[\"url\"]).execute()\n",
    "        if existing.data:\n",
    "            print(\"⚠️ 이미 저장된 기사:\", data[\"url\"])\n",
    "            return\n",
    "        response = supabase.table(\"articles\").insert([data]).execute()\n",
    "        print(\"✅ 저장 완료:\", data[\"title\"])\n",
    "    except Exception as e:\n",
    "        print(\"❌ 저장 실패:\", e)\n",
    "\n",
    "# ✅ 네이버 뉴스 검색 후 연합뉴스 기사만 저장\n",
    "def save_articles_from_naver(query):\n",
    "    client_id = \"_TznE38btYhyzWYsq9XK\"\n",
    "    client_secret = \"06UYVlSHF9\"\n",
    "\n",
    "    encoded_query = urllib.parse.quote(query)\n",
    "    headers = {\n",
    "        \"X-Naver-Client-Id\": client_id,\n",
    "        \"X-Naver-Client-Secret\": client_secret\n",
    "    }\n",
    "\n",
    "    display = 100\n",
    "    total_saved = 0\n",
    "\n",
    "    for start in range(1, 1000 + 1, display):\n",
    "        url = f\"https://openapi.naver.com/v1/search/news.json?query={encoded_query}&display={display}&start={start}&sort=date\"\n",
    "        response = requests.get(url, headers=headers)\n",
    "\n",
    "        if response.status_code != 200:\n",
    "            print(f\"❌ 요청 실패 at start={start}: {response.status_code}\")\n",
    "            continue\n",
    "\n",
    "        data = response.json()\n",
    "        items = data.get(\"items\", [])\n",
    "        if not items:\n",
    "            break\n",
    "\n",
    "        for item in items:\n",
    "            originallink = item.get(\"originallink\", \"\")\n",
    "            domain = urlparse(originallink).netloc\n",
    "            if domain == \"view.asiae.co.kr\": # 수정 ✅✅✅✅✅✅\n",
    "                article = crawl_asiae_news(originallink) # ✅✅✅✅✅✅\n",
    "                if article:\n",
    "                    save_to_supabase(article)\n",
    "                    total_saved += 1\n",
    "\n",
    "        if len(items) < display:\n",
    "            break\n",
    "\n",
    "    print(f\"\\n📰 기사 총 {total_saved}건 Supabase에 저장 완료\")\n",
    "\n",
    "# ✅ 실행\n",
    "save_articles_from_naver(\"딥페이크\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4241ecb8",
   "metadata": {},
   "source": [
    "## **통합 크롤러 코드**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537bb5c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ 이미 저장된 기사: https://www.news1.kr/politics/assembly/5785957\n",
      "⚠️ 이미 저장된 기사: https://www.newsis.com/view/NISX20250516_0003178759\n",
      "⚠️ 이미 저장된 기사: https://www.newsis.com/view/NISX20250517_0003179429\n",
      "⚠️ 이미 저장된 기사: https://www.newsis.com/view/NISX20250516_0003178893\n",
      "✅ 저장 완료: 디지털 아노미, 알고크러시... AI 문명 시대에 대한 사회학자의 경고\n",
      "⚠️ 이미 저장된 기사: https://www.chosun.com/politics/election2025/2025/05/16/3KMOI3VZRZGW7OJQ4IZ42RPSJY/?utm_source=naver&utm_medium=referral&utm_campaign=naver-news\n",
      "⚠️ 이미 저장된 기사: https://www.yna.co.kr/view/AKR20250516115200001?input=1195m\n",
      "✅ 저장 완료: 이재명, 여성공약 \"교제폭력·스토킹·디지털성범죄 강력 대응할 것\"\n",
      "⚠️ 이미 저장된 기사: https://www.newsis.com/view/NISX20250516_0003178868\n",
      "⚠️ 이미 저장된 기사: https://www.news1.kr/politics/assembly/5785352\n",
      "✅ 저장 완료: [속보]이재명 \"딥페이크 등 디지털 성범죄 집중 모니터링\"\n",
      "⚠️ 이미 저장된 기사: https://www.newsis.com/view/NISX20250516_0003178806\n",
      "⚠️ 이미 저장된 기사: https://www.newsis.com/view/NISX20250516_0003178097\n",
      "⚠️ 이미 저장된 기사: https://www.chosun.com/national/court_law/2025/05/15/3T2BMR6CZ5ACVCGK6HHG6WAZIY/?utm_source=naver&utm_medium=referral&utm_campaign=naver-news\n",
      "⚠️ 이미 저장된 기사: https://www.newsis.com/view/NISX20250515_0003177499\n",
      "⚠️ 이미 저장된 기사: https://www.news1.kr/it-science/general-it/5784220\n",
      "✅ 저장 완료: \"AI 확산에 통신서비스 행태 변화...이용자보호 새 접근 방식 필요\"\n",
      "⚠️ 이미 저장된 기사: https://www.yna.co.kr/view/AKR20250515098500056?input=1195m\n",
      "⚠️ 이미 저장된 기사: https://www.yna.co.kr/view/AKR20250515061200001?input=1195m\n",
      "⚠️ 이미 저장된 기사: https://www.newsis.com/view/NISX20250515_0003176700\n",
      "⚠️ 이미 저장된 기사: https://www.news1.kr/local/jeju/5783604\n",
      "⚠️ 이미 저장된 기사: https://www.news1.kr/politics/assembly/5783580\n",
      "⚠️ 이미 저장된 기사: https://www.news1.kr/society/incident-accident/5783095\n",
      "⚠️ 이미 저장된 기사: https://www.yna.co.kr/view/AKR20250514101700505?input=1195m\n",
      "⚠️ 이미 저장된 기사: https://www.chosun.com/national/education/2025/05/15/KXEGRV2C4JAUJGSDG3WU2HXHMM/?utm_source=naver&utm_medium=referral&utm_campaign=naver-news\n",
      "⚠️ 이미 저장된 기사: https://www.news1.kr/it-science/cc-newmedia/5782936\n",
      "⚠️ 이미 저장된 기사: https://www.yna.co.kr/view/AKR20250514138700017?input=1195m\n",
      "⚠️ 이미 저장된 기사: https://www.newsis.com/view/NISX20250514_0003175833\n",
      "✅ 저장 완료: 하루 만에 270건 늘었다… '불법 딥페이크'에 선관위·경찰 비상\n",
      "⚠️ 이미 저장된 기사: https://www.newsis.com/view/NISX20250514_0003175121\n",
      "⚠️ 이미 저장된 기사: https://www.yna.co.kr/view/AKR20250514039200017?input=1195m\n",
      "⚠️ 이미 저장된 기사: https://www.newsis.com/view/NISX20250514_0003174700\n",
      "✅ 저장 완료: \"다음 대통령은 다음(Daum)과 함께\"…제21대 대선 특집 페이지\n",
      "⚠️ 이미 저장된 기사: https://www.news1.kr/it-science/general-it/5782070\n",
      "⚠️ 이미 저장된 기사: https://www.newsis.com/view/NISX20250514_0003174607\n",
      "⚠️ 이미 저장된 기사: https://www.news1.kr/society/incident-accident/5781886\n",
      "⚠️ 이미 저장된 기사: https://www.newsis.com/view/NISX20250513_0003173845\n",
      "⚠️ 이미 저장된 기사: https://www.yna.co.kr/view/AKR20250513114500017?input=1195m\n",
      "⚠️ 이미 저장된 기사: https://www.yna.co.kr/view/AKR20250513047600530?input=1195m\n",
      "✅ 저장 완료: 작년 교권침해 4200여 건…'정당한 생활지도 불응'이 32%\n",
      "⚠️ 이미 저장된 기사: https://www.newsis.com/view/NISX20250513_0003173430\n",
      "⚠️ 이미 저장된 기사: https://www.news1.kr/society/education/5781170\n",
      "⚠️ 이미 저장된 기사: https://www.yna.co.kr/view/AKR20250513041200009?input=1195m\n",
      "⚠️ 이미 저장된 기사: https://www.yna.co.kr/view/AKR20250512074900004?input=1195m\n",
      "⚠️ 이미 저장된 기사: https://www.newsis.com/view/NISX20250512_0003172026\n",
      "⚠️ 이미 저장된 기사: https://www.news1.kr/society/incident-accident/5779858\n",
      "⚠️ 이미 저장된 기사: https://www.newsis.com/view/NISX20250512_0003171989\n",
      "⚠️ 이미 저장된 기사: https://www.news1.kr/politics/general-politics/5779752\n",
      "✅ 저장 완료: 이주호 권한대행 \"국민통합 위해 공정 선거 중요..위법시 무관용\"\n",
      "⚠️ 이미 저장된 기사: https://www.yna.co.kr/view/AKR20250512056800530?input=1195m\n",
      "⚠️ 이미 저장된 기사: https://www.newsis.com/view/NISX20250512_0003171697\n",
      "⚠️ 이미 저장된 기사: https://www.news1.kr/local/daegu-gyeongbuk/5778854\n",
      "⚠️ 이미 저장된 기사: https://www.news1.kr/politics/assembly/5777732\n",
      "⚠️ 이미 저장된 기사: https://www.yna.co.kr/view/AKR20250509066400052?input=1195m\n",
      "⚠️ 이미 저장된 기사: https://www.yna.co.kr/view/AKR20250509050300017?input=1195m\n",
      "⚠️ 이미 저장된 기사: https://www.news1.kr/it-science/general-it/5777457\n",
      "✅ 저장 완료: 강경윤 기자, 김세의 고소…\"故김새론 제보자 찾아간 적 없어\"\n",
      "⚠️ 이미 저장된 기사: https://www.newsis.com/view/NISX20250508_0003167665\n",
      "✅ 저장 완료: \"투자 성공, 40억 집 샀다\" 120억 뜯어간 그녀 정체에 '깜짝'…대처 어떻게\n",
      "⚠️ 이미 저장된 기사: https://www.news1.kr/local/gwangju-jeonnam/5776222\n",
      "✅ 저장 완료: 경찰, 대선 투표일에 16만8000명 투입…\"우발상황 철저 대비\"\n",
      "⚠️ 이미 저장된 기사: https://www.newsis.com/view/NISX20250508_0003167022\n",
      "⚠️ 이미 저장된 기사: https://www.news1.kr/society/incident-accident/5775389\n",
      "⚠️ 이미 저장된 기사: https://www.yna.co.kr/view/AKR20250508047200004?input=1195m\n",
      "⚠️ 이미 저장된 기사: https://www.news1.kr/society/general-society/5775503\n",
      "⚠️ 이미 저장된 기사: https://www.newsis.com/view/NISX20250507_0003166634\n",
      "⚠️ 이미 저장된 기사: https://www.yna.co.kr/view/AKR20250507090300505?input=1195m\n",
      "⚠️ 이미 저장된 기사: https://www.newsis.com/view/NISX20250507_0003165527\n",
      "✅ 저장 완료: 라온시큐어 5대1 액면병합 후 거래 재개... \"국내외 성장기반 강화\"\n",
      "✅ 저장 완료: 시작도 못한 대한민국 AI산업, 발목부터 잡는 '이것'\n",
      "✅ 저장 완료: 챗GPT 3년 됐는데 한국은?…'더 강한 AI규제' 만들기 바빴다\n",
      "⚠️ 이미 저장된 기사: https://www.news1.kr/society/incident-accident/5773697\n",
      "✅ 저장 완료: 어제의 피해자가 오늘의 가해자로…사이버 성폭력 늪에 빠진 10대들\n",
      "⚠️ 이미 저장된 기사: https://www.newsis.com/view/NISX20250504_0003164362\n",
      "✅ 저장 완료: 'AI 윤석열·이재명'이 대선판 흔드나…'불법 딥페이크' 주의보\n",
      "✅ 저장 완료: '쿵쿵' 심장박동까지 훔친 딥페이크…\"구분 더 어렵네\"\n",
      "⚠️ 이미 저장된 기사: https://www.newsis.com/view/NISX20250502_0003162438\n",
      "⚠️ 이미 저장된 기사: https://www.yna.co.kr/view/AKR20250503034400530?input=1195m\n",
      "⚠️ 이미 저장된 기사: https://www.news1.kr/finance/blockchain-fintech/5771501\n",
      "✅ 저장 완료: '딥페이크·불법 리베이트' 등 경찰관 11명, 특진\n",
      "⚠️ 이미 저장된 기사: https://www.news1.kr/society/incident-accident/5772097\n",
      "⚠️ 이미 저장된 기사: https://www.yna.co.kr/view/AKR20250501112800004?input=1195m\n",
      "✅ 저장 완료: \"초등생 자녀 성교육 어쩌지\"…부모들 신청 몰리더니 '조기 마감'\n",
      "⚠️ 이미 저장된 기사: https://www.yna.co.kr/view/AKR20250502070000530?input=1195m\n",
      "✅ 저장 완료: 여가부, 추경 10.9억...디지털성범죄 피해자 지원\n",
      "⚠️ 이미 저장된 기사: https://www.news1.kr/society/general-society/5771963\n",
      "⚠️ 이미 저장된 기사: https://www.newsis.com/view/NISX20250502_0003162646\n",
      "\n",
      "✅ 저장 요약\n",
      "📰 연합뉴스 기사 총 0건 Supabase에 저장 완료\n",
      "📰 조선일보 기사 총 0건 Supabase에 저장 완료\n",
      "📰 뉴시스 기사 총 0건 Supabase에 저장 완료\n",
      "📰 뉴스1 기사 총 0건 Supabase에 저장 완료\n",
      "📰 머니투데이 기사 총 20건 Supabase에 저장 완료\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import urllib.parse\n",
    "from urllib.parse import urlparse\n",
    "from bs4 import BeautifulSoup\n",
    "from supabase import create_client, Client\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import time\n",
    "\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "# ✅ Supabase 설정\n",
    "SUPABASE_URL = \"https://ypyujiaoeaqykbqetjef.supabase.co\"\n",
    "SUPABASE_KEY = \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6InlweXVqaWFvZWFxeWticWV0amVmIiwicm9sZSI6ImFub24iLCJpYXQiOjE3NDY1NDUyNTQsImV4cCI6MjA2MjEyMTI1NH0.RuR9l89gxCcMkSzO053EHluQ0ers-piN4SUjZ-LtWjU\"\n",
    "supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)\n",
    "\n",
    "# ✅ 연합뉴스 크롤링 함수\n",
    "def crawl_yonhap_news(url):\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        return None\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    title_tag = soup.find('h1', class_='tit01')\n",
    "    title = title_tag.get_text(strip=True) if title_tag else \"제목 없음\"\n",
    "\n",
    "    body_tag = soup.find('div', class_='story-news article')\n",
    "    if body_tag:\n",
    "        paragraphs = body_tag.find_all('p')\n",
    "        body_lines = []\n",
    "        for p in paragraphs:\n",
    "            if 'txt-copyright' in p.get('class', []): continue\n",
    "            text = p.get_text(strip=True)\n",
    "            if text:\n",
    "                body_lines.append(text)\n",
    "        body = '\\n'.join(body_lines)\n",
    "    else:\n",
    "        body = \"본문 없음\"\n",
    "\n",
    "    return {\"title\": title, \"url\": url, \"body\": body, \"media\": \"연합뉴스\"}\n",
    "\n",
    "# ✅ 조선일보 크롤링 함수\n",
    "def crawl_chosun_news(url):\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument('--headless')\n",
    "    options.add_argument('--no-sandbox')\n",
    "    options.add_argument('--disable-dev-shm-usage')\n",
    "\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "    driver.get(url)\n",
    "    time.sleep(3)\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    driver.quit()\n",
    "\n",
    "    title_tag = soup.find('h1')\n",
    "    title = title_tag.get_text(strip=True) if title_tag else \"제목 없음\"\n",
    "\n",
    "    body_paragraphs = soup.select('p.article-body__content.article-body__content-text')\n",
    "    body_lines = []\n",
    "    for p in body_paragraphs:\n",
    "        text = p.get_text(strip=True)\n",
    "        if not text:\n",
    "            continue\n",
    "        if any(keyword in text for keyword in ['기자', '무단 전재', '구독', 'Copyright']):\n",
    "            continue\n",
    "        body_lines.append(text)\n",
    "    body = '\\n'.join(body_lines) if body_lines else \"본문 없음\"\n",
    "\n",
    "    return {\"title\": title, \"url\": url, \"body\": body, \"media\": \"조선일보\"}\n",
    "\n",
    "# ✅ 뉴시스 크롤링 함수\n",
    "def crawl_newsis_news(url):\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        return None\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    title_tag = soup.select_one('h1.tit.title_area')\n",
    "    title = title_tag.get_text(strip=True) if title_tag else \"제목 없음\"\n",
    "\n",
    "    body_tag = soup.find('div', class_='viewer')\n",
    "    if body_tag:\n",
    "        raw_text = body_tag.get_text(separator=\"\\n\").strip()\n",
    "        body_lines = [line.strip() for line in raw_text.split(\"\\n\") if line.strip()]\n",
    "        body = '\\n'.join(body_lines)\n",
    "    else:\n",
    "        body = \"본문 없음\"\n",
    "\n",
    "    return {\"title\": title, \"url\": url, \"body\": body, \"media\": \"뉴시스\"}\n",
    "\n",
    "\n",
    "def crawl_news1_news(url):\n",
    "    # ✅ 셀레니움 옵션 설정\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument('--headless')  # 창 없이 실행\n",
    "    options.add_argument('--no-sandbox')\n",
    "    options.add_argument('--disable-dev-shm-usage')\n",
    "\n",
    "    # ✅ 드라이버 실행\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "    driver.get(url)\n",
    "\n",
    "    # ✅ 명시적 대기: 본문이 로딩될 때까지 최대 10초 대기\n",
    "    try:\n",
    "        WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.ID, \"articleBodyContent\"))\n",
    "        )\n",
    "    except:\n",
    "        print(\"⏰ 로딩 실패 또는 타임아웃 발생\")\n",
    "        driver.quit()\n",
    "        return None\n",
    "\n",
    "    # ✅ 페이지 파싱\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    driver.quit()\n",
    "\n",
    "    # ✅ 제목 추출\n",
    "    title_tag = soup.select_one('h1.article-h2-header-title.mb-40')\n",
    "    title = title_tag.get_text(strip=True) if title_tag else \"제목 없음\"\n",
    "\n",
    "    # ✅ 본문 추출\n",
    "    body_tag = soup.find('div', id='articleBodyContent')\n",
    "    if body_tag:\n",
    "        paragraphs = body_tag.find_all('p')\n",
    "        body_lines = [p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True)]\n",
    "        body = '\\n'.join(body_lines)\n",
    "    else:\n",
    "        body = \"본문 없음\"\n",
    "\n",
    "    return {\n",
    "        \"title\": title,\n",
    "        \"url\": url,\n",
    "        \"body\": body,\n",
    "        \"media\": \"뉴스1\"\n",
    "    }\n",
    "    \n",
    "# ✅ 머니투데이 뉴스 크롤링 함수\n",
    "def crawl_mt_news(url):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0'\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        print(\"❌ 기사 요청 실패:\", response.status_code)\n",
    "        return None\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # 제목 추출\n",
    "    title_tag = soup.find('h1', class_=['subject'])\n",
    "    title = title_tag.get_text(strip=True) if title_tag else \"제목 없음\"\n",
    "\n",
    "    # 본문 추출 (p 태그 중에서 저작권 정보 제외)\n",
    "    # 'view_text' 클래스를 가진 div를 찾음 (본문 전체 영역)\n",
    "    body_tag = soup.find('div', class_='view_text')\n",
    "\n",
    "    if body_tag:\n",
    "    # 본문 텍스트만 추출 (줄바꿈 기준 분리)\n",
    "        raw_text = body_tag.get_text(separator=\"\\n\").strip()\n",
    "\n",
    "    # 각 줄의 공백 제거 및 빈 줄 제거\n",
    "        body_lines = [line.strip() for line in raw_text.split(\"\\n\") if line.strip()]\n",
    "\n",
    "    # 다시 줄바꿈 문자로 합침\n",
    "        body = '\\n'.join(body_lines)\n",
    "    else:\n",
    "        body = \"본문 없음\"\n",
    "\n",
    "    return {\n",
    "        \"title\": title,\n",
    "        \"url\": url,\n",
    "        \"body\": body,\n",
    "        \"media\": \"머니투데이\"\n",
    "    }\n",
    "\n",
    "# ✅ 헤럴드경제 크롤링 함수\n",
    "def crawl_heraldcorp_news(url):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0'\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        print(\"❌ 기사 요청 실패:\", response.status_code)\n",
    "        return None\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # 제목 추출\n",
    "    title = soup.select_one('div.news_title > h1')\n",
    "    title = title.get_text(strip=True) if title else \"제목 없음\"\n",
    "\n",
    "    # 본문 추출 (p 태그 중에서 저작권 정보 제외)\n",
    "    # 'view_text' 클래스를 가진 div를 찾음 (본문 전체 영역)\n",
    "    body_tag = soup.find('article', id='articleText')\n",
    "\n",
    "    if body_tag:\n",
    "        # article 안의 모든 p 태그 텍스트를 줄바꿈 기준으로 합침\n",
    "        raw_text = \"\\n\".join([p.get_text(strip=True) for p in body_tag.find_all('p')]).strip()\n",
    "\n",
    "        # 각 줄의 공백 제거 및 빈 줄 제거\n",
    "        body_lines = [line.strip() for line in raw_text.split(\"\\n\") if line.strip()]\n",
    "\n",
    "        # 다시 줄바꿈 문자로 합침\n",
    "        body = '\\n'.join(body_lines)\n",
    "    else:\n",
    "        body = \"본문 없음\"\n",
    "\n",
    "    return {\n",
    "        \"title\": title,\n",
    "        \"url\": url,\n",
    "        \"body\": body,\n",
    "        \"media\": \"헤럴드경제\"\n",
    "    }\n",
    "\n",
    "\n",
    "# ✅ 서울경제 크롤링 함수\n",
    "def crawl_sedaily_news(url):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0'\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        print(\"❌ 기사 요청 실패:\", response.status_code)\n",
    "        return None\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # 제목 추출\n",
    "    title_tag = soup.find('h1', class_=['art_tit'])\n",
    "    title = title_tag.get_text(strip=True) if title_tag else \"제목 없음\"\n",
    "\n",
    "    # 본문 추출 (p 태그 중에서 저작권 정보 제외)\n",
    "    # 'view_text' 클래스를 가진 div를 찾음 (본문 전체 영역)\n",
    "    body_tag = soup.find('div', class_='article_view')\n",
    "\n",
    "    if body_tag:\n",
    "    # figure 태그 제거 (caption 포함)\n",
    "        for fig in body_tag.find_all('figure', class_='art_photo'):\n",
    "            fig.decompose()  # 해당 태그 및 하위 내용 완전 삭제\n",
    "\n",
    "    # <br> 태그를 '\\n'으로 변환\n",
    "        for br in body_tag.find_all('br'):\n",
    "            br.replace_with('\\n')\n",
    "\n",
    "    # 텍스트 추출 후 strip\n",
    "        raw_text = body_tag.get_text(separator=\"\\n\").strip()\n",
    "\n",
    "    # 공백 줄 제거\n",
    "        body_lines = [line.strip() for line in raw_text.split(\"\\n\") if line.strip()]\n",
    "\n",
    "    # 다시 줄바꿈으로 합침\n",
    "        body = '\\n'.join(body_lines)\n",
    "    else:\n",
    "        body = \"본문 없음\"\n",
    "\n",
    "    return {\n",
    "        \"title\": title,\n",
    "        \"url\": url,\n",
    "        \"body\": body,\n",
    "        \"media\": \"서울경제\"\n",
    "    }\n",
    "\n",
    "# ✅ 뉴스핌 크롤링 함수\n",
    "def crawl_newspim_news(url):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0'\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        print(\"❌ 기사 요청 실패:\", response.status_code)\n",
    "        return None\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # 제목 추출\n",
    "    title_tag = soup.find('h2')\n",
    "    title = title_tag.get_text(strip=True) if title_tag else \"제목 없음\"\n",
    "\n",
    "    body_tag = soup.find('div', class_='contents', itemprop='articleBody')\n",
    "\n",
    "    if body_tag:\n",
    "        # articleBody 내 모든 p 태그 텍스트를 줄바꿈 기준으로 합침\n",
    "        raw_text = \"\\n\".join([p.get_text(strip=True) for p in body_tag.find_all('p')]).strip()\n",
    "\n",
    "        # 각 줄 공백 제거 및 빈 줄 제거\n",
    "        body_lines = [line.strip() for line in raw_text.split(\"\\n\") if line.strip()]\n",
    "\n",
    "        body = ''.join(body_lines)\n",
    "\n",
    "    else:\n",
    "        body = \"본문 없음\"\n",
    "\n",
    "    return {\n",
    "        \"title\": title,\n",
    "        \"url\": url,\n",
    "        \"body\": body,\n",
    "        \"media\": \"뉴스핌\"\n",
    "    }\n",
    "\n",
    "\n",
    "# ✅ 데일리안 크롤링 함수\n",
    "def crawl_dailian_news(url):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0'\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        print(\"❌ 기사 요청 실패:\", response.status_code)\n",
    "        return None\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # 제목 추출\n",
    "    title_tag = soup.find('h1', class_=['title'])\n",
    "    title = title_tag.get_text(strip=True) if title_tag else \"제목 없음\"\n",
    "\n",
    "    body_tag = soup.find('div', class_='article')\n",
    "\n",
    "    if body_tag:\n",
    "        # articleBody 내 모든 p 태그 텍스트를 줄바꿈 기준으로 합침\n",
    "        raw_text = \"\\n\".join([p.get_text(strip=True) for p in body_tag.find_all('p')]).strip()\n",
    "\n",
    "        # 각 줄 공백 제거 및 빈 줄 제거\n",
    "        body_lines = [line.strip() for line in raw_text.split(\"\\n\") if line.strip()]\n",
    "\n",
    "        body = ''.join(body_lines)\n",
    "\n",
    "    else:\n",
    "        body = \"본문 없음\"\n",
    "\n",
    "    return {\n",
    "        \"title\": title,\n",
    "        \"url\": url,\n",
    "        \"body\": body,\n",
    "        \"media\": \"데일리안\"\n",
    "    }\n",
    "\n",
    "# ✅ 매일경제 크롤링 함수\n",
    "def crawl_mk_news(url):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0'\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        print(\"❌ 기사 요청 실패:\", response.status_code)\n",
    "        return None\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # 제목 추출\n",
    "    title_tag = soup.find('h2', class_=['news_ttl'])\n",
    "    title = title_tag.get_text(strip=True) if title_tag else \"제목 없음\"\n",
    "    \n",
    "    body_tag = soup.find('div', class_='news_cnt_detail_wrap', itemprop='articleBody')\n",
    "\n",
    "    if body_tag:\n",
    "        p_texts = [p.get_text(strip=True) for p in body_tag.find_all('p')]\n",
    "        p_texts = [text for text in p_texts if text]\n",
    "\n",
    "        if p_texts:\n",
    "            body = ''.join(p_texts)\n",
    "        else:\n",
    "            # p 태그 없거나 빈 경우 br 기준 텍스트 노드 추출\n",
    "            br_texts = [str(t).strip() for t in body_tag.children if t and str(t).strip() and not getattr(t, 'name', None)]\n",
    "            br_texts = [text for text in br_texts if text]\n",
    "            if br_texts:\n",
    "                body = ''.join(br_texts)\n",
    "            else:\n",
    "                body = \"본문 없음\"\n",
    "    else:\n",
    "        body = \"본문 없음\"\n",
    "        \n",
    "    return {\n",
    "        \"title\": title,\n",
    "        \"url\": url,\n",
    "        \"body\": body,\n",
    "        \"media\": \"매일경제\"\n",
    "    }\n",
    "\n",
    "# ✅ 아시아경제 크롤링 함수\n",
    "def crawl_asiae_news(url):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0'\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        print(\"❌ 기사 요청 실패:\", response.status_code)\n",
    "        return None\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # 제목 추출\n",
    "    title_tag = soup.find('h1')  # 그냥 첫 번째 h1 태그 찾기\n",
    "    title = title_tag.get_text(strip=True) if title_tag else \"제목 없음\"\n",
    "    \n",
    "    body_tag = soup.find('div', class_='article')\n",
    "\n",
    "    if body_tag:\n",
    "        # class가 txt_prohibition인 p 태그는 제외하고 텍스트 추출\n",
    "        p_tags = [p for p in body_tag.find_all('p') if 'txt_prohibition' not in p.get('class', [])]\n",
    "\n",
    "        raw_text = \"\\n\".join([p.get_text(strip=True) for p in p_tags]).strip()\n",
    "\n",
    "        # 각 줄 공백 제거 및 빈 줄 제거\n",
    "        body_lines = [line.strip() for line in raw_text.split(\"\\n\") if line.strip()]\n",
    "\n",
    "        body = ''.join(body_lines)\n",
    "    else:\n",
    "        body = \"본문 없음\"\n",
    "\n",
    "\n",
    "    return {\n",
    "        \"title\": title,\n",
    "        \"url\": url,\n",
    "        \"body\": body,\n",
    "        \"media\": \"아시아경제\"\n",
    "    }\n",
    "\n",
    "\n",
    "# ✅ Supabase 저장 함수\n",
    "def save_to_supabase(data):\n",
    "    try:\n",
    "        existing = supabase.table(\"articles\").select(\"id\").eq(\"url\", data[\"url\"]).execute()\n",
    "        if existing.data:\n",
    "            print(\"⚠️ 이미 저장된 기사:\", data[\"url\"])\n",
    "            return False\n",
    "        supabase.table(\"articles\").insert([data]).execute()\n",
    "        print(\"✅ 저장 완료:\", data['title'])\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(\"❌ 저장 실패:\", e)\n",
    "        return False\n",
    "\n",
    "# ✅ 언론사 도메인 → 크롤링 함수 매핑\n",
    "CRAWLER_FUNCTION_MAP = {\n",
    "    \"www.yna.co.kr\": crawl_yonhap_news,\n",
    "    \"www.chosun.com\": crawl_chosun_news,\n",
    "    \"www.newsis.com\": crawl_newsis_news,\n",
    "    \"www.news1.kr\" : crawl_news1_news,\n",
    "    \"news.mt.co.kr\" : crawl_mt_news,\n",
    "    \"biz.heraldcorp.com\" : crawl_heraldcorp_news,\n",
    "    \"www.sedaily.com\" : crawl_sedaily_news,\n",
    "    \"www.newspim.com\" : crawl_newspim_news,\n",
    "    \"www.dailian.co.kr\" : crawl_dailian_news,\n",
    "    \"www.mk.co.kr\" : crawl_mk_news,\n",
    "    \"view.asiae.co.kr\" : crawl_asiae_news,\n",
    "    \"www.khan.co.kr\" : crawl_khan_news,\n",
    "    \n",
    "    \n",
    "}\n",
    "\n",
    "# ✅ 언론사 도메인 → 언론사 이름 매핑\n",
    "MEDIA_NAME_MAP = {\n",
    "    \"www.yna.co.kr\": \"연합뉴스\",\n",
    "    \"www.chosun.com\": \"조선일보\",\n",
    "    \"www.newsis.com\": \"뉴시스\",\n",
    "    \"www.news1.kr\" : \"뉴스1\",\n",
    "    \"news.mt.co.kr\" : \"머니투데이\",\n",
    "    \"biz.heraldcorp.com\" : \"헤럴드경제\",\n",
    "    \"www.sedaily.com\" : \"서울경제\",\n",
    "    \"www.newspim.com\" : \"뉴스핌\",\n",
    "    \"www.dailian.co.kr\" : \"데일리안\",\n",
    "    \"www.mk.co.kr\" : \"매일경제\",\n",
    "    \"view.asiae.co.kr\" : \"아시아경제\",\n",
    "    \"www.khan.co.kr\" : \"경향신문\",\n",
    "    \n",
    "    \n",
    "}\n",
    "\n",
    "# ✅ 네이버 뉴스 수집 및 저장\n",
    "def save_articles_from_naver(query):\n",
    "    client_id = \"_TznE38btYhyzWYsq9XK\"\n",
    "    client_secret = \"06UYVlSHF9\"\n",
    "\n",
    "    encoded_query = urllib.parse.quote(query)\n",
    "    headers = {\n",
    "        \"X-Naver-Client-Id\": client_id,\n",
    "        \"X-Naver-Client-Secret\": client_secret\n",
    "    }\n",
    "\n",
    "    display = 100\n",
    "    saved_count_by_domain = {domain: 0 for domain in CRAWLER_FUNCTION_MAP.keys()}\n",
    "\n",
    "    for start in range(1, 1000 + 1, display):\n",
    "        url = f\"https://openapi.naver.com/v1/search/news.json?query={encoded_query}&display={display}&start={start}&sort=date\"\n",
    "        response = requests.get(url, headers=headers)\n",
    "\n",
    "        if response.status_code != 200:\n",
    "            print(f\"❌ 요청 실패 at start={start}: {response.status_code}\")\n",
    "            continue\n",
    "\n",
    "        data = response.json()\n",
    "        items = data.get(\"items\", [])\n",
    "        if not items:\n",
    "            break\n",
    "\n",
    "        for item in items:\n",
    "            originallink = item.get(\"originallink\", \"\")\n",
    "            domain = urlparse(originallink).netloc\n",
    "\n",
    "            if domain in CRAWLER_FUNCTION_MAP:\n",
    "                article = CRAWLER_FUNCTION_MAP[domain](originallink)\n",
    "                if article:\n",
    "                    success = save_to_supabase(article)\n",
    "                    if success:\n",
    "                        saved_count_by_domain[domain] += 1\n",
    "\n",
    "        if len(items) < display:\n",
    "            break\n",
    "\n",
    "    # ✅ 최종 결과 출력\n",
    "    print(\"\\n✅ 저장 요약\")\n",
    "    for domain, count in saved_count_by_domain.items():\n",
    "        media = MEDIA_NAME_MAP.get(domain, domain)\n",
    "        print(f\"📰 {media} 기사 총 {count}건 Supabase에 저장 완료\")\n",
    "\n",
    "# ✅ 실행\n",
    "save_articles_from_naver(\"딥페이크\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87579bb",
   "metadata": {},
   "source": [
    "## **크롤러 경량화**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b86729e",
   "metadata": {},
   "source": [
    "동적 페이지 언론사만 빼고 제일 많이 검색되는 언론사 몇개만 넣어보기."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334453cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 저장 완료: 두나무, 웹3 보안 인재 양성 업사이드 아카데미 3기 참가자 모집\n",
      "✅ 저장 완료: [지방의회, 내가 뛴다] 하남시의회 오지연 의원\n",
      "✅ 저장 완료: 성남시, 사이버대학 4곳과 수업료 감면 협약\n",
      "✅ 저장 완료: 맥북에 지원금까지···두나무 보안 인재 육성 프로그램 참가자 모집\n",
      "✅ 저장 완료: '마트 대못' 13년째… 전통시장 상인도 \"마트 휴업일 의미 없다\"\n",
      "✅ 저장 완료: 동국제강 첫 셧다운…최악 경기부진에 전기료까지 급등[Pick코노미]\n",
      "✅ 저장 완료: 시장마저 무너뜨린 '마트 대못' 13년\n",
      "✅ 저장 완료: “보안 인재 육성” 두나무 ‘업사이드 아카데미’ 3기 모집\n",
      "✅ 저장 완료: 시장 상인 \"두부 한모도 배달시키는데…마트 규제 의미 없어\"\n",
      "✅ 저장 완료: 최악 경기부진에 전기료까지 급등…\"추경 집행이라도 서둘러야\"\n",
      "✅ 저장 완료: 국회서 ‘생성형 AI와 뉴스 저작권’ 간담회 28일 열린다\n",
      "✅ 저장 완료: 제목 없음\n",
      "✅ 저장 완료: 한국공인회계사회, 제18회 지속가능성인증포럼 개최\n",
      "✅ 저장 완료: 오석환 차관, 유보통합 위한 디지털 기반 구축 업무협약 체결\n",
      "✅ 저장 완료: 오석환 차관, 유보통합 위한 디지털 기반 구축 업무협약 체결\n",
      "✅ 저장 완료: 두나무, 보안 인재 양성 '업사이드 아카데미' 3기 모집\n",
      "✅ 저장 완료: \n",
      "✅ 저장 완료: 성남시민이면 세종사이버大 등 4곳서 수업료 최대 30% 감면\n",
      "✅ 저장 완료: 오석환 차관, 유보통합 위한 디지털 기반 구축 업무협약 체결\n",
      "✅ 저장 완료: 세종대, 애니메이션 제작사 '스튜디오 미르' 견학 프로그램 운영\n",
      "✅ 저장 완료: 교육부, BK21 우주 분야 4개 연구단 추가 선정…매년 54억 지원\n",
      "✅ 저장 완료: '4단계 BK21 혁신인재 양성사업' 우주분야 4곳 추가 선정\n",
      "✅ 저장 완료: [부고]손필훈(고용노동부 고용서비스정책관)씨 모친상\n",
      "✅ 저장 완료: AI·센서·IoT·네트워크·엣지 컴퓨팅·데이터 융합 필요한 엠비언트 AI\n",
      "✅ 저장 완료: [부고] 손필훈(고용노동부 고용서비스정책관) 씨 모친상\n",
      "✅ 저장 완료: 시키지 않아도…알아서 예측해 행동하는 AI를 주목하라\n",
      "✅ 저장 완료: 엔비디아, K특허 침해했나? \"수천억 로열티 수익 가능\"[알짜배기 지식재산]\n",
      "✅ 저장 완료: 우주 분야 인재 키운다…교육부, 신규 교육연구단 4개 예비 선정\n",
      "✅ 저장 완료: 성남시, 사이버대 4곳과 협력…시민 수업료 최대 30% 감면\n",
      "✅ 저장 완료: 교육부, BK21 우주 분야 4개 연구단 추가 선정…매년 54억 지원\n",
      "✅ 저장 완료: ‘숏폼 스토리텔링’ 와이낫미디어 이민석 대표 발제…28일 8차 코카프(KOCAF) 포럼 개최\n",
      "✅ 저장 완료: 전국 78개 시험장서 ‘TOPCIT 제23회 정기평가’ 동시 실시\n",
      "✅ 저장 완료: 성남시민이라면 '사이버대' 4곳 수업료 최대 30% 감면\n",
      "✅ 저장 완료: 제목 없음\n",
      "✅ 저장 완료: “1500원 간다며”…주춤하는 달러화, 지금 사두는 게 이득일까\n",
      "✅ 저장 완료: 소프트웨어 토익 ‘TOPCIT’…정기평가 7586명 응시\n",
      "✅ 저장 완료: 세종대, '상반기 채용 릴레이 특강' 운영\n",
      "✅ 저장 완료: 새 정부 출범 코앞 터진 ‘주한미군 감축론’…한미동맹 시험대\n",
      "✅ 저장 완료: “기업 이익 개선 없으면 증시 부양도 없다” 대선 후보 모두 놓친 증시 부양책 [투자360]\n",
      "✅ 저장 완료: [오늘의 주요일정]사회(5월25일 일요일)\n",
      "✅ 저장 완료: [오늘의 주요일정]사회(5월24일 토요일)\n",
      "✅ 저장 완료: 불황에 ‘실탄’ 마른다…기업 보유현금 급감\n",
      "✅ 저장 완료: 박우찬 세종대 교수, 차세대 GPU 기술 개발...주요 외신들 주목\n",
      "✅ 저장 완료: \"정책·집행 분산돼 비효율적, 금융감독 한 기관에 모아야\"\n",
      "✅ 저장 완료: \"독립기구 신설\" \"내부조직 강화\"…금융소비자보호 기능 놓고 갈려\n",
      "✅ 저장 완료: [오늘의 정치일정] 5월 22일(목)\n",
      "✅ 저장 완료: 광고주협회, 굿데이터코퍼레이션과 업무협약\n",
      "✅ 저장 완료: 세종대 박우찬 교수, 레이트레이싱 GPU 기술로 국제적 주목\n",
      "✅ 저장 완료: IB 고등교육 포럼, '중·고 교육 통합 협력 방안' 머리 맞댔다\n",
      "✅ 저장 완료: 사회탐구 응시자 10만명 늘어…\"'사탐런', 올해 입시 중대 변수\"\n",
      "✅ 저장 완료: 세종대, '2025 상반기 채용 릴레이 특강' 운영\n",
      "❌ 크롤링 중 예외 발생: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "✅ 저장 완료: 과기정통부, 혁신도전형 R&D 관리자들과 정책방향 논의\n",
      "✅ 저장 완료: [알짜배기 지식재산]저작물 등록 사상 첫 감소…\"투자 위축, 불황 때문\"\n",
      "✅ 저장 완료: [2025 금융비전포럼-이모저모3] 대선 정국서 다시 불붙은 '상법 개정안'\n",
      "✅ 저장 완료: 논란 딛고 완판…삼성SDI 유상증자, 조직 신뢰로 순항\n",
      "✅ 저장 완료: [2025 금융비전포럼-토론종합] 상법 개정, 찬반 토론…\"경영 위축 부작용 vs 기업 신뢰 회복\"\n",
      "✅ 저장 완료: 불황에 ‘비상실탄’ 마른다…10대 기업 중 8곳 보유현금 감소\n",
      "✅ 저장 완료: \"추격에서 선도하는 R&D로\"…범부처 R&D 제도 개선 모색\n",
      "✅ 저장 완료: 세종사이버대 바리스타·소믈리에학과, 안동 '진맥소주’ 견학\n",
      "✅ 저장 완료: [오늘의 주요일정]정치(5월22일 목요일)\n",
      "✅ 저장 완료: 주제토론하는 김대종 세종대 경영학부 교수\n",
      "✅ 저장 완료: 서울시립대, ‘2025 자율주행 로봇레이스 1차 대회’ 대상\n",
      "✅ 저장 완료: 2025 글로벌 금융비전 포럼 주제토론\n",
      "✅ 저장 완료: [2025 금융비전포럼] 참석해 축하해 주신 분들\n",
      "✅ 저장 완료: [2025 금융비전포럼-토론] 김대종 세종대 교수 \"상법 개정안, 해외 투기자본에게 경영권 반납하는 것\"\n",
      "✅ 저장 완료: 피싱? 수사기관 경고?…헷갈리는 문자 통보 어쩌죠?\n",
      "✅ 저장 완료: 故이희중 작가 회고전 세종뮤지엄갤러리서 열려\n",
      "✅ 저장 완료: 배·반도체·밥을 짓는 사람들\n",
      "❌ 크롤링 중 예외 발생: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "✅ 저장 완료: 세종대, ‘서울마이칼리지’ 운영기관 선정\n",
      "✅ 저장 완료: 세종사이버대, 국내 첫 '메타버스 캠퍼스'…군인·경찰·소방관 입학금 면제\n",
      "✅ 저장 완료: 서울시, 서울소재 35개 대학에 765억 지원\n",
      "✅ 저장 완료: 보건의료정보관리사 탄생 40주년…건강정보 안전 관리 '철저'\n",
      "✅ 저장 완료: 서울시, ‘RISE’ 사업수행 35개 대학 최종 선정…“지역·대학 동반 성장”\n",
      "✅ 저장 완료: 보훈부, 재난안전체험교육 진행…본부 직원 100여명 참가\n",
      "✅ 저장 완료: 한국국제경영학회 춘계학술대회, 오는 23일 개최…‘K-글로벌 경영의 과거, 현재 그리고 미래’ 주제로\n",
      "✅ 저장 완료: '지브리 화풍' 챗GPT가 저작권 위반?…\"韓선 부정경쟁방지법 위반 소지\"\n",
      "✅ 저장 완료: 서울 라이즈 사업에 35개 대학 선정…국비·시비 765억 지원\n",
      "✅ 저장 완료: \n",
      "✅ 저장 완료: 세종대, '2025 상반기 면접 역량 강화 프로그램' 운영\n",
      "✅ 저장 완료: \"챗GPT '지브리'풍 이미지, 韓 부정경쟁방지법 위법 가능성\"\n",
      "✅ 저장 완료: 발명을 통한 창의·혁신의 실현 보여주고 미래 예측까지\n",
      "✅ 저장 완료: 재외동포 언론인, 국제심포지엄서 글로벌 K-언론 발전과제 논의\n",
      "✅ 저장 완료: [사고] 데일리안 '2025 글로벌 금융비전 포럼', 5월21일 여의도 CCMM빌딩 개최\n",
      "✅ 저장 완료: 세종대 미래교육원, '2025 서울마이칼리지' 운영기관 선정\n",
      "✅ 저장 완료: ‘제6회 섬의 날’ 홍보대사에 위하준, 하현우, 안성훈, 트리플에스, 오세득, 정지선 위촉\n",
      "✅ 저장 완료: \"외국계 취업, 현장에서 길을 찾다\" 세종대, GTF 박람회 참가\n",
      "✅ 저장 완료: \"영업이익률 25%라니\" 황제주 등극 삼양식품…\"불닭 성공, 운 아니다\"\n",
      "✅ 저장 완료: [오늘의 정치일정] 5월 19일(월)\n",
      "✅ 저장 완료: 용인특례시, '제2회 대한민국 대학연극제' 본선 진출 12개 대학팀 최종 선발\n",
      "✅ 저장 완료: 용인시, '제2회 대한민국 대학연극제' 본선 진출팀 선발\n",
      "✅ 저장 완료: 한국을 '베이징에 가장 가까이 있는 항공모함'으로 인식하는 트럼프 행정부\n",
      "✅ 저장 완료: 용인시 '제2회 대한민국 대학연극제' 본선 진출 12팀 확정\n",
      "✅ 저장 완료: [오늘의 주요일정]사회(5월19일 월요일)\n",
      "✅ 저장 완료: [오늘의 주요일정]정치(5월19일 월요일)\n",
      "✅ 저장 완료: [오늘의 주요일정]사회(5월18일 일요일)\n",
      "✅ 저장 완료: 민주 선대위 호사카 유지 \"김문수 뉴라이트 망언은 '매국'\"\n",
      "✅ 저장 완료: [오늘의 주요일정]사회(5월17일 토요일)\n",
      "✅ 저장 완료: 시민단체 \"민주주의 위기 불평등에 있어...해결책은 복지국가\"\n",
      "✅ 저장 완료: 중구, 2026학년도 대학별 입학설명회 개최…17개 대학 참여\n",
      "✅ 저장 완료: [오늘의 주요일정]사회(5월16일 금요일)\n",
      "✅ 저장 완료: 가장 부실한 사립대학 어디?…사교련, 38개 대학 진단 평가 공개\n",
      "✅ 저장 완료: 세종대 위정민 교수, 세라믹팔레스홀에서 초청 독창회 개최\n",
      "✅ 저장 완료: 세종대 학술정보원, '제3회 학정포럼' 개최\n",
      "✅ 저장 완료: \"법인 평가 없이 사립대 발전 없다\"…38개 대학 진단 결과(종합)\n",
      "✅ 저장 완료: \n",
      "✅ 저장 완료: \"AI 시대, 무엇이 살아남을까\" 세종대 학정포럼, 직업의 미래 조망\n",
      "✅ 저장 완료: 한국공인회계사회, 오는 21일 '제18회 지속가능성인증포럼' 개최\n",
      "✅ 저장 완료: 한국공인회계사회, 오는 21일 ‘지속가능성인증포럼’ 개최\n",
      "✅ 저장 완료: 김민석, 호사카 유지 교수와 유세…“김문수, 왜곡된 역사관”\n",
      "✅ 저장 완료: 세종대 물리천문학과 교수진, '2025 브레이크스루상' 공동 수상\n",
      "✅ 저장 완료: 한국재무관리학회 “국민연금, 사업보국 이루어질 수 있는 방향으로 의결권 행사해야”\n",
      "✅ 저장 완료: 서울 중구, 2026학년도 대학별 입학설명회 개최\n",
      "✅ 저장 완료: 민주, 김문수에 \"5·18 정신 헌법수록 공약해야\"…호사카 교수 영입도\n",
      "✅ 저장 완료: 세종대, '바이오·제약 HPLC 교육 프로그램' 운영\n",
      "✅ 저장 완료: [오늘의 주요일정]사회(5월15일 목요일)\n",
      "✅ 저장 완료: 김민석, 호사카 유지 교수와 유세…\"김문수, 왜곡된 역사관\"\n",
      "✅ 저장 완료: 세종대 물리천문학과 교수진, '브레이크스루상' 공동 수상\n",
      "✅ 저장 완료: 세종대, '컴퓨터활용능력 1급 대비반' 운영\n",
      "✅ 저장 완료: 서울 중구 '대학별 입학설명회' 3차례 개최…17개 대학 참여\n",
      "\n",
      "✅ 저장 요약\n",
      "📰 연합뉴스 기사 총 10건 Supabase test 테이블에 저장 완료\n",
      "📰 뉴시스 기사 총 33건 Supabase test 테이블에 저장 완료\n",
      "📰 머니투데이 기사 총 8건 Supabase test 테이블에 저장 완료\n",
      "📰 헤럴드경제 기사 총 8건 Supabase test 테이블에 저장 완료\n",
      "📰 서울경제 기사 총 7건 Supabase test 테이블에 저장 완료\n",
      "📰 뉴스핌 기사 총 8건 Supabase test 테이블에 저장 완료\n",
      "📰 데일리안 기사 총 12건 Supabase test 테이블에 저장 완료\n",
      "📰 매일경제 기사 총 6건 Supabase test 테이블에 저장 완료\n",
      "📰 아시아경제 기사 총 7건 Supabase test 테이블에 저장 완료\n",
      "📰 노컷뉴스 기사 총 3건 Supabase test 테이블에 저장 완료\n",
      "📰 이데일리뉴스 기사 총 12건 Supabase test 테이블에 저장 완료\n",
      "📰 경인일보 기사 총 1건 Supabase test 테이블에 저장 완료\n",
      "📰 서울신문 기사 총 4건 Supabase test 테이블에 저장 완료\n",
      "\n",
      "✅ 저장 요약을 '세종대_news_save_summary.txt' 파일로 저장했습니다.\n",
      "총 실행 시간: 22.60초\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import urllib.parse\n",
    "from urllib.parse import urlparse\n",
    "from bs4 import BeautifulSoup\n",
    "from supabase import create_client, Client\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import time\n",
    "\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "# ✅ Supabase 설정\n",
    "SUPABASE_URL = \"https://ypyujiaoeaqykbqetjef.supabase.co\"\n",
    "SUPABASE_KEY = \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6InlweXVqaWFvZWFxeWticWV0amVmIiwicm9sZSI6ImFub24iLCJpYXQiOjE3NDY1NDUyNTQsImV4cCI6MjA2MjEyMTI1NH0.RuR9l89gxCcMkSzO053EHluQ0ers-piN4SUjZ-LtWjU\"\n",
    "supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)\n",
    "\n",
    "\n",
    "## -----------------언론사 크롤러 코드 시작!!!! 여기부터 안 건드려도 돼요--------------------\n",
    "def crawl_yonhap_news(url):\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        return None\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    title_tag = soup.find('h1', class_='tit01')\n",
    "    title = title_tag.get_text(strip=True) if title_tag else \"제목 없음\"\n",
    "\n",
    "    body_tag = soup.find('div', class_='story-news article')\n",
    "    if body_tag:\n",
    "        paragraphs = body_tag.find_all('p')\n",
    "        body_lines = []\n",
    "        for p in paragraphs:\n",
    "            if 'txt-copyright' in p.get('class', []): continue\n",
    "            text = p.get_text(strip=True)\n",
    "            if text:\n",
    "                body_lines.append(text)\n",
    "        body = '\\n'.join(body_lines)\n",
    "    else:\n",
    "        body = \"본문 없음\"\n",
    "\n",
    "    return {\"title\": title, \"url\": url, \"body\": body, \"media\": \"연합뉴스\"}\n",
    "\n",
    "\n",
    "def crawl_newsis_news(url):\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        return None\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    title_tag = soup.select_one('h1.tit.title_area')\n",
    "    title = title_tag.get_text(strip=True) if title_tag else \"제목 없음\"\n",
    "\n",
    "    body_tag = soup.find('div', class_='viewer')\n",
    "    if body_tag:\n",
    "        raw_text = body_tag.get_text(separator=\"\\n\").strip()\n",
    "        body_lines = [line.strip() for line in raw_text.split(\"\\n\") if line.strip()]\n",
    "        body = '\\n'.join(body_lines)\n",
    "    else:\n",
    "        body = \"본문 없음\"\n",
    "\n",
    "    return {\"title\": title, \"url\": url, \"body\": body, \"media\": \"뉴시스\"}\n",
    "\n",
    "\n",
    "def crawl_mt_news(url):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0'\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        print(\"❌ 기사 요청 실패:\", response.status_code)\n",
    "        return None\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    title_tag = soup.find('h1', class_=['subject'])\n",
    "    title = title_tag.get_text(strip=True) if title_tag else \"제목 없음\"\n",
    "\n",
    "    body_tag = soup.find('div', class_='view_text')\n",
    "\n",
    "    if body_tag:\n",
    "        raw_text = body_tag.get_text(separator=\"\\n\").strip()\n",
    "        body_lines = [line.strip() for line in raw_text.split(\"\\n\") if line.strip()]\n",
    "        body = '\\n'.join(body_lines)\n",
    "    else:\n",
    "        body = \"본문 없음\"\n",
    "\n",
    "    return {\n",
    "        \"title\": title,\n",
    "        \"url\": url,\n",
    "        \"body\": body,\n",
    "        \"media\": \"머니투데이\"\n",
    "    }\n",
    "\n",
    "\n",
    "def crawl_heraldcorp_news(url):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0'\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        print(\"❌ 기사 요청 실패:\", response.status_code)\n",
    "        return None\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    title = soup.select_one('div.news_title > h1')\n",
    "    title = title.get_text(strip=True) if title else \"제목 없음\"\n",
    "\n",
    "    body_tag = soup.find('article', id='articleText')\n",
    "\n",
    "    if body_tag:\n",
    "        raw_text = \"\\n\".join([p.get_text(strip=True) for p in body_tag.find_all('p')]).strip()\n",
    "        body_lines = [line.strip() for line in raw_text.split(\"\\n\") if line.strip()]\n",
    "        body = '\\n'.join(body_lines)\n",
    "    else:\n",
    "        body = \"본문 없음\"\n",
    "\n",
    "    return {\n",
    "        \"title\": title,\n",
    "        \"url\": url,\n",
    "        \"body\": body,\n",
    "        \"media\": \"헤럴드경제\"\n",
    "    }\n",
    "\n",
    "\n",
    "def crawl_sedaily_news(url):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0'\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        print(\"❌ 기사 요청 실패:\", response.status_code)\n",
    "        return None\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    title_tag = soup.find('h1', class_=['art_tit'])\n",
    "    title = title_tag.get_text(strip=True) if title_tag else \"제목 없음\"\n",
    "\n",
    "    body_tag = soup.find('div', class_='article_view')\n",
    "\n",
    "    if body_tag:\n",
    "        for fig in body_tag.find_all('figure', class_='art_photo'):\n",
    "            fig.decompose()\n",
    "\n",
    "        for br in body_tag.find_all('br'):\n",
    "            br.replace_with('\\n')\n",
    "\n",
    "        raw_text = body_tag.get_text(separator=\"\\n\").strip()\n",
    "        body_lines = [line.strip() for line in raw_text.split(\"\\n\") if line.strip()]\n",
    "        body = '\\n'.join(body_lines)\n",
    "    else:\n",
    "        body = \"본문 없음\"\n",
    "\n",
    "    return {\n",
    "        \"title\": title,\n",
    "        \"url\": url,\n",
    "        \"body\": body,\n",
    "        \"media\": \"서울경제\"\n",
    "    }\n",
    "\n",
    "\n",
    "def crawl_newspim_news(url):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0'\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        print(\"❌ 기사 요청 실패:\", response.status_code)\n",
    "        return None\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    title_tag = soup.find('h2')\n",
    "    title = title_tag.get_text(strip=True) if title_tag else \"제목 없음\"\n",
    "\n",
    "    body_tag = soup.find('div', class_='contents', itemprop='articleBody')\n",
    "\n",
    "    if body_tag:\n",
    "        raw_text = \"\\n\".join([p.get_text(strip=True) for p in body_tag.find_all('p')]).strip()\n",
    "        body_lines = [line.strip() for line in raw_text.split(\"\\n\") if line.strip()]\n",
    "        body = ''.join(body_lines)\n",
    "    else:\n",
    "        body = \"본문 없음\"\n",
    "\n",
    "    return {\n",
    "        \"title\": title,\n",
    "        \"url\": url,\n",
    "        \"body\": body,\n",
    "        \"media\": \"뉴스핌\"\n",
    "    }\n",
    "\n",
    "\n",
    "def crawl_dailian_news(url):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0'\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        print(\"❌ 기사 요청 실패:\", response.status_code)\n",
    "        return None\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    title_tag = soup.find('h1', class_=['title'])\n",
    "    title = title_tag.get_text(strip=True) if title_tag else \"제목 없음\"\n",
    "\n",
    "    body_tag = soup.find('div', class_='article')\n",
    "\n",
    "    if body_tag:\n",
    "        raw_text = \"\\n\".join([p.get_text(strip=True) for p in body_tag.find_all('p')]).strip()\n",
    "        body_lines = [line.strip() for line in raw_text.split(\"\\n\") if line.strip()]\n",
    "        body = ''.join(body_lines)\n",
    "    else:\n",
    "        body = \"본문 없음\"\n",
    "\n",
    "    return {\n",
    "        \"title\": title,\n",
    "        \"url\": url,\n",
    "        \"body\": body,\n",
    "        \"media\": \"데일리안\"\n",
    "    }\n",
    "\n",
    "\n",
    "def crawl_mk_news(url):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0'\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        print(\"❌ 기사 요청 실패:\", response.status_code)\n",
    "        return None\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    title_tag = soup.find('h2', class_=['news_ttl'])\n",
    "    title = title_tag.get_text(strip=True) if title_tag else \"제목 없음\"\n",
    "    \n",
    "    body_tag = soup.find('div', class_='news_cnt_detail_wrap', itemprop='articleBody')\n",
    "\n",
    "    if body_tag:\n",
    "        p_texts = [p.get_text(strip=True) for p in body_tag.find_all('p')]\n",
    "        p_texts = [text for text in p_texts if text]\n",
    "\n",
    "        if p_texts:\n",
    "            body = ''.join(p_texts)\n",
    "        else:\n",
    "            br_texts = [str(t).strip() for t in body_tag.children if t and str(t).strip() and not getattr(t, 'name', None)]\n",
    "            br_texts = [text for text in br_texts if text]\n",
    "            if br_texts:\n",
    "                body = ''.join(br_texts)\n",
    "            else:\n",
    "                body = \"본문 없음\"\n",
    "    else:\n",
    "        body = \"본문 없음\"\n",
    "        \n",
    "    return {\n",
    "        \"title\": title,\n",
    "        \"url\": url,\n",
    "        \"body\": body,\n",
    "        \"media\": \"매일경제\"\n",
    "    }\n",
    "\n",
    "\n",
    "def crawl_asiae_news(url):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0'\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        print(\"❌ 기사 요청 실패:\", response.status_code)\n",
    "        return None\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    title_tag = soup.find('h1')\n",
    "    title = title_tag.get_text(strip=True) if title_tag else \"제목 없음\"\n",
    "    \n",
    "    body_tag = soup.find('div', class_='article')\n",
    "\n",
    "    if body_tag:\n",
    "        p_tags = [p for p in body_tag.find_all('p') if 'txt_prohibition' not in p.get('class', [])]\n",
    "\n",
    "        raw_text = \"\\n\".join([p.get_text(strip=True) for p in p_tags]).strip()\n",
    "\n",
    "        body_lines = [line.strip() for line in raw_text.split(\"\\n\") if line.strip()]\n",
    "\n",
    "        body = ''.join(body_lines)\n",
    "    else:\n",
    "        body = \"본문 없음\"\n",
    "\n",
    "    return {\n",
    "        \"title\": title,\n",
    "        \"url\": url,\n",
    "        \"body\": body,\n",
    "        \"media\": \"아시아경제\"\n",
    "    }\n",
    "    \n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from supabase import create_client, Client\n",
    "\n",
    "#Supabase 설정\n",
    "SUPABASE_URL = \"https://ypyujiaoeaqykbqetjef.supabase.co\"\n",
    "SUPABASE_KEY = \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6InlweXVqaWFvZWFxeWticWV0amVmIiwicm9sZSI6ImFub24iLCJpYXQiOjE3NDY1NDUyNTQsImV4cCI6MjA2MjEyMTI1NH0.RuR9l89gxCcMkSzO053EHluQ0ers-piN4SUjZ-LtWjU\"\n",
    "supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)\n",
    "\n",
    "#노컷뉴스 크롤링 함수\n",
    "def crawl_nocut_news(url):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0'\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        print(\"❌ 기사 요청 실패:\", response.status_code)\n",
    "        return None\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # 제목 추출\n",
    "    title_tag = soup.find('h2')\n",
    "    title = title_tag.get_text(strip=True) if title_tag else \"제목 없음\"\n",
    "    \n",
    "    body_tag = soup.find('div', id='pnlContent')  # id 속성으로 본문 div를 찾음\n",
    "\n",
    "    if body_tag:\n",
    "        for br in body_tag.find_all(\"br\"):\n",
    "            br.replace_with(\"\")  # br 태그 삭제, 줄바꿈 없이 이어붙임\n",
    "\n",
    "        raw_text = body_tag.get_text(strip=True)\n",
    "        # 불필요한 빈 줄 제거 및 공백 정리\n",
    "        body_lines = [line.strip() for line in raw_text.split(\"\\n\") if line.strip()]\n",
    "        body = \"\\n\".join(body_lines)\n",
    "    else:\n",
    "        body = \"본문 없음\"\n",
    "\n",
    "    return {\n",
    "        \"title\": title,\n",
    "        \"url\": url,\n",
    "        \"body\": body,\n",
    "        \"media\": \"노컷뉴스\"\n",
    "    }\n",
    "    \n",
    "def crawl_edaily_news(url):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0'\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        print(\"❌ 기사 요청 실패:\", response.status_code)\n",
    "        return None\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # 제목 추출\n",
    "    title_tag = soup.find('h1')\n",
    "    title = title_tag.get_text(strip=True) if title_tag else \"제목 없음\"\n",
    "    \n",
    "    body_tag = soup.find('div', class_='news_body', itemprop='articleBody')\n",
    "\n",
    "    if body_tag:\n",
    "    # 부수 요소 제거\n",
    "        for tag_to_remove in body_tag.find_all(['table', 'div'], class_=['gg_textshow']):\n",
    "            tag_to_remove.decompose()  # 태그 자체 삭제\n",
    "\n",
    "    # <br> 태그는 줄바꿈 문자로 변환\n",
    "        for br in body_tag.find_all(\"br\"):\n",
    "            br.replace_with(\"\\n\")\n",
    "\n",
    "        raw_text = body_tag.get_text(separator=\"\\n\", strip=True)\n",
    "\n",
    "    # 빈 줄 제거 및 공백 정리\n",
    "        body_lines = [line.strip() for line in raw_text.split(\"\\n\") if line.strip()]\n",
    "        body = \"\\n\".join(body_lines)\n",
    "        \n",
    "        body = body.replace('\\n', '').replace('\\r', '')\n",
    "    else:\n",
    "        body = \"본문 없음\"\n",
    "\n",
    "    return {\n",
    "        \"title\": title,\n",
    "        \"url\": url,\n",
    "        \"body\": body,\n",
    "        \"media\": \"이데일리\"\n",
    "    }\n",
    "    \n",
    "#경인일보 크롤링 함수\n",
    "def crawl_kyeongin_news(url):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0'\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        print(\"❌ 기사 요청 실패:\", response.status_code)\n",
    "        return None\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # 제목 추출\n",
    "    title_tag = soup.find('h1')\n",
    "    title = title_tag.get_text(strip=True) if title_tag else \"제목 없음\"\n",
    "    \n",
    "    body_tag = soup.find('div', class_='article-body')  # 혹은 id='article-body'\n",
    "\n",
    "    if body_tag:\n",
    "        # 광고용 div 등 불필요한 요소 제거: id가 'svcad_'로 시작하는 div 제거\n",
    "        for ad_div in body_tag.find_all('div'):\n",
    "            if ad_div.get('id') and ad_div['id'].startswith('svcad_'):\n",
    "                ad_div.decompose()\n",
    "\n",
    "        # table, 특정 클래스 div 제거 (필요시 추가)\n",
    "        for tag_to_remove in body_tag.find_all(['table', 'div'], class_=['gg_textshow']):\n",
    "            tag_to_remove.decompose()\n",
    "\n",
    "        # <br> 태그를 줄바꿈으로 변환\n",
    "        for br in body_tag.find_all('br'):\n",
    "            br.replace_with('\\n')\n",
    "\n",
    "        raw_text = body_tag.get_text(separator='\\n', strip=True)\n",
    "\n",
    "        # 빈 줄 제거 및 공백 정리\n",
    "        body_lines = [line.strip() for line in raw_text.split(\"\\n\") if line.strip()]\n",
    "        body = \"\\n\".join(body_lines)\n",
    "        \n",
    "        body = body.replace('\\n', '').replace('\\r', '')\n",
    "\n",
    "    else:\n",
    "        body = \"본문 없음\"\n",
    "   \n",
    "    return {\n",
    "        \"title\": title,\n",
    "        \"url\": url,\n",
    "        \"body\": body,\n",
    "        \"media\": \"경인일보\"\n",
    "    }\n",
    "\n",
    "def crawl_seoul_news(url):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0'\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, headers=headers)\n",
    "\n",
    "    # 인코딩 강제 지정 (utf-8 또는 euc-kr 둘 중 하나 시도)\n",
    "    response.encoding = 'utf-8'  # 또는 'euc-kr'\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        print(\"❌ 기사 요청 실패:\", response.status_code)\n",
    "        return None\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # 제목 추출\n",
    "    title_tag = soup.find('h1')\n",
    "    title = title_tag.get_text(strip=True) if title_tag else \"제목 없음\"\n",
    "    \n",
    "    body_tag = soup.find('div', class_='viewContent body18 color700')\n",
    "\n",
    "    if body_tag:\n",
    "        # 광고 div 제거 (예: id가 svcad_로 시작하는 div)\n",
    "        for ad_div in body_tag.find_all('div'):\n",
    "            if ad_div.get('id') and ad_div['id'].startswith('svcad_'):\n",
    "                ad_div.decompose()\n",
    "\n",
    "        # 불필요한 태그 제거 (필요 시 추가)\n",
    "        for tag_to_remove in body_tag.find_all(['table', 'div'], class_=['gg_textshow']):\n",
    "            tag_to_remove.decompose()\n",
    "\n",
    "        # <br> 태그를 줄바꿈 문자로 대체\n",
    "        for br in body_tag.find_all('br'):\n",
    "            br.replace_with('\\n')\n",
    "\n",
    "        # 텍스트 추출 및 빈 줄 제거\n",
    "        raw_text = body_tag.get_text(separator='\\n', strip=True)\n",
    "        body_lines = [line.strip() for line in raw_text.split('\\n') if line.strip()]\n",
    "        body = '\\n'.join(body_lines)\n",
    "        body = body.replace('\\n', '').replace('\\r', '')\n",
    "\n",
    "    else:\n",
    "        body = \"본문 없음\"\n",
    "\n",
    "    return {\n",
    "        \"title\": title,\n",
    "        \"url\": url,\n",
    "        \"body\": body,\n",
    "        \"media\": \"서울신문\"\n",
    "    }\n",
    "    \n",
    "def crawl_fn_news(url):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0'\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, headers=headers)\n",
    "\n",
    "# 인코딩 강제 지정 (utf-8 또는 euc-kr 둘 중 하나 시도)\n",
    "    response.encoding = 'utf-8'  # 또는 'euc-kr'\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        print(\"❌ 기사 요청 실패:\", response.status_code)\n",
    "        return None\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # 제목 추출\n",
    "    title_tag = soup.find('h1')\n",
    "    title = title_tag.get_text(strip=True) if title_tag else \"제목 없음\"\n",
    "    \n",
    "    body_tag = soup.find('div', class_='cont_view', id='article_content')\n",
    "\n",
    "    if body_tag:\n",
    "        # 광고 div 제거 (필요 시 조건 추가)\n",
    "        for ad_div in body_tag.find_all('div'):\n",
    "            if ad_div.get('id') and ad_div['id'].startswith('svcad_'):\n",
    "                ad_div.decompose()\n",
    "\n",
    "        # 부수 요소 제거 (필요하면 더 추가 가능)\n",
    "        for tag_to_remove in body_tag.find_all(['table', 'div'], class_=['gg_textshow']):\n",
    "            tag_to_remove.decompose()\n",
    "\n",
    "        # <br> 태그를 줄바꿈 문자로 대체\n",
    "        for br in body_tag.find_all('br'):\n",
    "            br.replace_with('\\n')\n",
    "\n",
    "        # 텍스트 추출 및 빈 줄 제거\n",
    "        raw_text = body_tag.get_text(separator='\\n', strip=True)\n",
    "        body_lines = [line.strip() for line in raw_text.split('\\n') if line.strip()]\n",
    "        body = '\\n'.join(body_lines)\n",
    "        body = body.replace('\\n', '').replace('\\r', '')\n",
    "\n",
    "    else:\n",
    "        body = \"본문 없음\"\n",
    "\n",
    "    return {\n",
    "        \"title\": title,\n",
    "        \"url\": url,\n",
    "        \"body\": body,\n",
    "        \"media\": \"파이낸셜뉴스\"\n",
    "    }\n",
    "\n",
    "## -----------------언론사 크롤러 코드 끝!!!! 여기까지 안 건드려도 돼요--------------------\n",
    "\n",
    "\n",
    "# ✅ Supabase 저장 함수 (수정: 'test' 테이블, keyword 컬럼 추가)\n",
    "def save_to_supabase(data, keyword, log_path=\"save_log.txt\"):\n",
    "    try:\n",
    "        # url과 keyword가 같은 데이터가 이미 있는지 확인\n",
    "        existing = supabase.table(\"test\").select(\"id\").eq(\"url\", data[\"url\"]).eq(\"keyword\", keyword).execute()\n",
    "        if existing.data:\n",
    "            msg = f\"⚠️ 이미 저장된 기사: {data['url']}\"\n",
    "            print(msg)\n",
    "            with open(log_path, \"a\", encoding=\"utf-8\") as f:\n",
    "                f.write(msg + \"\\n\")\n",
    "            return False\n",
    "\n",
    "        data_with_keyword = data.copy()\n",
    "        data_with_keyword[\"keyword\"] = keyword\n",
    "\n",
    "        supabase.table(\"test\").insert([data_with_keyword]).execute()\n",
    "        msg = f\"✅ 저장 완료: {data['title']}\"\n",
    "        print(msg)\n",
    "        with open(log_path, \"a\", encoding=\"utf-8\") as f:\n",
    "            f.write(msg + \"\\n\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        msg = f\"❌ 저장 실패: {e}\"\n",
    "        print(msg)\n",
    "        with open(log_path, \"a\", encoding=\"utf-8\") as f:\n",
    "            f.write(msg + \"\\n\")\n",
    "        return False\n",
    "\n",
    "\n",
    "# 언론사 도메인 → 크롤링 함수 매핑\n",
    "CRAWLER_FUNCTION_MAP = {\n",
    "    \"www.yna.co.kr\": crawl_yonhap_news,\n",
    "    \"www.newsis.com\": crawl_newsis_news,\n",
    "    \"news.mt.co.kr\" : crawl_mt_news,\n",
    "    \"biz.heraldcorp.com\" : crawl_heraldcorp_news,\n",
    "    \"www.sedaily.com\" : crawl_sedaily_news,\n",
    "    \"www.newspim.com\" : crawl_newspim_news,\n",
    "    \"www.dailian.co.kr\" : crawl_dailian_news,\n",
    "    \"www.mk.co.kr\" : crawl_mk_news,\n",
    "    \"view.asiae.co.kr\" : crawl_asiae_news,\n",
    "    \"www.nocutnews.co.kr\" : crawl_nocut_news,\n",
    "    \"www.edaily.co.kr\" : crawl_edaily_news,\n",
    "    \"www.kyeongin.com\" : crawl_kyeongin_news,\n",
    "    \"www.seoul.co.kr\" : crawl_seoul_news,\n",
    "    \"www.fnnews.com\" : crawl_fn_news,\n",
    "}\n",
    "\n",
    "# 언론사 도메인 → 언론사 이름 매핑\n",
    "MEDIA_NAME_MAP = {\n",
    "    \"www.yna.co.kr\": \"연합뉴스\",\n",
    "    \"www.newsis.com\": \"뉴시스\",\n",
    "    \"news.mt.co.kr\" : \"머니투데이\",\n",
    "    \"biz.heraldcorp.com\" : \"헤럴드경제\",\n",
    "    \"www.sedaily.com\" : \"서울경제\",\n",
    "    \"www.newspim.com\" : \"뉴스핌\",\n",
    "    \"www.dailian.co.kr\" : \"데일리안\",\n",
    "    \"www.mk.co.kr\" : \"매일경제\",\n",
    "    \"view.asiae.co.kr\" : \"아시아경제\",\n",
    "    \"www.nocutnews.co.kr\" : \"노컷뉴스\",\n",
    "    \"www.edaily.co.kr\" : \"이데일리뉴스\",\n",
    "    \"www.kyeongin.com\" : \"경인일보\",\n",
    "    \"www.seoul.co.kr\" : \"서울신문\",\n",
    "    \"www.fnnews.com\" : \"파이낸셜뉴스\",\n",
    "\n",
    "    \n",
    "}\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "def save_articles_from_naver_parallel(query, max_workers=10):  # 병렬처리 시도\n",
    "    client_id = \"_TznE38btYhyzWYsq9XK\"\n",
    "    client_secret = \"06UYVlSHF9\"\n",
    "\n",
    "    encoded_query = urllib.parse.quote(query)\n",
    "    headers = {\n",
    "        \"X-Naver-Client-Id\": client_id,\n",
    "        \"X-Naver-Client-Secret\": client_secret\n",
    "    }\n",
    "\n",
    "    display = 100\n",
    "    saved_count_by_domain = {domain: 0 for domain in CRAWLER_FUNCTION_MAP.keys()}\n",
    "\n",
    "    for start in range(1, 1000 + 1, display):\n",
    "        url = f\"https://openapi.naver.com/v1/search/news.json?query={encoded_query}&display={display}&start={start}&sort=date\"\n",
    "        response = requests.get(url, headers=headers)\n",
    "\n",
    "        if response.status_code != 200:\n",
    "            print(f\"❌ 요청 실패 at start={start}: {response.status_code}\")\n",
    "            continue\n",
    "\n",
    "        data = response.json()\n",
    "        items = data.get(\"items\", [])\n",
    "        if not items:\n",
    "            break\n",
    "\n",
    "        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "            futures = []\n",
    "\n",
    "            for item in items:\n",
    "                originallink = item.get(\"originallink\", \"\")\n",
    "                domain = urlparse(originallink).netloc\n",
    "\n",
    "                if domain in CRAWLER_FUNCTION_MAP:\n",
    "                    futures.append(executor.submit(CRAWLER_FUNCTION_MAP[domain], originallink))\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "            for future in as_completed(futures):\n",
    "                try:\n",
    "                    article = future.result()\n",
    "                except Exception as e:\n",
    "                    print(f\"❌ 크롤링 중 예외 발생: {e}\")\n",
    "                    continue\n",
    "\n",
    "                if article:\n",
    "                    success = save_to_supabase(article, query)\n",
    "                    if success:\n",
    "                        domain = urlparse(article[\"url\"]).netloc\n",
    "                        saved_count_by_domain[domain] += 1\n",
    "\n",
    "        if len(items) < display:\n",
    "            break\n",
    "\n",
    "    # 1) 출력\n",
    "    print(\"\\n✅ 저장 요약\")\n",
    "    for domain, count in saved_count_by_domain.items():\n",
    "        media = MEDIA_NAME_MAP.get(domain, domain)\n",
    "        print(f\"📰 {media} 기사 총 {count}건 Supabase test 테이블에 저장 완료\")\n",
    "\n",
    "    # 2) 텍스트 파일로 저장\n",
    "    filename = f\"{query}_news_save_summary.txt\"\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(f\"검색어: {query}\\n\\n\")\n",
    "        f.write(\"언론사별 저장 건수 요약:\\n\")\n",
    "        for domain, count in saved_count_by_domain.items():\n",
    "            media = MEDIA_NAME_MAP.get(domain, domain)\n",
    "            f.write(f\"{media}: {count}건\\n\")\n",
    "\n",
    "    print(f\"\\n✅ 저장 요약을 '{filename}' 파일로 저장했습니다.\")\n",
    "\n",
    "# main 실행부 (input으로 검색어 받음)  \n",
    "if __name__ == \"__main__\":\n",
    "    search_keyword = input(\"검색어를 입력하세요: \").strip()\n",
    "    if search_keyword:\n",
    "        start_time = time.time()\n",
    "        save_articles_from_naver_parallel(search_keyword)\n",
    "        end_time = time.time()\n",
    "        print(f\"총 실행 시간: {end_time - start_time:.2f}초\")\n",
    "    else:\n",
    "        print(\"검색어가 입력되지 않았습니다.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "webenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
