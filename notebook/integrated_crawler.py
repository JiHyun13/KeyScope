import requests
import urllib.parse
from urllib.parse import urlparse
from bs4 import BeautifulSoup
from supabase import create_client, Client
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
import time

from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

# âœ… Supabase ì„¤ì •
SUPABASE_URL = "https://ypyujiaoeaqykbqetjef.supabase.co"
SUPABASE_KEY = "eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6InlweXVqaWFvZWFxeWticWV0amVmIiwicm9sZSI6ImFub24iLCJpYXQiOjE3NDY1NDUyNTQsImV4cCI6MjA2MjEyMTI1NH0.RuR9l89gxCcMkSzO053EHluQ0ers-piN4SUjZ-LtWjU"
supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)

# âœ… ì—°í•©ë‰´ìŠ¤ í¬ë¡¤ë§ í•¨ìˆ˜
def crawl_yonhap_news(url):
    headers = {'User-Agent': 'Mozilla/5.0'}
    response = requests.get(url, headers=headers)
    if response.status_code != 200:
        return None
    soup = BeautifulSoup(response.text, 'html.parser')
    title_tag = soup.find('h1', class_='tit01')
    title = title_tag.get_text(strip=True) if title_tag else "ì œëª© ì—†ìŒ"

    body_tag = soup.find('div', class_='story-news article')
    if body_tag:
        paragraphs = body_tag.find_all('p')
        body_lines = []
        for p in paragraphs:
            if 'txt-copyright' in p.get('class', []): continue
            text = p.get_text(strip=True)
            if text:
                body_lines.append(text)
        body = '\n'.join(body_lines)
    else:
        body = "ë³¸ë¬¸ ì—†ìŒ"

    return {"title": title, "url": url, "body": body, "media": "ì—°í•©ë‰´ìŠ¤"}

# âœ… ì¡°ì„ ì¼ë³´ í¬ë¡¤ë§ í•¨ìˆ˜
def crawl_chosun_news(url):
    options = webdriver.ChromeOptions()
    options.add_argument('--headless')
    options.add_argument('--no-sandbox')
    options.add_argument('--disable-dev-shm-usage')

    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)
    driver.get(url)
    time.sleep(3)

    soup = BeautifulSoup(driver.page_source, 'html.parser')
    driver.quit()

    title_tag = soup.find('h1')
    title = title_tag.get_text(strip=True) if title_tag else "ì œëª© ì—†ìŒ"

    body_paragraphs = soup.select('p.article-body__content.article-body__content-text')
    body_lines = []
    for p in body_paragraphs:
        text = p.get_text(strip=True)
        if not text:
            continue
        if any(keyword in text for keyword in ['ê¸°ì', 'ë¬´ë‹¨ ì „ì¬', 'êµ¬ë…', 'Copyright']):
            continue
        body_lines.append(text)
    body = '\n'.join(body_lines) if body_lines else "ë³¸ë¬¸ ì—†ìŒ"

    return {"title": title, "url": url, "body": body, "media": "ì¡°ì„ ì¼ë³´"}

# âœ… ë‰´ì‹œìŠ¤ í¬ë¡¤ë§ í•¨ìˆ˜
def crawl_newsis_news(url):
    headers = {'User-Agent': 'Mozilla/5.0'}
    response = requests.get(url, headers=headers)
    if response.status_code != 200:
        return None
    soup = BeautifulSoup(response.text, 'html.parser')

    title_tag = soup.select_one('h1.tit.title_area')
    title = title_tag.get_text(strip=True) if title_tag else "ì œëª© ì—†ìŒ"

    body_tag = soup.find('div', class_='viewer')
    if body_tag:
        raw_text = body_tag.get_text(separator="\n").strip()
        body_lines = [line.strip() for line in raw_text.split("\n") if line.strip()]
        body = '\n'.join(body_lines)
    else:
        body = "ë³¸ë¬¸ ì—†ìŒ"

    return {"title": title, "url": url, "body": body, "media": "ë‰´ì‹œìŠ¤"}


def crawl_news1_news(url):
    # âœ… ì…€ë ˆë‹ˆì›€ ì˜µì…˜ ì„¤ì •
    options = webdriver.ChromeOptions()
    options.add_argument('--headless')  # ì°½ ì—†ì´ ì‹¤í–‰
    options.add_argument('--no-sandbox')
    options.add_argument('--disable-dev-shm-usage')

    # âœ… ë“œë¼ì´ë²„ ì‹¤í–‰
    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)
    driver.get(url)

    # âœ… ëª…ì‹œì  ëŒ€ê¸°: ë³¸ë¬¸ì´ ë¡œë”©ë  ë•Œê¹Œì§€ ìµœëŒ€ 10ì´ˆ ëŒ€ê¸°
    try:
        WebDriverWait(driver, 10).until(
            EC.presence_of_element_located((By.ID, "articleBodyContent"))
        )
    except:
        print("â° ë¡œë”© ì‹¤íŒ¨ ë˜ëŠ” íƒ€ì„ì•„ì›ƒ ë°œìƒ")
        driver.quit()
        return None

    # âœ… í˜ì´ì§€ íŒŒì‹±
    soup = BeautifulSoup(driver.page_source, 'html.parser')
    driver.quit()

    # âœ… ì œëª© ì¶”ì¶œ
    title_tag = soup.select_one('h1.article-h2-header-title.mb-40')
    title = title_tag.get_text(strip=True) if title_tag else "ì œëª© ì—†ìŒ"

    # âœ… ë³¸ë¬¸ ì¶”ì¶œ
    body_tag = soup.find('div', id='articleBodyContent')
    if body_tag:
        paragraphs = body_tag.find_all('p')
        body_lines = [p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True)]
        body = '\n'.join(body_lines)
    else:
        body = "ë³¸ë¬¸ ì—†ìŒ"

    return {
        "title": title,
        "url": url,
        "body": body,
        "media": "ë‰´ìŠ¤1"
    }
    
# âœ… ë¨¸ë‹ˆíˆ¬ë°ì´ ë‰´ìŠ¤ í¬ë¡¤ë§ í•¨ìˆ˜
def crawl_mt_news(url):
    headers = {
        'User-Agent': 'Mozilla/5.0'
    }

    response = requests.get(url, headers=headers)
    if response.status_code != 200:
        print("âŒ ê¸°ì‚¬ ìš”ì²­ ì‹¤íŒ¨:", response.status_code)
        return None

    soup = BeautifulSoup(response.text, 'html.parser')

    # ì œëª© ì¶”ì¶œ
    title_tag = soup.find('h1', class_=['subject'])
    title = title_tag.get_text(strip=True) if title_tag else "ì œëª© ì—†ìŒ"

    # ë³¸ë¬¸ ì¶”ì¶œ (p íƒœê·¸ ì¤‘ì—ì„œ ì €ì‘ê¶Œ ì •ë³´ ì œì™¸)
    # 'view_text' í´ë˜ìŠ¤ë¥¼ ê°€ì§„ divë¥¼ ì°¾ìŒ (ë³¸ë¬¸ ì „ì²´ ì˜ì—­)
    body_tag = soup.find('div', class_='view_text')

    if body_tag:
    # ë³¸ë¬¸ í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ (ì¤„ë°”ê¿ˆ ê¸°ì¤€ ë¶„ë¦¬)
        raw_text = body_tag.get_text(separator="\n").strip()

    # ê° ì¤„ì˜ ê³µë°± ì œê±° ë° ë¹ˆ ì¤„ ì œê±°
        body_lines = [line.strip() for line in raw_text.split("\n") if line.strip()]

    # ë‹¤ì‹œ ì¤„ë°”ê¿ˆ ë¬¸ìë¡œ í•©ì¹¨
        body = '\n'.join(body_lines)
    else:
        body = "ë³¸ë¬¸ ì—†ìŒ"

    return {
        "title": title,
        "url": url,
        "body": body,
        "media": "ë¨¸ë‹ˆíˆ¬ë°ì´"
    }

# âœ… í—¤ëŸ´ë“œê²½ì œ í¬ë¡¤ë§ í•¨ìˆ˜
def crawl_heraldcorp_news(url):
    headers = {
        'User-Agent': 'Mozilla/5.0'
    }

    response = requests.get(url, headers=headers)
    if response.status_code != 200:
        print("âŒ ê¸°ì‚¬ ìš”ì²­ ì‹¤íŒ¨:", response.status_code)
        return None

    soup = BeautifulSoup(response.text, 'html.parser')

    # ì œëª© ì¶”ì¶œ
    title = soup.select_one('div.news_title > h1')
    title = title.get_text(strip=True) if title else "ì œëª© ì—†ìŒ"

    # ë³¸ë¬¸ ì¶”ì¶œ (p íƒœê·¸ ì¤‘ì—ì„œ ì €ì‘ê¶Œ ì •ë³´ ì œì™¸)
    # 'view_text' í´ë˜ìŠ¤ë¥¼ ê°€ì§„ divë¥¼ ì°¾ìŒ (ë³¸ë¬¸ ì „ì²´ ì˜ì—­)
    body_tag = soup.find('article', id='articleText')

    if body_tag:
        # article ì•ˆì˜ ëª¨ë“  p íƒœê·¸ í…ìŠ¤íŠ¸ë¥¼ ì¤„ë°”ê¿ˆ ê¸°ì¤€ìœ¼ë¡œ í•©ì¹¨
        raw_text = "\n".join([p.get_text(strip=True) for p in body_tag.find_all('p')]).strip()

        # ê° ì¤„ì˜ ê³µë°± ì œê±° ë° ë¹ˆ ì¤„ ì œê±°
        body_lines = [line.strip() for line in raw_text.split("\n") if line.strip()]

        # ë‹¤ì‹œ ì¤„ë°”ê¿ˆ ë¬¸ìë¡œ í•©ì¹¨
        body = '\n'.join(body_lines)
    else:
        body = "ë³¸ë¬¸ ì—†ìŒ"

    return {
        "title": title,
        "url": url,
        "body": body,
        "media": "í—¤ëŸ´ë“œê²½ì œ"
    }


# âœ… ì„œìš¸ê²½ì œ í¬ë¡¤ë§ í•¨ìˆ˜
def crawl_sedaily_news(url):
    headers = {
        'User-Agent': 'Mozilla/5.0'
    }

    response = requests.get(url, headers=headers)
    if response.status_code != 200:
        print("âŒ ê¸°ì‚¬ ìš”ì²­ ì‹¤íŒ¨:", response.status_code)
        return None

    soup = BeautifulSoup(response.text, 'html.parser')

    # ì œëª© ì¶”ì¶œ
    title_tag = soup.find('h1', class_=['art_tit'])
    title = title_tag.get_text(strip=True) if title_tag else "ì œëª© ì—†ìŒ"

    # ë³¸ë¬¸ ì¶”ì¶œ (p íƒœê·¸ ì¤‘ì—ì„œ ì €ì‘ê¶Œ ì •ë³´ ì œì™¸)
    # 'view_text' í´ë˜ìŠ¤ë¥¼ ê°€ì§„ divë¥¼ ì°¾ìŒ (ë³¸ë¬¸ ì „ì²´ ì˜ì—­)
    body_tag = soup.find('div', class_='article_view')

    if body_tag:
    # figure íƒœê·¸ ì œê±° (caption í¬í•¨)
        for fig in body_tag.find_all('figure', class_='art_photo'):
            fig.decompose()  # í•´ë‹¹ íƒœê·¸ ë° í•˜ìœ„ ë‚´ìš© ì™„ì „ ì‚­ì œ

    # <br> íƒœê·¸ë¥¼ '\n'ìœ¼ë¡œ ë³€í™˜
        for br in body_tag.find_all('br'):
            br.replace_with('\n')

    # í…ìŠ¤íŠ¸ ì¶”ì¶œ í›„ strip
        raw_text = body_tag.get_text(separator="\n").strip()

    # ê³µë°± ì¤„ ì œê±°
        body_lines = [line.strip() for line in raw_text.split("\n") if line.strip()]

    # ë‹¤ì‹œ ì¤„ë°”ê¿ˆìœ¼ë¡œ í•©ì¹¨
        body = '\n'.join(body_lines)
    else:
        body = "ë³¸ë¬¸ ì—†ìŒ"

    return {
        "title": title,
        "url": url,
        "body": body,
        "media": "ì„œìš¸ê²½ì œ"
    }

# âœ… ë‰´ìŠ¤í•Œ í¬ë¡¤ë§ í•¨ìˆ˜
def crawl_newspim_news(url):
    headers = {
        'User-Agent': 'Mozilla/5.0'
    }

    response = requests.get(url, headers=headers)
    if response.status_code != 200:
        print("âŒ ê¸°ì‚¬ ìš”ì²­ ì‹¤íŒ¨:", response.status_code)
        return None

    soup = BeautifulSoup(response.text, 'html.parser')

    # ì œëª© ì¶”ì¶œ
    title_tag = soup.find('h2')
    title = title_tag.get_text(strip=True) if title_tag else "ì œëª© ì—†ìŒ"

    body_tag = soup.find('div', class_='contents', itemprop='articleBody')

    if body_tag:
        # articleBody ë‚´ ëª¨ë“  p íƒœê·¸ í…ìŠ¤íŠ¸ë¥¼ ì¤„ë°”ê¿ˆ ê¸°ì¤€ìœ¼ë¡œ í•©ì¹¨
        raw_text = "\n".join([p.get_text(strip=True) for p in body_tag.find_all('p')]).strip()

        # ê° ì¤„ ê³µë°± ì œê±° ë° ë¹ˆ ì¤„ ì œê±°
        body_lines = [line.strip() for line in raw_text.split("\n") if line.strip()]

        body = ''.join(body_lines)

    else:
        body = "ë³¸ë¬¸ ì—†ìŒ"

    return {
        "title": title,
        "url": url,
        "body": body,
        "media": "ë‰´ìŠ¤í•Œ"
    }


# âœ… ë°ì¼ë¦¬ì•ˆ í¬ë¡¤ë§ í•¨ìˆ˜
def crawl_dailian_news(url):
    headers = {
        'User-Agent': 'Mozilla/5.0'
    }

    response = requests.get(url, headers=headers)
    if response.status_code != 200:
        print("âŒ ê¸°ì‚¬ ìš”ì²­ ì‹¤íŒ¨:", response.status_code)
        return None

    soup = BeautifulSoup(response.text, 'html.parser')

    # ì œëª© ì¶”ì¶œ
    title_tag = soup.find('h1', class_=['title'])
    title = title_tag.get_text(strip=True) if title_tag else "ì œëª© ì—†ìŒ"

    body_tag = soup.find('div', class_='article')

    if body_tag:
        # articleBody ë‚´ ëª¨ë“  p íƒœê·¸ í…ìŠ¤íŠ¸ë¥¼ ì¤„ë°”ê¿ˆ ê¸°ì¤€ìœ¼ë¡œ í•©ì¹¨
        raw_text = "\n".join([p.get_text(strip=True) for p in body_tag.find_all('p')]).strip()

        # ê° ì¤„ ê³µë°± ì œê±° ë° ë¹ˆ ì¤„ ì œê±°
        body_lines = [line.strip() for line in raw_text.split("\n") if line.strip()]

        body = ''.join(body_lines)

    else:
        body = "ë³¸ë¬¸ ì—†ìŒ"

    return {
        "title": title,
        "url": url,
        "body": body,
        "media": "ë°ì¼ë¦¬ì•ˆ"
    }

# âœ… ë§¤ì¼ê²½ì œ í¬ë¡¤ë§ í•¨ìˆ˜
def crawl_mk_news(url):
    headers = {
        'User-Agent': 'Mozilla/5.0'
    }

    response = requests.get(url, headers=headers)
    if response.status_code != 200:
        print("âŒ ê¸°ì‚¬ ìš”ì²­ ì‹¤íŒ¨:", response.status_code)
        return None

    soup = BeautifulSoup(response.text, 'html.parser')

    # ì œëª© ì¶”ì¶œ
    title_tag = soup.find('h2', class_=['news_ttl'])
    title = title_tag.get_text(strip=True) if title_tag else "ì œëª© ì—†ìŒ"
    
    body_tag = soup.find('div', class_='news_cnt_detail_wrap', itemprop='articleBody')

    if body_tag:
        p_texts = [p.get_text(strip=True) for p in body_tag.find_all('p')]
        p_texts = [text for text in p_texts if text]

        if p_texts:
            body = ''.join(p_texts)
        else:
            # p íƒœê·¸ ì—†ê±°ë‚˜ ë¹ˆ ê²½ìš° br ê¸°ì¤€ í…ìŠ¤íŠ¸ ë…¸ë“œ ì¶”ì¶œ
            br_texts = [str(t).strip() for t in body_tag.children if t and str(t).strip() and not getattr(t, 'name', None)]
            br_texts = [text for text in br_texts if text]
            if br_texts:
                body = ''.join(br_texts)
            else:
                body = "ë³¸ë¬¸ ì—†ìŒ"
    else:
        body = "ë³¸ë¬¸ ì—†ìŒ"
        
    return {
        "title": title,
        "url": url,
        "body": body,
        "media": "ë§¤ì¼ê²½ì œ"
    }

# âœ… ì•„ì‹œì•„ê²½ì œ í¬ë¡¤ë§ í•¨ìˆ˜
def crawl_asiae_news(url):
    headers = {
        'User-Agent': 'Mozilla/5.0'
    }

    response = requests.get(url, headers=headers)
    if response.status_code != 200:
        print("âŒ ê¸°ì‚¬ ìš”ì²­ ì‹¤íŒ¨:", response.status_code)
        return None

    soup = BeautifulSoup(response.text, 'html.parser')

    # ì œëª© ì¶”ì¶œ
    title_tag = soup.find('h1')  # ê·¸ëƒ¥ ì²« ë²ˆì§¸ h1 íƒœê·¸ ì°¾ê¸°
    title = title_tag.get_text(strip=True) if title_tag else "ì œëª© ì—†ìŒ"
    
    body_tag = soup.find('div', class_='article')

    if body_tag:
        # classê°€ txt_prohibitionì¸ p íƒœê·¸ëŠ” ì œì™¸í•˜ê³  í…ìŠ¤íŠ¸ ì¶”ì¶œ
        p_tags = [p for p in body_tag.find_all('p') if 'txt_prohibition' not in p.get('class', [])]

        raw_text = "\n".join([p.get_text(strip=True) for p in p_tags]).strip()

        # ê° ì¤„ ê³µë°± ì œê±° ë° ë¹ˆ ì¤„ ì œê±°
        body_lines = [line.strip() for line in raw_text.split("\n") if line.strip()]

        body = ''.join(body_lines)
    else:
        body = "ë³¸ë¬¸ ì—†ìŒ"


    return {
        "title": title,
        "url": url,
        "body": body,
        "media": "ì•„ì‹œì•„ê²½ì œ"
    }


# âœ… Supabase ì €ì¥ í•¨ìˆ˜
def save_to_supabase(data):
    try:
        existing = supabase.table("articles").select("id").eq("url", data["url"]).execute()
        if existing.data:
            print("âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬:", data["url"])
            return False
        supabase.table("articles").insert([data]).execute()
        print("âœ… ì €ì¥ ì™„ë£Œ:", data['title'])
        return True
    except Exception as e:
        print("âŒ ì €ì¥ ì‹¤íŒ¨:", e)
        return False

# âœ… ì–¸ë¡ ì‚¬ ë„ë©”ì¸ â†’ í¬ë¡¤ë§ í•¨ìˆ˜ ë§¤í•‘
CRAWLER_FUNCTION_MAP = {
    "www.yna.co.kr": crawl_yonhap_news,
    "www.chosun.com": crawl_chosun_news,
    "www.newsis.com": crawl_newsis_news,
    "www.news1.kr" : crawl_news1_news,
    "news.mt.co.kr" : crawl_mt_news,
    "biz.heraldcorp.com" : crawl_heraldcorp_news,
    "www.sedaily.com" : crawl_sedaily_news,
    "www.newspim.com" : crawl_newspim_news,
    "www.dailian.co.kr" : crawl_dailian_news,
    "www.mk.co.kr" : crawl_mk_news,
    "view.asiae.co.kr" : crawl_asiae_news,
    "www.khan.co.kr" : crawl_khan_news,
    
    
}

# âœ… ì–¸ë¡ ì‚¬ ë„ë©”ì¸ â†’ ì–¸ë¡ ì‚¬ ì´ë¦„ ë§¤í•‘
MEDIA_NAME_MAP = {
    "www.yna.co.kr": "ì—°í•©ë‰´ìŠ¤",
    "www.chosun.com": "ì¡°ì„ ì¼ë³´",
    "www.newsis.com": "ë‰´ì‹œìŠ¤",
    "www.news1.kr" : "ë‰´ìŠ¤1",
    "news.mt.co.kr" : "ë¨¸ë‹ˆíˆ¬ë°ì´",
    "biz.heraldcorp.com" : "í—¤ëŸ´ë“œê²½ì œ",
    "www.sedaily.com" : "ì„œìš¸ê²½ì œ",
    "www.newspim.com" : "ë‰´ìŠ¤í•Œ",
    "www.dailian.co.kr" : "ë°ì¼ë¦¬ì•ˆ",
    "www.mk.co.kr" : "ë§¤ì¼ê²½ì œ",
    "view.asiae.co.kr" : "ì•„ì‹œì•„ê²½ì œ",
    "www.khan.co.kr" : "ê²½í–¥ì‹ ë¬¸",
    
    
}

# âœ… ë„¤ì´ë²„ ë‰´ìŠ¤ ìˆ˜ì§‘ ë° ì €ì¥
def save_articles_from_naver(query):
    client_id = "_TznE38btYhyzWYsq9XK"
    client_secret = "06UYVlSHF9"

    encoded_query = urllib.parse.quote(query)
    headers = {
        "X-Naver-Client-Id": client_id,
        "X-Naver-Client-Secret": client_secret
    }

    display = 100
    saved_count_by_domain = {domain: 0 for domain in CRAWLER_FUNCTION_MAP.keys()}

    for start in range(1, 1000 + 1, display):
        url = f"https://openapi.naver.com/v1/search/news.json?query={encoded_query}&display={display}&start={start}&sort=date"
        response = requests.get(url, headers=headers)

        if response.status_code != 200:
            print(f"âŒ ìš”ì²­ ì‹¤íŒ¨ at start={start}: {response.status_code}")
            continue

        data = response.json()
        items = data.get("items", [])
        if not items:
            break

        for item in items:
            originallink = item.get("originallink", "")
            domain = urlparse(originallink).netloc

            if domain in CRAWLER_FUNCTION_MAP:
                article = CRAWLER_FUNCTION_MAP[domain](originallink)
                if article:
                    success = save_to_supabase(article)
                    if success:
                        saved_count_by_domain[domain] += 1

        if len(items) < display:
            break

    # âœ… ìµœì¢… ê²°ê³¼ ì¶œë ¥
    print("\nâœ… ì €ì¥ ìš”ì•½")
    for domain, count in saved_count_by_domain.items():
        media = MEDIA_NAME_MAP.get(domain, domain)
        print(f"ğŸ“° {media} ê¸°ì‚¬ ì´ {count}ê±´ Supabaseì— ì €ì¥ ì™„ë£Œ")

# âœ… ì‹¤í–‰
save_articles_from_naver("ë”¥í˜ì´í¬")
