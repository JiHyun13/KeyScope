import requests
import urllib.parse
from urllib.parse import urlparse
from bs4 import BeautifulSoup
from supabase import create_client, Client
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
from keybert import KeyBERT
import time
import os
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

# âœ… Supabase ì„¤ì •
SUPABASE_URL = "https://ypyujiaoeaqykbqetjef.supabase.co"
SUPABASE_KEY = "eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6InlweXVqaWFvZWFxeWticWV0amVmIiwicm9sZSI6ImFub24iLCJpYXQiOjE3NDY1NDUyNTQsImV4cCI6MjA2MjEyMTI1NH0.RuR9l89gxCcMkSzO053EHluQ0ers-piN4SUjZ-LtWjU"
supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)


## -----------------ì–¸ë¡ ì‚¬ í¬ë¡¤ëŸ¬ ì½”ë“œ ì‹œì‘!!!! ì—¬ê¸°ë¶€í„° ì•ˆ ê±´ë“œë ¤ë„ ë¼ìš”--------------------
def crawl_yonhap_news(url):
    headers = {'User-Agent': 'Mozilla/5.0'}
    response = requests.get(url, headers=headers)
    if response.status_code != 200:
        return None
    soup = BeautifulSoup(response.text, 'html.parser')
    title_tag = soup.find('h1', class_='tit01')
    title = title_tag.get_text(strip=True) if title_tag else "ì œëª© ì—†ìŒ"

    body_tag = soup.find('div', class_='story-news article')
    if body_tag:
        paragraphs = body_tag.find_all('p')
        body_lines = []
        for p in paragraphs:
            if 'txt-copyright' in p.get('class', []): continue
            text = p.get_text(strip=True)
            if text:
                body_lines.append(text)
        body = '\n'.join(body_lines)
    else:
        body = "ë³¸ë¬¸ ì—†ìŒ"

    return {"title": title, "url": url, "body": body, "media": "ì—°í•©ë‰´ìŠ¤"}


def crawl_newsis_news(url):
    headers = {'User-Agent': 'Mozilla/5.0'}
    response = requests.get(url, headers=headers)
    if response.status_code != 200:
        return None
    soup = BeautifulSoup(response.text, 'html.parser')

    title_tag = soup.select_one('h1.tit.title_area')
    title = title_tag.get_text(strip=True) if title_tag else "ì œëª© ì—†ìŒ"

    body_tag = soup.find('div', class_='viewer')
    if body_tag:
        raw_text = body_tag.get_text(separator="\n").strip()
        body_lines = [line.strip() for line in raw_text.split("\n") if line.strip()]
        body = '\n'.join(body_lines)
    else:
        body = "ë³¸ë¬¸ ì—†ìŒ"

    return {"title": title, "url": url, "body": body, "media": "ë‰´ì‹œìŠ¤"}


def crawl_mt_news(url):
    headers = {
        'User-Agent': 'Mozilla/5.0'
    }

    response = requests.get(url, headers=headers)
    if response.status_code != 200:
        print("âŒ ê¸°ì‚¬ ìš”ì²­ ì‹¤íŒ¨:", response.status_code)
        return None

    soup = BeautifulSoup(response.text, 'html.parser')

    title_tag = soup.find('h1', class_=['subject'])
    title = title_tag.get_text(strip=True) if title_tag else "ì œëª© ì—†ìŒ"

    body_tag = soup.find('div', class_='view_text')

    if body_tag:
        raw_text = body_tag.get_text(separator="\n").strip()
        body_lines = [line.strip() for line in raw_text.split("\n") if line.strip()]
        body = '\n'.join(body_lines)
    else:
        body = "ë³¸ë¬¸ ì—†ìŒ"

    return {
        "title": title,
        "url": url,
        "body": body,
        "media": "ë¨¸ë‹ˆíˆ¬ë°ì´"
    }


def crawl_heraldcorp_news(url):
    headers = {
        'User-Agent': 'Mozilla/5.0'
    }

    response = requests.get(url, headers=headers)
    if response.status_code != 200:
        print("âŒ ê¸°ì‚¬ ìš”ì²­ ì‹¤íŒ¨:", response.status_code)
        return None

    soup = BeautifulSoup(response.text, 'html.parser')

    title = soup.select_one('div.news_title > h1')
    title = title.get_text(strip=True) if title else "ì œëª© ì—†ìŒ"

    body_tag = soup.find('article', id='articleText')

    if body_tag:
        raw_text = "\n".join([p.get_text(strip=True) for p in body_tag.find_all('p')]).strip()
        body_lines = [line.strip() for line in raw_text.split("\n") if line.strip()]
        body = '\n'.join(body_lines)
    else:
        body = "ë³¸ë¬¸ ì—†ìŒ"

    return {
        "title": title,
        "url": url,
        "body": body,
        "media": "í—¤ëŸ´ë“œê²½ì œ"
    }


def crawl_sedaily_news(url):
    headers = {
        'User-Agent': 'Mozilla/5.0'
    }

    response = requests.get(url, headers=headers)
    if response.status_code != 200:
        print("âŒ ê¸°ì‚¬ ìš”ì²­ ì‹¤íŒ¨:", response.status_code)
        return None

    soup = BeautifulSoup(response.text, 'html.parser')

    title_tag = soup.find('h1', class_=['art_tit'])
    title = title_tag.get_text(strip=True) if title_tag else "ì œëª© ì—†ìŒ"

    body_tag = soup.find('div', class_='article_view')

    if body_tag:
        for fig in body_tag.find_all('figure', class_='art_photo'):
            fig.decompose()

        for br in body_tag.find_all('br'):
            br.replace_with('\n')

        raw_text = body_tag.get_text(separator="\n").strip()
        body_lines = [line.strip() for line in raw_text.split("\n") if line.strip()]
        body = '\n'.join(body_lines)
    else:
        body = "ë³¸ë¬¸ ì—†ìŒ"

    return {
        "title": title,
        "url": url,
        "body": body,
        "media": "ì„œìš¸ê²½ì œ"
    }


def crawl_newspim_news(url):
    headers = {
        'User-Agent': 'Mozilla/5.0'
    }

    response = requests.get(url, headers=headers)
    if response.status_code != 200:
        print("âŒ ê¸°ì‚¬ ìš”ì²­ ì‹¤íŒ¨:", response.status_code)
        return None

    soup = BeautifulSoup(response.text, 'html.parser')

    title_tag = soup.find('h2')
    title = title_tag.get_text(strip=True) if title_tag else "ì œëª© ì—†ìŒ"

    body_tag = soup.find('div', class_='contents', itemprop='articleBody')

    if body_tag:
        raw_text = "\n".join([p.get_text(strip=True) for p in body_tag.find_all('p')]).strip()
        body_lines = [line.strip() for line in raw_text.split("\n") if line.strip()]
        body = ''.join(body_lines)
    else:
        body = "ë³¸ë¬¸ ì—†ìŒ"

    return {
        "title": title,
        "url": url,
        "body": body,
        "media": "ë‰´ìŠ¤í•Œ"
    }


def crawl_dailian_news(url):
    headers = {
        'User-Agent': 'Mozilla/5.0'
    }

    response = requests.get(url, headers=headers)
    if response.status_code != 200:
        print("âŒ ê¸°ì‚¬ ìš”ì²­ ì‹¤íŒ¨:", response.status_code)
        return None

    soup = BeautifulSoup(response.text, 'html.parser')

    title_tag = soup.find('h1', class_=['title'])
    title = title_tag.get_text(strip=True) if title_tag else "ì œëª© ì—†ìŒ"

    body_tag = soup.find('div', class_='article')

    if body_tag:
        raw_text = "\n".join([p.get_text(strip=True) for p in body_tag.find_all('p')]).strip()
        body_lines = [line.strip() for line in raw_text.split("\n") if line.strip()]
        body = ''.join(body_lines)
    else:
        body = "ë³¸ë¬¸ ì—†ìŒ"

    return {
        "title": title,
        "url": url,
        "body": body,
        "media": "ë°ì¼ë¦¬ì•ˆ"
    }


def crawl_mk_news(url):
    headers = {
        'User-Agent': 'Mozilla/5.0'
    }

    response = requests.get(url, headers=headers)
    if response.status_code != 200:
        print("âŒ ê¸°ì‚¬ ìš”ì²­ ì‹¤íŒ¨:", response.status_code)
        return None

    soup = BeautifulSoup(response.text, 'html.parser')

    title_tag = soup.find('h2', class_=['news_ttl'])
    title = title_tag.get_text(strip=True) if title_tag else "ì œëª© ì—†ìŒ"
    
    body_tag = soup.find('div', class_='news_cnt_detail_wrap', itemprop='articleBody')

    if body_tag:
        p_texts = [p.get_text(strip=True) for p in body_tag.find_all('p')]
        p_texts = [text for text in p_texts if text]

        if p_texts:
            body = ''.join(p_texts)
        else:
            br_texts = [str(t).strip() for t in body_tag.children if t and str(t).strip() and not getattr(t, 'name', None)]
            br_texts = [text for text in br_texts if text]
            if br_texts:
                body = ''.join(br_texts)
            else:
                body = "ë³¸ë¬¸ ì—†ìŒ"
    else:
        body = "ë³¸ë¬¸ ì—†ìŒ"
        
    return {
        "title": title,
        "url": url,
        "body": body,
        "media": "ë§¤ì¼ê²½ì œ"
    }


def crawl_asiae_news(url):
    headers = {
        'User-Agent': 'Mozilla/5.0'
    }

    response = requests.get(url, headers=headers)
    if response.status_code != 200:
        print("âŒ ê¸°ì‚¬ ìš”ì²­ ì‹¤íŒ¨:", response.status_code)
        return None

    soup = BeautifulSoup(response.text, 'html.parser')

    title_tag = soup.find('h1')
    title = title_tag.get_text(strip=True) if title_tag else "ì œëª© ì—†ìŒ"
    
    body_tag = soup.find('div', class_='article')

    if body_tag:
        p_tags = [p for p in body_tag.find_all('p') if 'txt_prohibition' not in p.get('class', [])]

        raw_text = "\n".join([p.get_text(strip=True) for p in p_tags]).strip()

        body_lines = [line.strip() for line in raw_text.split("\n") if line.strip()]

        body = ''.join(body_lines)
    else:
        body = "ë³¸ë¬¸ ì—†ìŒ"

    return {
        "title": title,
        "url": url,
        "body": body,
        "media": "ì•„ì‹œì•„ê²½ì œ"
    }
    
def crawl_nocut_news(url):
    headers = {
        'User-Agent': 'Mozilla/5.0'
    }

    response = requests.get(url, headers=headers)
    if response.status_code != 200:
        print("âŒ ê¸°ì‚¬ ìš”ì²­ ì‹¤íŒ¨:", response.status_code)
        return None

    soup = BeautifulSoup(response.text, 'html.parser')
    
    # ì œëª© ì¶”ì¶œ
    title_tag = soup.find('h2')
    title = title_tag.get_text(strip=True) if title_tag else "ì œëª© ì—†ìŒ"
    
    body_tag = soup.find('div', id='pnlContent')  # id ì†ì„±ìœ¼ë¡œ ë³¸ë¬¸ divë¥¼ ì°¾ìŒ

    if body_tag:
        for br in body_tag.find_all("br"):
            br.replace_with("")  # br íƒœê·¸ ì‚­ì œ, ì¤„ë°”ê¿ˆ ì—†ì´ ì´ì–´ë¶™ì„

        raw_text = body_tag.get_text(strip=True)
        # ë¶ˆí•„ìš”í•œ ë¹ˆ ì¤„ ì œê±° ë° ê³µë°± ì •ë¦¬
        body_lines = [line.strip() for line in raw_text.split("\n") if line.strip()]
        body = "\n".join(body_lines)
    else:
        body = "ë³¸ë¬¸ ì—†ìŒ"

    return {
        "title": title,
        "url": url,
        "body": body,
        "media": "ë…¸ì»·ë‰´ìŠ¤"
    }
    
def crawl_edaily_news(url):
    headers = {
        'User-Agent': 'Mozilla/5.0'
    }

    response = requests.get(url, headers=headers)
    if response.status_code != 200:
        print("âŒ ê¸°ì‚¬ ìš”ì²­ ì‹¤íŒ¨:", response.status_code)
        return None

    soup = BeautifulSoup(response.text, 'html.parser')
    
    # ì œëª© ì¶”ì¶œ
    title_tag = soup.find('h1')
    title = title_tag.get_text(strip=True) if title_tag else "ì œëª© ì—†ìŒ"
    
    body_tag = soup.find('div', class_='news_body', itemprop='articleBody')

    if body_tag:
    # ë¶€ìˆ˜ ìš”ì†Œ ì œê±°
        for tag_to_remove in body_tag.find_all(['table', 'div'], class_=['gg_textshow']):
            tag_to_remove.decompose()  # íƒœê·¸ ìì²´ ì‚­ì œ

    # <br> íƒœê·¸ëŠ” ì¤„ë°”ê¿ˆ ë¬¸ìë¡œ ë³€í™˜
        for br in body_tag.find_all("br"):
            br.replace_with("\n")

        raw_text = body_tag.get_text(separator="\n", strip=True)

    # ë¹ˆ ì¤„ ì œê±° ë° ê³µë°± ì •ë¦¬
        body_lines = [line.strip() for line in raw_text.split("\n") if line.strip()]
        body = "\n".join(body_lines)
        
        body = body.replace('\n', '').replace('\r', '')
    else:
        body = "ë³¸ë¬¸ ì—†ìŒ"

    return {
        "title": title,
        "url": url,
        "body": body,
        "media": "ì´ë°ì¼ë¦¬"
    }
    
#ê²½ì¸ì¼ë³´ í¬ë¡¤ë§ í•¨ìˆ˜
def crawl_kyeongin_news(url):
    headers = {
        'User-Agent': 'Mozilla/5.0'
    }

    response = requests.get(url, headers=headers)
    if response.status_code != 200:
        print("âŒ ê¸°ì‚¬ ìš”ì²­ ì‹¤íŒ¨:", response.status_code)
        return None

    soup = BeautifulSoup(response.text, 'html.parser')
    
    # ì œëª© ì¶”ì¶œ
    title_tag = soup.find('h1')
    title = title_tag.get_text(strip=True) if title_tag else "ì œëª© ì—†ìŒ"
    
    body_tag = soup.find('div', class_='article-body')  # í˜¹ì€ id='article-body'

    if body_tag:
        # ê´‘ê³ ìš© div ë“± ë¶ˆí•„ìš”í•œ ìš”ì†Œ ì œê±°: idê°€ 'svcad_'ë¡œ ì‹œì‘í•˜ëŠ” div ì œê±°
        for ad_div in body_tag.find_all('div'):
            if ad_div.get('id') and ad_div['id'].startswith('svcad_'):
                ad_div.decompose()

        # table, íŠ¹ì • í´ë˜ìŠ¤ div ì œê±° (í•„ìš”ì‹œ ì¶”ê°€)
        for tag_to_remove in body_tag.find_all(['table', 'div'], class_=['gg_textshow']):
            tag_to_remove.decompose()

        # <br> íƒœê·¸ë¥¼ ì¤„ë°”ê¿ˆìœ¼ë¡œ ë³€í™˜
        for br in body_tag.find_all('br'):
            br.replace_with('\n')

        raw_text = body_tag.get_text(separator='\n', strip=True)

        # ë¹ˆ ì¤„ ì œê±° ë° ê³µë°± ì •ë¦¬
        body_lines = [line.strip() for line in raw_text.split("\n") if line.strip()]
        body = "\n".join(body_lines)
        
        body = body.replace('\n', '').replace('\r', '')

    else:
        body = "ë³¸ë¬¸ ì—†ìŒ"
   
    return {
        "title": title,
        "url": url,
        "body": body,
        "media": "ê²½ì¸ì¼ë³´"
    }

def crawl_seoul_news(url):
    headers = {
        'User-Agent': 'Mozilla/5.0'
    }

    response = requests.get(url, headers=headers)

    # ì¸ì½”ë”© ê°•ì œ ì§€ì • (utf-8 ë˜ëŠ” euc-kr ë‘˜ ì¤‘ í•˜ë‚˜ ì‹œë„)
    response.encoding = 'utf-8'  # ë˜ëŠ” 'euc-kr'

    if response.status_code != 200:
        print("âŒ ê¸°ì‚¬ ìš”ì²­ ì‹¤íŒ¨:", response.status_code)
        return None

    soup = BeautifulSoup(response.text, 'html.parser')
    
    # ì œëª© ì¶”ì¶œ
    title_tag = soup.find('h1')
    title = title_tag.get_text(strip=True) if title_tag else "ì œëª© ì—†ìŒ"
    
    body_tag = soup.find('div', class_='viewContent body18 color700')

    if body_tag:
        # ê´‘ê³  div ì œê±° (ì˜ˆ: idê°€ svcad_ë¡œ ì‹œì‘í•˜ëŠ” div)
        for ad_div in body_tag.find_all('div'):
            if ad_div.get('id') and ad_div['id'].startswith('svcad_'):
                ad_div.decompose()

        # ë¶ˆí•„ìš”í•œ íƒœê·¸ ì œê±° (í•„ìš” ì‹œ ì¶”ê°€)
        for tag_to_remove in body_tag.find_all(['table', 'div'], class_=['gg_textshow']):
            tag_to_remove.decompose()

        # <br> íƒœê·¸ë¥¼ ì¤„ë°”ê¿ˆ ë¬¸ìë¡œ ëŒ€ì²´
        for br in body_tag.find_all('br'):
            br.replace_with('\n')

        # í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° ë¹ˆ ì¤„ ì œê±°
        raw_text = body_tag.get_text(separator='\n', strip=True)
        body_lines = [line.strip() for line in raw_text.split('\n') if line.strip()]
        body = '\n'.join(body_lines)
        body = body.replace('\n', '').replace('\r', '')

    else:
        body = "ë³¸ë¬¸ ì—†ìŒ"

    return {
        "title": title,
        "url": url,
        "body": body,
        "media": "ì„œìš¸ì‹ ë¬¸"
    }
    
def crawl_fn_news(url):
    headers = {
        'User-Agent': 'Mozilla/5.0'
    }

    response = requests.get(url, headers=headers)

# ì¸ì½”ë”© ê°•ì œ ì§€ì • (utf-8 ë˜ëŠ” euc-kr ë‘˜ ì¤‘ í•˜ë‚˜ ì‹œë„)
    response.encoding = 'utf-8'  # ë˜ëŠ” 'euc-kr'

    if response.status_code != 200:
        print("âŒ ê¸°ì‚¬ ìš”ì²­ ì‹¤íŒ¨:", response.status_code)
        return None

    soup = BeautifulSoup(response.text, 'html.parser')
    
    # ì œëª© ì¶”ì¶œ
    title_tag = soup.find('h1')
    title = title_tag.get_text(strip=True) if title_tag else "ì œëª© ì—†ìŒ"
    
    body_tag = soup.find('div', class_='cont_view', id='article_content')

    if body_tag:
        # ê´‘ê³  div ì œê±° (í•„ìš” ì‹œ ì¡°ê±´ ì¶”ê°€)
        for ad_div in body_tag.find_all('div'):
            if ad_div.get('id') and ad_div['id'].startswith('svcad_'):
                ad_div.decompose()

        # ë¶€ìˆ˜ ìš”ì†Œ ì œê±° (í•„ìš”í•˜ë©´ ë” ì¶”ê°€ ê°€ëŠ¥)
        for tag_to_remove in body_tag.find_all(['table', 'div'], class_=['gg_textshow']):
            tag_to_remove.decompose()

        # <br> íƒœê·¸ë¥¼ ì¤„ë°”ê¿ˆ ë¬¸ìë¡œ ëŒ€ì²´
        for br in body_tag.find_all('br'):
            br.replace_with('\n')

        # í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° ë¹ˆ ì¤„ ì œê±°
        raw_text = body_tag.get_text(separator='\n', strip=True)
        body_lines = [line.strip() for line in raw_text.split('\n') if line.strip()]
        body = '\n'.join(body_lines)
        body = body.replace('\n', '').replace('\r', '')

    else:
        body = "ë³¸ë¬¸ ì—†ìŒ"

    return {
        "title": title,
        "url": url,
        "body": body,
        "media": "íŒŒì´ë‚¸ì…œë‰´ìŠ¤"
    }

## -----------------ì–¸ë¡ ì‚¬ í¬ë¡¤ëŸ¬ ì½”ë“œ ë!!!! ì—¬ê¸°ê¹Œì§€ ì•ˆ ê±´ë“œë ¤ë„ ë¼ìš”--------------------

kw_model = KeyBERT(model='distiluse-base-multilingual-cased')
# âœ… Supabase ì €ì¥ í•¨ìˆ˜ (ìˆ˜ì •: 'test' í…Œì´ë¸”, keyword ì»¬ëŸ¼ ì¶”ê°€)
def extract_keywords_with_scores(body, top_n=5):
    raw = kw_model.extract_keywords(body, top_n=top_n)
    return [{"keyword": kw, "score": round(score, 4)} for kw, score in raw]

# âœ… Supabase ì €ì¥ í•¨ìˆ˜ ìˆ˜ì •: 'keywords' í¬í•¨
def save_to_supabase(data, query_keyword, log_path="save_log.txt"):
    try:
        existing = supabase.table("test").select("id").eq("url", data["url"]).eq("query_keyword", query_keyword).execute()
        if existing.data:
            print(f"âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: {data['url']}")
            return False
        article_keywords = extract_keywords_with_scores(data["body"], top_n=5)
        record = data.copy()
        record["query_keyword"] = query_keyword
        record["article_keywords"] = article_keywords  # âœ… jsonbë¡œ ì €ì¥ë  êµ¬ì¡°

        supabase.table("test").insert([record]).execute()
        print(f"âœ… ì €ì¥ ì™„ë£Œ: {data['title']}")
        return True

    except Exception as e:
        print(f"âŒ ì €ì¥ ì‹¤íŒ¨: {e}")
        return False


    except Exception as e:
        msg = f"âŒ ì €ì¥ ì‹¤íŒ¨: {e}"
        print(msg)
        with open(log_path, "a", encoding="utf-8") as f:
            f.write(msg + "\n")
        return False
# ì–¸ë¡ ì‚¬ ë„ë©”ì¸ â†’ í¬ë¡¤ë§ í•¨ìˆ˜ ë§¤í•‘
CRAWLER_FUNCTION_MAP = {
    "www.yna.co.kr": crawl_yonhap_news,
    "www.newsis.com": crawl_newsis_news,
    "news.mt.co.kr" : crawl_mt_news,
    "biz.heraldcorp.com" : crawl_heraldcorp_news,
    "www.sedaily.com" : crawl_sedaily_news,
    "www.newspim.com" : crawl_newspim_news,
    "www.dailian.co.kr" : crawl_dailian_news,
    "www.mk.co.kr" : crawl_mk_news,
    "view.asiae.co.kr" : crawl_asiae_news,
    "www.nocutnews.co.kr" : crawl_nocut_news,
    "www.edaily.co.kr" : crawl_edaily_news,
    "www.kyeongin.com" : crawl_kyeongin_news,
    "www.seoul.co.kr" : crawl_seoul_news,
    "www.fnnews.com" : crawl_fn_news,
}

# ì–¸ë¡ ì‚¬ ë„ë©”ì¸ â†’ ì–¸ë¡ ì‚¬ ì´ë¦„ ë§¤í•‘
MEDIA_NAME_MAP = {
    "www.yna.co.kr": "ì—°í•©ë‰´ìŠ¤",
    "www.newsis.com": "ë‰´ì‹œìŠ¤",
    "news.mt.co.kr" : "ë¨¸ë‹ˆíˆ¬ë°ì´",
    "biz.heraldcorp.com" : "í—¤ëŸ´ë“œê²½ì œ",
    "www.sedaily.com" : "ì„œìš¸ê²½ì œ",
    "www.newspim.com" : "ë‰´ìŠ¤í•Œ",
    "www.dailian.co.kr" : "ë°ì¼ë¦¬ì•ˆ",
    "www.mk.co.kr" : "ë§¤ì¼ê²½ì œ",
    "view.asiae.co.kr" : "ì•„ì‹œì•„ê²½ì œ",
    "www.nocutnews.co.kr" : "ë…¸ì»·ë‰´ìŠ¤",
    "www.edaily.co.kr" : "ì´ë°ì¼ë¦¬ë‰´ìŠ¤",
    "www.kyeongin.com" : "ê²½ì¸ì¼ë³´",
    "www.seoul.co.kr" : "ì„œìš¸ì‹ ë¬¸",
    "www.fnnews.com" : "íŒŒì´ë‚¸ì…œë‰´ìŠ¤",

    
}

from concurrent.futures import ThreadPoolExecutor, as_completed

def save_articles_from_naver_parallel(query, max_workers=10):  # ë³‘ë ¬ì²˜ë¦¬ ì‹œë„
    client_id = "_TznE38btYhyzWYsq9XK"
    client_secret = "06UYVlSHF9"

    encoded_query = urllib.parse.quote(query)
    headers = {
        "X-Naver-Client-Id": client_id,
        "X-Naver-Client-Secret": client_secret
    }

    display = 10
    saved_count_by_domain = {domain: 0 for domain in CRAWLER_FUNCTION_MAP.keys()}

    for start in range(1, 101, display):
        url = f"https://openapi.naver.com/v1/search/news.json?query={encoded_query}&display={display}&start={start}&sort=date"
        response = requests.get(url, headers=headers)

        if response.status_code != 200:
            print(f"âŒ ìš”ì²­ ì‹¤íŒ¨ at start={start}: {response.status_code}")
            continue

        data = response.json()
        items = data.get("items", [])
        if not items:
            break

        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            futures = []

            for item in items:
                originallink = item.get("originallink", "")
                domain = urlparse(originallink).netloc

                if domain in CRAWLER_FUNCTION_MAP:
                    futures.append(executor.submit(CRAWLER_FUNCTION_MAP[domain], originallink))
                else:
                    continue

            for future in as_completed(futures):
                try:
                    article = future.result()
                except Exception as e:
                    print(f"âŒ í¬ë¡¤ë§ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e}")
                    continue

                if article:
                    success = save_to_supabase(article, query)
                    if success:
                        domain = urlparse(article["url"]).netloc
                        saved_count_by_domain[domain] += 1

        if len(items) < display:
            break

    # 1) ì¶œë ¥
    print("\nâœ… ì €ì¥ ìš”ì•½")
    for domain, count in saved_count_by_domain.items():
        media = MEDIA_NAME_MAP.get(domain, domain)
        print(f"ğŸ“° {media} ê¸°ì‚¬ ì´ {count}ê±´ Supabase test í…Œì´ë¸”ì— ì €ì¥ ì™„ë£Œ")

    # 2) í…ìŠ¤íŠ¸ íŒŒì¼ë¡œ ì €ì¥
    os.makedirs("log", exist_ok=True)

    filename = f"log/{query}_news_save_summary.txt"
    with open(filename, "w", encoding="utf-8") as f:
        f.write(f"ê²€ìƒ‰ì–´: {query}\n\n")
        f.write("ì–¸ë¡ ì‚¬ë³„ ì €ì¥ ê±´ìˆ˜ ìš”ì•½:\n")
        for domain, count in saved_count_by_domain.items():
            media = MEDIA_NAME_MAP.get(domain, domain)
            f.write(f"{media}: {count}ê±´\n")

    print(f"\nâœ… ì €ì¥ ìš”ì•½ì„ '{filename}' íŒŒì¼ë¡œ ì €ì¥í–ˆìŠµë‹ˆë‹¤.")