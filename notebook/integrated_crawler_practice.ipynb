{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4a2edb1",
   "metadata": {},
   "source": [
    "# **ì—°í•©ë‰´ìŠ¤ í¬ë¡¤ëŸ¬ë§Œ ì ìš©í•¨**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "aeeb5cda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ì €ì¥ ì™„ë£Œ: ç¾ ê³µë¬´ì›ë“¤ë„ ë‹¹í•˜ëŠ” AI ì‚¬ê¸°â€¦FBIì„œ ìŒì„±ë©”ì‹œì§€ ì£¼ì˜ ê²½ë³´\n",
      "âœ… ì €ì¥ ì™„ë£Œ: \"ì—¬ìë¼ì„œ ì£½ì—ˆë‹¤\" ê°•ë‚¨ì—­ ì‚´ì¸ì‚¬ê±´ 9ì£¼ê¸°â€¦'ì—¬í˜ ê·œíƒ„' ì‹œìœ„ ê³³ê³³ì„œ\n",
      "âœ… ì €ì¥ ì™„ë£Œ: ì†Œê°œíŒ… ì—¬ì„±ì´ ì‹ ì²´ ì ‘ì´‰ ê±°ì ˆí•˜ì 'ë°±ì´ˆí¬'í•œ 20ëŒ€ë‚¨\n",
      "âœ… ì €ì¥ ì™„ë£Œ: ì´ì¬ëª… \"ì—¬ì„± ë°›ëŠ” ì°¨ë³„ ê°œì„  ë…¸ë ¥í•´ì•¼â€¦ë‚¨ë…€ êµ¬ë¶„í•´ ê°ˆë“±í•˜ëŠ” ê²ƒ ì˜³ì§€ ì•Šì•„\"\n",
      "âœ… ì €ì¥ ì™„ë£Œ: ìœ„ì¥ì „ì… ì´ì¬ëª…Â·ìš¸ë³´ ê¹€ë¬¸ìˆ˜â€¦'ìˆ˜ë°±ë§Œ í´ë¦­' ë¶€ë¥´ëŠ” ë§¤ìš´ë§› 'ê°€ì§œë‰´ìŠ¤ ë°ˆ'[ê°€ì§œ íŒì¹˜ëŠ” SNSì •ì¹˜â‘ ]\n",
      "âœ… ì €ì¥ ì™„ë£Œ: ë¶€ì‚°êµìœ¡ì²­, êµì‚¬ 350ëª… ëŒ€ìƒ â€˜ë””ì§€í„¸ ì„±í­ë ¥ ì˜ˆë°© ì—°ìˆ˜â€™â€¦ë”¥í˜ì´í¬ ëŒ€ì‘\n",
      "âœ… ì €ì¥ ì™„ë£Œ: ï§¡ëŒ€í–‰ \"ëŒ€í†µë ¹ ì„ ê±°, êµ­ë¯¼ í†µí•© ì´ë£¨ëŠ” ì—­ì‚¬ì  ì „í™˜ì  ë¼ì•¼\"\n",
      "âœ… ì €ì¥ ì™„ë£Œ: ì˜¤ëŠ” 12ì¼ë¶€í„° 6Â·3ëŒ€ì„  ê³µì‹ì„ ê±°ìš´ë™â€¦\"ì¼ë°˜ ìœ ê¶Œìë„ ì„ ê±°ìš´ë™ ê°€ëŠ¥\"\n",
      "âœ… ì €ì¥ ì™„ë£Œ: ê´‘ì£¼ê²½ì°°, ì¹´ì¹´ì˜¤Tì™€ ë³´ì´ìŠ¤í”¼ì‹±Â·ë”¥í˜ì´í¬ ì˜ˆë°© í™ë³´\n",
      "âœ… ì €ì¥ ì™„ë£Œ: â€˜ëŒ€ì„  ëŒ€ë¹„â€™ ì „êµ­ ê²½ì°° ì§€íœ˜ë¶€ í™”ìƒíšŒì˜â€¦â€œìš°ë°œìƒí™© ì² ì € ëŒ€ë¹„â€\n",
      "âœ… ì €ì¥ ì™„ë£Œ: ë¼ì˜¨ì‹œíì–´, ì£¼ì‹ ì•¡ë©´ë³‘í•© í›„ ê±°ë˜ ì¬ê°œâ€¦ëª¨ë°”ì¼ì‹ ë¶„ì¦Â·ì–‘ìë‚´ì„±ì•”í˜¸Â·AIë³´ì•ˆ ì„±ì¥ê¸°ë°˜ ê°•í™”\n",
      "âœ… ì €ì¥ ì™„ë£Œ: \"ìœ ì „ì´ì•¼ë§ë¡œ ê°€ì¥ ê°•ë ¥í•œ AI\"â€¦ì˜ˆìœ ë§¨ì–¼êµ´ ì˜ìƒìœ¼ë¡œ 'í•©ì„±' ë…¼ë€ ì ì¬ìš´ ì‹ ë¶€\n",
      "âœ… ì €ì¥ ì™„ë£Œ: ì—¬ê°€ë¶€, 10.9ì–µì› ì¶”ê²½â€¦ë””ì§€í„¸ì„±ë²”ì£„ í”¼í•´ì ì§€ì›\n",
      "\n",
      "ğŸ“° ê¸°ì‚¬ ì´ 13ê±´ Supabaseì— ì €ì¥ ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import urllib.parse\n",
    "from urllib.parse import urlparse\n",
    "from bs4 import BeautifulSoup\n",
    "from supabase import create_client, Client\n",
    "\n",
    "# âœ… Supabase ì„¤ì •\n",
    "SUPABASE_URL = \"https://ypyujiaoeaqykbqetjef.supabase.co\"\n",
    "SUPABASE_KEY = \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6InlweXVqaWFvZWFxeWticWV0amVmIiwicm9sZSI6ImFub24iLCJpYXQiOjE3NDY1NDUyNTQsImV4cCI6MjA2MjEyMTI1NH0.RuR9l89gxCcMkSzO053EHluQ0ers-piN4SUjZ-LtWjU\"\n",
    "supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)\n",
    "\n",
    "# âœ… ì•„ì‹œì•„ê²½ì œ í¬ë¡¤ë§ í•¨ìˆ˜\n",
    "def crawl_asiae_news(url):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0'\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        print(\"âŒ ê¸°ì‚¬ ìš”ì²­ ì‹¤íŒ¨:\", response.status_code)\n",
    "        return None\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # ì œëª© ì¶”ì¶œ\n",
    "    title_tag = soup.find('h1')  # ê·¸ëƒ¥ ì²« ë²ˆì§¸ h1 íƒœê·¸ ì°¾ê¸°\n",
    "    title = title_tag.get_text(strip=True) if title_tag else \"ì œëª© ì—†ìŒ\"\n",
    "    \n",
    "    body_tag = soup.find('div', class_='article')\n",
    "\n",
    "    if body_tag:\n",
    "        # classê°€ txt_prohibitionì¸ p íƒœê·¸ëŠ” ì œì™¸í•˜ê³  í…ìŠ¤íŠ¸ ì¶”ì¶œ\n",
    "        p_tags = [p for p in body_tag.find_all('p') if 'txt_prohibition' not in p.get('class', [])]\n",
    "\n",
    "        raw_text = \"\\n\".join([p.get_text(strip=True) for p in p_tags]).strip()\n",
    "\n",
    "        # ê° ì¤„ ê³µë°± ì œê±° ë° ë¹ˆ ì¤„ ì œê±°\n",
    "        body_lines = [line.strip() for line in raw_text.split(\"\\n\") if line.strip()]\n",
    "\n",
    "        body = ''.join(body_lines)\n",
    "    else:\n",
    "        body = \"ë³¸ë¬¸ ì—†ìŒ\"\n",
    "\n",
    "\n",
    "    return {\n",
    "        \"title\": title,\n",
    "        \"url\": url,\n",
    "        \"body\": body,\n",
    "        \"media\": \"ì•„ì‹œì•„ê²½ì œ\"\n",
    "    }\n",
    "\n",
    "# âœ… Supabaseì— ì €ì¥\n",
    "def save_to_supabase(data):\n",
    "    try:\n",
    "        # ì¤‘ë³µ í™•ì¸ (url ê¸°ì¤€)\n",
    "        existing = supabase.table(\"articles\").select(\"id\").eq(\"url\", data[\"url\"]).execute()\n",
    "        if existing.data:\n",
    "            print(\"âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬:\", data[\"url\"])\n",
    "            return\n",
    "        response = supabase.table(\"articles\").insert([data]).execute()\n",
    "        print(\"âœ… ì €ì¥ ì™„ë£Œ:\", data[\"title\"])\n",
    "    except Exception as e:\n",
    "        print(\"âŒ ì €ì¥ ì‹¤íŒ¨:\", e)\n",
    "\n",
    "# âœ… ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ í›„ ì—°í•©ë‰´ìŠ¤ ê¸°ì‚¬ë§Œ ì €ì¥\n",
    "def save_articles_from_naver(query):\n",
    "    client_id = \"_TznE38btYhyzWYsq9XK\"\n",
    "    client_secret = \"06UYVlSHF9\"\n",
    "\n",
    "    encoded_query = urllib.parse.quote(query)\n",
    "    headers = {\n",
    "        \"X-Naver-Client-Id\": client_id,\n",
    "        \"X-Naver-Client-Secret\": client_secret\n",
    "    }\n",
    "\n",
    "    display = 100\n",
    "    total_saved = 0\n",
    "\n",
    "    for start in range(1, 1000 + 1, display):\n",
    "        url = f\"https://openapi.naver.com/v1/search/news.json?query={encoded_query}&display={display}&start={start}&sort=date\"\n",
    "        response = requests.get(url, headers=headers)\n",
    "\n",
    "        if response.status_code != 200:\n",
    "            print(f\"âŒ ìš”ì²­ ì‹¤íŒ¨ at start={start}: {response.status_code}\")\n",
    "            continue\n",
    "\n",
    "        data = response.json()\n",
    "        items = data.get(\"items\", [])\n",
    "        if not items:\n",
    "            break\n",
    "\n",
    "        for item in items:\n",
    "            originallink = item.get(\"originallink\", \"\")\n",
    "            domain = urlparse(originallink).netloc\n",
    "            if domain == \"view.asiae.co.kr\": # ìˆ˜ì • âœ…âœ…âœ…âœ…âœ…âœ…\n",
    "                article = crawl_asiae_news(originallink) # âœ…âœ…âœ…âœ…âœ…âœ…\n",
    "                if article:\n",
    "                    save_to_supabase(article)\n",
    "                    total_saved += 1\n",
    "\n",
    "        if len(items) < display:\n",
    "            break\n",
    "\n",
    "    print(f\"\\nğŸ“° ê¸°ì‚¬ ì´ {total_saved}ê±´ Supabaseì— ì €ì¥ ì™„ë£Œ\")\n",
    "\n",
    "# âœ… ì‹¤í–‰\n",
    "save_articles_from_naver(\"ë”¥í˜ì´í¬\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537bb5c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.news1.kr/politics/assembly/5785957\n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.newsis.com/view/NISX20250516_0003178759\n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.newsis.com/view/NISX20250517_0003179429\n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.newsis.com/view/NISX20250516_0003178893\n",
      "âœ… ì €ì¥ ì™„ë£Œ: ë””ì§€í„¸ ì•„ë…¸ë¯¸, ì•Œê³ í¬ëŸ¬ì‹œ... AI ë¬¸ëª… ì‹œëŒ€ì— ëŒ€í•œ ì‚¬íšŒí•™ìì˜ ê²½ê³ \n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.chosun.com/politics/election2025/2025/05/16/3KMOI3VZRZGW7OJQ4IZ42RPSJY/?utm_source=naver&utm_medium=referral&utm_campaign=naver-news\n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.yna.co.kr/view/AKR20250516115200001?input=1195m\n",
      "âœ… ì €ì¥ ì™„ë£Œ: ì´ì¬ëª…, ì—¬ì„±ê³µì•½ \"êµì œí­ë ¥Â·ìŠ¤í† í‚¹Â·ë””ì§€í„¸ì„±ë²”ì£„ ê°•ë ¥ ëŒ€ì‘í•  ê²ƒ\"\n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.newsis.com/view/NISX20250516_0003178868\n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.news1.kr/politics/assembly/5785352\n",
      "âœ… ì €ì¥ ì™„ë£Œ: [ì†ë³´]ì´ì¬ëª… \"ë”¥í˜ì´í¬ ë“± ë””ì§€í„¸ ì„±ë²”ì£„ ì§‘ì¤‘ ëª¨ë‹ˆí„°ë§\"\n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.newsis.com/view/NISX20250516_0003178806\n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.newsis.com/view/NISX20250516_0003178097\n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.chosun.com/national/court_law/2025/05/15/3T2BMR6CZ5ACVCGK6HHG6WAZIY/?utm_source=naver&utm_medium=referral&utm_campaign=naver-news\n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.newsis.com/view/NISX20250515_0003177499\n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.news1.kr/it-science/general-it/5784220\n",
      "âœ… ì €ì¥ ì™„ë£Œ: \"AI í™•ì‚°ì— í†µì‹ ì„œë¹„ìŠ¤ í–‰íƒœ ë³€í™”...ì´ìš©ìë³´í˜¸ ìƒˆ ì ‘ê·¼ ë°©ì‹ í•„ìš”\"\n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.yna.co.kr/view/AKR20250515098500056?input=1195m\n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.yna.co.kr/view/AKR20250515061200001?input=1195m\n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.newsis.com/view/NISX20250515_0003176700\n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.news1.kr/local/jeju/5783604\n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.news1.kr/politics/assembly/5783580\n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.news1.kr/society/incident-accident/5783095\n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.yna.co.kr/view/AKR20250514101700505?input=1195m\n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.chosun.com/national/education/2025/05/15/KXEGRV2C4JAUJGSDG3WU2HXHMM/?utm_source=naver&utm_medium=referral&utm_campaign=naver-news\n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.news1.kr/it-science/cc-newmedia/5782936\n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.yna.co.kr/view/AKR20250514138700017?input=1195m\n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.newsis.com/view/NISX20250514_0003175833\n",
      "âœ… ì €ì¥ ì™„ë£Œ: í•˜ë£¨ ë§Œì— 270ê±´ ëŠ˜ì—ˆë‹¤â€¦ 'ë¶ˆë²• ë”¥í˜ì´í¬'ì— ì„ ê´€ìœ„Â·ê²½ì°° ë¹„ìƒ\n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.newsis.com/view/NISX20250514_0003175121\n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.yna.co.kr/view/AKR20250514039200017?input=1195m\n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.newsis.com/view/NISX20250514_0003174700\n",
      "âœ… ì €ì¥ ì™„ë£Œ: \"ë‹¤ìŒ ëŒ€í†µë ¹ì€ ë‹¤ìŒ(Daum)ê³¼ í•¨ê»˜\"â€¦ì œ21ëŒ€ ëŒ€ì„  íŠ¹ì§‘ í˜ì´ì§€\n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.news1.kr/it-science/general-it/5782070\n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.newsis.com/view/NISX20250514_0003174607\n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.news1.kr/society/incident-accident/5781886\n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.newsis.com/view/NISX20250513_0003173845\n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.yna.co.kr/view/AKR20250513114500017?input=1195m\n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.yna.co.kr/view/AKR20250513047600530?input=1195m\n",
      "âœ… ì €ì¥ ì™„ë£Œ: ì‘ë…„ êµê¶Œì¹¨í•´ 4200ì—¬ ê±´â€¦'ì •ë‹¹í•œ ìƒí™œì§€ë„ ë¶ˆì‘'ì´ 32%\n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.newsis.com/view/NISX20250513_0003173430\n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.news1.kr/society/education/5781170\n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.yna.co.kr/view/AKR20250513041200009?input=1195m\n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.yna.co.kr/view/AKR20250512074900004?input=1195m\n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.newsis.com/view/NISX20250512_0003172026\n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.news1.kr/society/incident-accident/5779858\n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.newsis.com/view/NISX20250512_0003171989\n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.news1.kr/politics/general-politics/5779752\n",
      "âœ… ì €ì¥ ì™„ë£Œ: ì´ì£¼í˜¸ ê¶Œí•œëŒ€í–‰ \"êµ­ë¯¼í†µí•© ìœ„í•´ ê³µì • ì„ ê±° ì¤‘ìš”..ìœ„ë²•ì‹œ ë¬´ê´€ìš©\"\n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.yna.co.kr/view/AKR20250512056800530?input=1195m\n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.newsis.com/view/NISX20250512_0003171697\n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.news1.kr/local/daegu-gyeongbuk/5778854\n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.news1.kr/politics/assembly/5777732\n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.yna.co.kr/view/AKR20250509066400052?input=1195m\n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.yna.co.kr/view/AKR20250509050300017?input=1195m\n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.news1.kr/it-science/general-it/5777457\n",
      "âœ… ì €ì¥ ì™„ë£Œ: ê°•ê²½ìœ¤ ê¸°ì, ê¹€ì„¸ì˜ ê³ ì†Œâ€¦\"æ•…ê¹€ìƒˆë¡  ì œë³´ì ì°¾ì•„ê°„ ì  ì—†ì–´\"\n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.newsis.com/view/NISX20250508_0003167665\n",
      "âœ… ì €ì¥ ì™„ë£Œ: \"íˆ¬ì ì„±ê³µ, 40ì–µ ì§‘ ìƒ€ë‹¤\" 120ì–µ ëœ¯ì–´ê°„ ê·¸ë…€ ì •ì²´ì— 'ê¹œì§'â€¦ëŒ€ì²˜ ì–´ë–»ê²Œ\n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.news1.kr/local/gwangju-jeonnam/5776222\n",
      "âœ… ì €ì¥ ì™„ë£Œ: ê²½ì°°, ëŒ€ì„  íˆ¬í‘œì¼ì— 16ë§Œ8000ëª… íˆ¬ì…â€¦\"ìš°ë°œìƒí™© ì² ì € ëŒ€ë¹„\"\n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.newsis.com/view/NISX20250508_0003167022\n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.news1.kr/society/incident-accident/5775389\n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.yna.co.kr/view/AKR20250508047200004?input=1195m\n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.news1.kr/society/general-society/5775503\n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.newsis.com/view/NISX20250507_0003166634\n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.yna.co.kr/view/AKR20250507090300505?input=1195m\n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.newsis.com/view/NISX20250507_0003165527\n",
      "âœ… ì €ì¥ ì™„ë£Œ: ë¼ì˜¨ì‹œíì–´ 5ëŒ€1 ì•¡ë©´ë³‘í•© í›„ ê±°ë˜ ì¬ê°œ... \"êµ­ë‚´ì™¸ ì„±ì¥ê¸°ë°˜ ê°•í™”\"\n",
      "âœ… ì €ì¥ ì™„ë£Œ: ì‹œì‘ë„ ëª»í•œ ëŒ€í•œë¯¼êµ­ AIì‚°ì—…, ë°œëª©ë¶€í„° ì¡ëŠ” 'ì´ê²ƒ'\n",
      "âœ… ì €ì¥ ì™„ë£Œ: ì±—GPT 3ë…„ ëëŠ”ë° í•œêµ­ì€?â€¦'ë” ê°•í•œ AIê·œì œ' ë§Œë“¤ê¸° ë°”ë¹´ë‹¤\n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.news1.kr/society/incident-accident/5773697\n",
      "âœ… ì €ì¥ ì™„ë£Œ: ì–´ì œì˜ í”¼í•´ìê°€ ì˜¤ëŠ˜ì˜ ê°€í•´ìë¡œâ€¦ì‚¬ì´ë²„ ì„±í­ë ¥ ëŠªì— ë¹ ì§„ 10ëŒ€ë“¤\n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.newsis.com/view/NISX20250504_0003164362\n",
      "âœ… ì €ì¥ ì™„ë£Œ: 'AI ìœ¤ì„ì—´Â·ì´ì¬ëª…'ì´ ëŒ€ì„ íŒ í”ë“œë‚˜â€¦'ë¶ˆë²• ë”¥í˜ì´í¬' ì£¼ì˜ë³´\n",
      "âœ… ì €ì¥ ì™„ë£Œ: 'ì¿µì¿µ' ì‹¬ì¥ë°•ë™ê¹Œì§€ í›”ì¹œ ë”¥í˜ì´í¬â€¦\"êµ¬ë¶„ ë” ì–´ë µë„¤\"\n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.newsis.com/view/NISX20250502_0003162438\n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.yna.co.kr/view/AKR20250503034400530?input=1195m\n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.news1.kr/finance/blockchain-fintech/5771501\n",
      "âœ… ì €ì¥ ì™„ë£Œ: 'ë”¥í˜ì´í¬Â·ë¶ˆë²• ë¦¬ë² ì´íŠ¸' ë“± ê²½ì°°ê´€ 11ëª…, íŠ¹ì§„\n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.news1.kr/society/incident-accident/5772097\n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.yna.co.kr/view/AKR20250501112800004?input=1195m\n",
      "âœ… ì €ì¥ ì™„ë£Œ: \"ì´ˆë“±ìƒ ìë…€ ì„±êµìœ¡ ì–´ì©Œì§€\"â€¦ë¶€ëª¨ë“¤ ì‹ ì²­ ëª°ë¦¬ë”ë‹ˆ 'ì¡°ê¸° ë§ˆê°'\n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.yna.co.kr/view/AKR20250502070000530?input=1195m\n",
      "âœ… ì €ì¥ ì™„ë£Œ: ì—¬ê°€ë¶€, ì¶”ê²½ 10.9ì–µ...ë””ì§€í„¸ì„±ë²”ì£„ í”¼í•´ì ì§€ì›\n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.news1.kr/society/general-society/5771963\n",
      "âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬: https://www.newsis.com/view/NISX20250502_0003162646\n",
      "\n",
      "âœ… ì €ì¥ ìš”ì•½\n",
      "ğŸ“° ì—°í•©ë‰´ìŠ¤ ê¸°ì‚¬ ì´ 0ê±´ Supabaseì— ì €ì¥ ì™„ë£Œ\n",
      "ğŸ“° ì¡°ì„ ì¼ë³´ ê¸°ì‚¬ ì´ 0ê±´ Supabaseì— ì €ì¥ ì™„ë£Œ\n",
      "ğŸ“° ë‰´ì‹œìŠ¤ ê¸°ì‚¬ ì´ 0ê±´ Supabaseì— ì €ì¥ ì™„ë£Œ\n",
      "ğŸ“° ë‰´ìŠ¤1 ê¸°ì‚¬ ì´ 0ê±´ Supabaseì— ì €ì¥ ì™„ë£Œ\n",
      "ğŸ“° ë¨¸ë‹ˆíˆ¬ë°ì´ ê¸°ì‚¬ ì´ 20ê±´ Supabaseì— ì €ì¥ ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import urllib.parse\n",
    "from urllib.parse import urlparse\n",
    "from bs4 import BeautifulSoup\n",
    "from supabase import create_client, Client\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import time\n",
    "\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "# âœ… Supabase ì„¤ì •\n",
    "SUPABASE_URL = \"https://ypyujiaoeaqykbqetjef.supabase.co\"\n",
    "SUPABASE_KEY = \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6InlweXVqaWFvZWFxeWticWV0amVmIiwicm9sZSI6ImFub24iLCJpYXQiOjE3NDY1NDUyNTQsImV4cCI6MjA2MjEyMTI1NH0.RuR9l89gxCcMkSzO053EHluQ0ers-piN4SUjZ-LtWjU\"\n",
    "supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)\n",
    "\n",
    "# âœ… ì—°í•©ë‰´ìŠ¤ í¬ë¡¤ë§ í•¨ìˆ˜\n",
    "def crawl_yonhap_news(url):\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        return None\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    title_tag = soup.find('h1', class_='tit01')\n",
    "    title = title_tag.get_text(strip=True) if title_tag else \"ì œëª© ì—†ìŒ\"\n",
    "\n",
    "    body_tag = soup.find('div', class_='story-news article')\n",
    "    if body_tag:\n",
    "        paragraphs = body_tag.find_all('p')\n",
    "        body_lines = []\n",
    "        for p in paragraphs:\n",
    "            if 'txt-copyright' in p.get('class', []): continue\n",
    "            text = p.get_text(strip=True)\n",
    "            if text:\n",
    "                body_lines.append(text)\n",
    "        body = '\\n'.join(body_lines)\n",
    "    else:\n",
    "        body = \"ë³¸ë¬¸ ì—†ìŒ\"\n",
    "\n",
    "    return {\"title\": title, \"url\": url, \"body\": body, \"media\": \"ì—°í•©ë‰´ìŠ¤\"}\n",
    "\n",
    "# âœ… ì¡°ì„ ì¼ë³´ í¬ë¡¤ë§ í•¨ìˆ˜\n",
    "def crawl_chosun_news(url):\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument('--headless')\n",
    "    options.add_argument('--no-sandbox')\n",
    "    options.add_argument('--disable-dev-shm-usage')\n",
    "\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "    driver.get(url)\n",
    "    time.sleep(3)\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    driver.quit()\n",
    "\n",
    "    title_tag = soup.find('h1')\n",
    "    title = title_tag.get_text(strip=True) if title_tag else \"ì œëª© ì—†ìŒ\"\n",
    "\n",
    "    body_paragraphs = soup.select('p.article-body__content.article-body__content-text')\n",
    "    body_lines = []\n",
    "    for p in body_paragraphs:\n",
    "        text = p.get_text(strip=True)\n",
    "        if not text:\n",
    "            continue\n",
    "        if any(keyword in text for keyword in ['ê¸°ì', 'ë¬´ë‹¨ ì „ì¬', 'êµ¬ë…', 'Copyright']):\n",
    "            continue\n",
    "        body_lines.append(text)\n",
    "    body = '\\n'.join(body_lines) if body_lines else \"ë³¸ë¬¸ ì—†ìŒ\"\n",
    "\n",
    "    return {\"title\": title, \"url\": url, \"body\": body, \"media\": \"ì¡°ì„ ì¼ë³´\"}\n",
    "\n",
    "# âœ… ë‰´ì‹œìŠ¤ í¬ë¡¤ë§ í•¨ìˆ˜\n",
    "def crawl_newsis_news(url):\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        return None\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    title_tag = soup.select_one('h1.tit.title_area')\n",
    "    title = title_tag.get_text(strip=True) if title_tag else \"ì œëª© ì—†ìŒ\"\n",
    "\n",
    "    body_tag = soup.find('div', class_='viewer')\n",
    "    if body_tag:\n",
    "        raw_text = body_tag.get_text(separator=\"\\n\").strip()\n",
    "        body_lines = [line.strip() for line in raw_text.split(\"\\n\") if line.strip()]\n",
    "        body = '\\n'.join(body_lines)\n",
    "    else:\n",
    "        body = \"ë³¸ë¬¸ ì—†ìŒ\"\n",
    "\n",
    "    return {\"title\": title, \"url\": url, \"body\": body, \"media\": \"ë‰´ì‹œìŠ¤\"}\n",
    "\n",
    "\n",
    "def crawl_news1_news(url):\n",
    "    # âœ… ì…€ë ˆë‹ˆì›€ ì˜µì…˜ ì„¤ì •\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument('--headless')  # ì°½ ì—†ì´ ì‹¤í–‰\n",
    "    options.add_argument('--no-sandbox')\n",
    "    options.add_argument('--disable-dev-shm-usage')\n",
    "\n",
    "    # âœ… ë“œë¼ì´ë²„ ì‹¤í–‰\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "    driver.get(url)\n",
    "\n",
    "    # âœ… ëª…ì‹œì  ëŒ€ê¸°: ë³¸ë¬¸ì´ ë¡œë”©ë  ë•Œê¹Œì§€ ìµœëŒ€ 10ì´ˆ ëŒ€ê¸°\n",
    "    try:\n",
    "        WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.ID, \"articleBodyContent\"))\n",
    "        )\n",
    "    except:\n",
    "        print(\"â° ë¡œë”© ì‹¤íŒ¨ ë˜ëŠ” íƒ€ì„ì•„ì›ƒ ë°œìƒ\")\n",
    "        driver.quit()\n",
    "        return None\n",
    "\n",
    "    # âœ… í˜ì´ì§€ íŒŒì‹±\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    driver.quit()\n",
    "\n",
    "    # âœ… ì œëª© ì¶”ì¶œ\n",
    "    title_tag = soup.select_one('h1.article-h2-header-title.mb-40')\n",
    "    title = title_tag.get_text(strip=True) if title_tag else \"ì œëª© ì—†ìŒ\"\n",
    "\n",
    "    # âœ… ë³¸ë¬¸ ì¶”ì¶œ\n",
    "    body_tag = soup.find('div', id='articleBodyContent')\n",
    "    if body_tag:\n",
    "        paragraphs = body_tag.find_all('p')\n",
    "        body_lines = [p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True)]\n",
    "        body = '\\n'.join(body_lines)\n",
    "    else:\n",
    "        body = \"ë³¸ë¬¸ ì—†ìŒ\"\n",
    "\n",
    "    return {\n",
    "        \"title\": title,\n",
    "        \"url\": url,\n",
    "        \"body\": body,\n",
    "        \"media\": \"ë‰´ìŠ¤1\"\n",
    "    }\n",
    "    \n",
    "# âœ… ë¨¸ë‹ˆíˆ¬ë°ì´ ë‰´ìŠ¤ í¬ë¡¤ë§ í•¨ìˆ˜\n",
    "def crawl_mt_news(url):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0'\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        print(\"âŒ ê¸°ì‚¬ ìš”ì²­ ì‹¤íŒ¨:\", response.status_code)\n",
    "        return None\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # ì œëª© ì¶”ì¶œ\n",
    "    title_tag = soup.find('h1', class_=['subject'])\n",
    "    title = title_tag.get_text(strip=True) if title_tag else \"ì œëª© ì—†ìŒ\"\n",
    "\n",
    "    # ë³¸ë¬¸ ì¶”ì¶œ (p íƒœê·¸ ì¤‘ì—ì„œ ì €ì‘ê¶Œ ì •ë³´ ì œì™¸)\n",
    "    # 'view_text' í´ë˜ìŠ¤ë¥¼ ê°€ì§„ divë¥¼ ì°¾ìŒ (ë³¸ë¬¸ ì „ì²´ ì˜ì—­)\n",
    "    body_tag = soup.find('div', class_='view_text')\n",
    "\n",
    "    if body_tag:\n",
    "    # ë³¸ë¬¸ í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ (ì¤„ë°”ê¿ˆ ê¸°ì¤€ ë¶„ë¦¬)\n",
    "        raw_text = body_tag.get_text(separator=\"\\n\").strip()\n",
    "\n",
    "    # ê° ì¤„ì˜ ê³µë°± ì œê±° ë° ë¹ˆ ì¤„ ì œê±°\n",
    "        body_lines = [line.strip() for line in raw_text.split(\"\\n\") if line.strip()]\n",
    "\n",
    "    # ë‹¤ì‹œ ì¤„ë°”ê¿ˆ ë¬¸ìë¡œ í•©ì¹¨\n",
    "        body = '\\n'.join(body_lines)\n",
    "    else:\n",
    "        body = \"ë³¸ë¬¸ ì—†ìŒ\"\n",
    "\n",
    "    return {\n",
    "        \"title\": title,\n",
    "        \"url\": url,\n",
    "        \"body\": body,\n",
    "        \"media\": \"ë¨¸ë‹ˆíˆ¬ë°ì´\"\n",
    "    }\n",
    "\n",
    "# âœ… í—¤ëŸ´ë“œê²½ì œ í¬ë¡¤ë§ í•¨ìˆ˜\n",
    "def crawl_heraldcorp_news(url):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0'\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        print(\"âŒ ê¸°ì‚¬ ìš”ì²­ ì‹¤íŒ¨:\", response.status_code)\n",
    "        return None\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # ì œëª© ì¶”ì¶œ\n",
    "    title = soup.select_one('div.news_title > h1')\n",
    "    title = title.get_text(strip=True) if title else \"ì œëª© ì—†ìŒ\"\n",
    "\n",
    "    # ë³¸ë¬¸ ì¶”ì¶œ (p íƒœê·¸ ì¤‘ì—ì„œ ì €ì‘ê¶Œ ì •ë³´ ì œì™¸)\n",
    "    # 'view_text' í´ë˜ìŠ¤ë¥¼ ê°€ì§„ divë¥¼ ì°¾ìŒ (ë³¸ë¬¸ ì „ì²´ ì˜ì—­)\n",
    "    body_tag = soup.find('article', id='articleText')\n",
    "\n",
    "    if body_tag:\n",
    "        # article ì•ˆì˜ ëª¨ë“  p íƒœê·¸ í…ìŠ¤íŠ¸ë¥¼ ì¤„ë°”ê¿ˆ ê¸°ì¤€ìœ¼ë¡œ í•©ì¹¨\n",
    "        raw_text = \"\\n\".join([p.get_text(strip=True) for p in body_tag.find_all('p')]).strip()\n",
    "\n",
    "        # ê° ì¤„ì˜ ê³µë°± ì œê±° ë° ë¹ˆ ì¤„ ì œê±°\n",
    "        body_lines = [line.strip() for line in raw_text.split(\"\\n\") if line.strip()]\n",
    "\n",
    "        # ë‹¤ì‹œ ì¤„ë°”ê¿ˆ ë¬¸ìë¡œ í•©ì¹¨\n",
    "        body = '\\n'.join(body_lines)\n",
    "    else:\n",
    "        body = \"ë³¸ë¬¸ ì—†ìŒ\"\n",
    "\n",
    "    return {\n",
    "        \"title\": title,\n",
    "        \"url\": url,\n",
    "        \"body\": body,\n",
    "        \"media\": \"í—¤ëŸ´ë“œê²½ì œ\"\n",
    "    }\n",
    "\n",
    "\n",
    "# âœ… ì„œìš¸ê²½ì œ í¬ë¡¤ë§ í•¨ìˆ˜\n",
    "def crawl_sedaily_news(url):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0'\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        print(\"âŒ ê¸°ì‚¬ ìš”ì²­ ì‹¤íŒ¨:\", response.status_code)\n",
    "        return None\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # ì œëª© ì¶”ì¶œ\n",
    "    title_tag = soup.find('h1', class_=['art_tit'])\n",
    "    title = title_tag.get_text(strip=True) if title_tag else \"ì œëª© ì—†ìŒ\"\n",
    "\n",
    "    # ë³¸ë¬¸ ì¶”ì¶œ (p íƒœê·¸ ì¤‘ì—ì„œ ì €ì‘ê¶Œ ì •ë³´ ì œì™¸)\n",
    "    # 'view_text' í´ë˜ìŠ¤ë¥¼ ê°€ì§„ divë¥¼ ì°¾ìŒ (ë³¸ë¬¸ ì „ì²´ ì˜ì—­)\n",
    "    body_tag = soup.find('div', class_='article_view')\n",
    "\n",
    "    if body_tag:\n",
    "    # figure íƒœê·¸ ì œê±° (caption í¬í•¨)\n",
    "        for fig in body_tag.find_all('figure', class_='art_photo'):\n",
    "            fig.decompose()  # í•´ë‹¹ íƒœê·¸ ë° í•˜ìœ„ ë‚´ìš© ì™„ì „ ì‚­ì œ\n",
    "\n",
    "    # <br> íƒœê·¸ë¥¼ '\\n'ìœ¼ë¡œ ë³€í™˜\n",
    "        for br in body_tag.find_all('br'):\n",
    "            br.replace_with('\\n')\n",
    "\n",
    "    # í…ìŠ¤íŠ¸ ì¶”ì¶œ í›„ strip\n",
    "        raw_text = body_tag.get_text(separator=\"\\n\").strip()\n",
    "\n",
    "    # ê³µë°± ì¤„ ì œê±°\n",
    "        body_lines = [line.strip() for line in raw_text.split(\"\\n\") if line.strip()]\n",
    "\n",
    "    # ë‹¤ì‹œ ì¤„ë°”ê¿ˆìœ¼ë¡œ í•©ì¹¨\n",
    "        body = '\\n'.join(body_lines)\n",
    "    else:\n",
    "        body = \"ë³¸ë¬¸ ì—†ìŒ\"\n",
    "\n",
    "    return {\n",
    "        \"title\": title,\n",
    "        \"url\": url,\n",
    "        \"body\": body,\n",
    "        \"media\": \"ì„œìš¸ê²½ì œ\"\n",
    "    }\n",
    "\n",
    "# âœ… ë‰´ìŠ¤í•Œ í¬ë¡¤ë§ í•¨ìˆ˜\n",
    "def crawl_newspim_news(url):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0'\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        print(\"âŒ ê¸°ì‚¬ ìš”ì²­ ì‹¤íŒ¨:\", response.status_code)\n",
    "        return None\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # ì œëª© ì¶”ì¶œ\n",
    "    title_tag = soup.find('h2')\n",
    "    title = title_tag.get_text(strip=True) if title_tag else \"ì œëª© ì—†ìŒ\"\n",
    "\n",
    "    body_tag = soup.find('div', class_='contents', itemprop='articleBody')\n",
    "\n",
    "    if body_tag:\n",
    "        # articleBody ë‚´ ëª¨ë“  p íƒœê·¸ í…ìŠ¤íŠ¸ë¥¼ ì¤„ë°”ê¿ˆ ê¸°ì¤€ìœ¼ë¡œ í•©ì¹¨\n",
    "        raw_text = \"\\n\".join([p.get_text(strip=True) for p in body_tag.find_all('p')]).strip()\n",
    "\n",
    "        # ê° ì¤„ ê³µë°± ì œê±° ë° ë¹ˆ ì¤„ ì œê±°\n",
    "        body_lines = [line.strip() for line in raw_text.split(\"\\n\") if line.strip()]\n",
    "\n",
    "        body = ''.join(body_lines)\n",
    "\n",
    "    else:\n",
    "        body = \"ë³¸ë¬¸ ì—†ìŒ\"\n",
    "\n",
    "    return {\n",
    "        \"title\": title,\n",
    "        \"url\": url,\n",
    "        \"body\": body,\n",
    "        \"media\": \"ë‰´ìŠ¤í•Œ\"\n",
    "    }\n",
    "\n",
    "\n",
    "# âœ… ë°ì¼ë¦¬ì•ˆ í¬ë¡¤ë§ í•¨ìˆ˜\n",
    "def crawl_dailian_news(url):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0'\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        print(\"âŒ ê¸°ì‚¬ ìš”ì²­ ì‹¤íŒ¨:\", response.status_code)\n",
    "        return None\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # ì œëª© ì¶”ì¶œ\n",
    "    title_tag = soup.find('h1', class_=['title'])\n",
    "    title = title_tag.get_text(strip=True) if title_tag else \"ì œëª© ì—†ìŒ\"\n",
    "\n",
    "    body_tag = soup.find('div', class_='article')\n",
    "\n",
    "    if body_tag:\n",
    "        # articleBody ë‚´ ëª¨ë“  p íƒœê·¸ í…ìŠ¤íŠ¸ë¥¼ ì¤„ë°”ê¿ˆ ê¸°ì¤€ìœ¼ë¡œ í•©ì¹¨\n",
    "        raw_text = \"\\n\".join([p.get_text(strip=True) for p in body_tag.find_all('p')]).strip()\n",
    "\n",
    "        # ê° ì¤„ ê³µë°± ì œê±° ë° ë¹ˆ ì¤„ ì œê±°\n",
    "        body_lines = [line.strip() for line in raw_text.split(\"\\n\") if line.strip()]\n",
    "\n",
    "        body = ''.join(body_lines)\n",
    "\n",
    "    else:\n",
    "        body = \"ë³¸ë¬¸ ì—†ìŒ\"\n",
    "\n",
    "    return {\n",
    "        \"title\": title,\n",
    "        \"url\": url,\n",
    "        \"body\": body,\n",
    "        \"media\": \"ë°ì¼ë¦¬ì•ˆ\"\n",
    "    }\n",
    "\n",
    "# âœ… ë§¤ì¼ê²½ì œ í¬ë¡¤ë§ í•¨ìˆ˜\n",
    "def crawl_mk_news(url):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0'\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        print(\"âŒ ê¸°ì‚¬ ìš”ì²­ ì‹¤íŒ¨:\", response.status_code)\n",
    "        return None\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # ì œëª© ì¶”ì¶œ\n",
    "    title_tag = soup.find('h2', class_=['news_ttl'])\n",
    "    title = title_tag.get_text(strip=True) if title_tag else \"ì œëª© ì—†ìŒ\"\n",
    "    \n",
    "    body_tag = soup.find('div', class_='news_cnt_detail_wrap', itemprop='articleBody')\n",
    "\n",
    "    if body_tag:\n",
    "        p_texts = [p.get_text(strip=True) for p in body_tag.find_all('p')]\n",
    "        p_texts = [text for text in p_texts if text]\n",
    "\n",
    "        if p_texts:\n",
    "            body = ''.join(p_texts)\n",
    "        else:\n",
    "            # p íƒœê·¸ ì—†ê±°ë‚˜ ë¹ˆ ê²½ìš° br ê¸°ì¤€ í…ìŠ¤íŠ¸ ë…¸ë“œ ì¶”ì¶œ\n",
    "            br_texts = [str(t).strip() for t in body_tag.children if t and str(t).strip() and not getattr(t, 'name', None)]\n",
    "            br_texts = [text for text in br_texts if text]\n",
    "            if br_texts:\n",
    "                body = ''.join(br_texts)\n",
    "            else:\n",
    "                body = \"ë³¸ë¬¸ ì—†ìŒ\"\n",
    "    else:\n",
    "        body = \"ë³¸ë¬¸ ì—†ìŒ\"\n",
    "        \n",
    "    return {\n",
    "        \"title\": title,\n",
    "        \"url\": url,\n",
    "        \"body\": body,\n",
    "        \"media\": \"ë§¤ì¼ê²½ì œ\"\n",
    "    }\n",
    "\n",
    "# âœ… ì•„ì‹œì•„ê²½ì œ í¬ë¡¤ë§ í•¨ìˆ˜\n",
    "def crawl_asiae_news(url):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0'\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        print(\"âŒ ê¸°ì‚¬ ìš”ì²­ ì‹¤íŒ¨:\", response.status_code)\n",
    "        return None\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # ì œëª© ì¶”ì¶œ\n",
    "    title_tag = soup.find('h1')  # ê·¸ëƒ¥ ì²« ë²ˆì§¸ h1 íƒœê·¸ ì°¾ê¸°\n",
    "    title = title_tag.get_text(strip=True) if title_tag else \"ì œëª© ì—†ìŒ\"\n",
    "    \n",
    "    body_tag = soup.find('div', class_='article')\n",
    "\n",
    "    if body_tag:\n",
    "        # classê°€ txt_prohibitionì¸ p íƒœê·¸ëŠ” ì œì™¸í•˜ê³  í…ìŠ¤íŠ¸ ì¶”ì¶œ\n",
    "        p_tags = [p for p in body_tag.find_all('p') if 'txt_prohibition' not in p.get('class', [])]\n",
    "\n",
    "        raw_text = \"\\n\".join([p.get_text(strip=True) for p in p_tags]).strip()\n",
    "\n",
    "        # ê° ì¤„ ê³µë°± ì œê±° ë° ë¹ˆ ì¤„ ì œê±°\n",
    "        body_lines = [line.strip() for line in raw_text.split(\"\\n\") if line.strip()]\n",
    "\n",
    "        body = ''.join(body_lines)\n",
    "    else:\n",
    "        body = \"ë³¸ë¬¸ ì—†ìŒ\"\n",
    "\n",
    "\n",
    "    return {\n",
    "        \"title\": title,\n",
    "        \"url\": url,\n",
    "        \"body\": body,\n",
    "        \"media\": \"ì•„ì‹œì•„ê²½ì œ\"\n",
    "    }\n",
    "\n",
    "\n",
    "# âœ… Supabase ì €ì¥ í•¨ìˆ˜\n",
    "def save_to_supabase(data):\n",
    "    try:\n",
    "        existing = supabase.table(\"articles\").select(\"id\").eq(\"url\", data[\"url\"]).execute()\n",
    "        if existing.data:\n",
    "            print(\"âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê¸°ì‚¬:\", data[\"url\"])\n",
    "            return False\n",
    "        supabase.table(\"articles\").insert([data]).execute()\n",
    "        print(\"âœ… ì €ì¥ ì™„ë£Œ:\", data['title'])\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(\"âŒ ì €ì¥ ì‹¤íŒ¨:\", e)\n",
    "        return False\n",
    "\n",
    "# âœ… ì–¸ë¡ ì‚¬ ë„ë©”ì¸ â†’ í¬ë¡¤ë§ í•¨ìˆ˜ ë§¤í•‘\n",
    "CRAWLER_FUNCTION_MAP = {\n",
    "    \"www.yna.co.kr\": crawl_yonhap_news,\n",
    "    \"www.chosun.com\": crawl_chosun_news,\n",
    "    \"www.newsis.com\": crawl_newsis_news,\n",
    "    \"www.news1.kr\" : crawl_news1_news,\n",
    "    \"news.mt.co.kr\" : crawl_mt_news,\n",
    "    \"biz.heraldcorp.com\" : crawl_heraldcorp_news,\n",
    "    \"www.sedaily.com\" : crawl_sedaily_news,\n",
    "    \"www.newspim.com\" : crawl_newspim_news,\n",
    "    \"www.dailian.co.kr\" : crawl_dailian_news,\n",
    "    \"www.mk.co.kr\" : crawl_mk_news,\n",
    "    \"view.asiae.co.kr\" : crawl_asiae_news,\n",
    "    \"www.khan.co.kr\" : crawl_khan_news,\n",
    "    \n",
    "    \n",
    "}\n",
    "\n",
    "# âœ… ì–¸ë¡ ì‚¬ ë„ë©”ì¸ â†’ ì–¸ë¡ ì‚¬ ì´ë¦„ ë§¤í•‘\n",
    "MEDIA_NAME_MAP = {\n",
    "    \"www.yna.co.kr\": \"ì—°í•©ë‰´ìŠ¤\",\n",
    "    \"www.chosun.com\": \"ì¡°ì„ ì¼ë³´\",\n",
    "    \"www.newsis.com\": \"ë‰´ì‹œìŠ¤\",\n",
    "    \"www.news1.kr\" : \"ë‰´ìŠ¤1\",\n",
    "    \"news.mt.co.kr\" : \"ë¨¸ë‹ˆíˆ¬ë°ì´\",\n",
    "    \"biz.heraldcorp.com\" : \"í—¤ëŸ´ë“œê²½ì œ\",\n",
    "    \"www.sedaily.com\" : \"ì„œìš¸ê²½ì œ\",\n",
    "    \"www.newspim.com\" : \"ë‰´ìŠ¤í•Œ\",\n",
    "    \"www.dailian.co.kr\" : \"ë°ì¼ë¦¬ì•ˆ\",\n",
    "    \"www.mk.co.kr\" : \"ë§¤ì¼ê²½ì œ\",\n",
    "    \"view.asiae.co.kr\" : \"ì•„ì‹œì•„ê²½ì œ\",\n",
    "    \"www.khan.co.kr\" : \"ê²½í–¥ì‹ ë¬¸\",\n",
    "    \n",
    "    \n",
    "}\n",
    "\n",
    "# âœ… ë„¤ì´ë²„ ë‰´ìŠ¤ ìˆ˜ì§‘ ë° ì €ì¥\n",
    "def save_articles_from_naver(query):\n",
    "    client_id = \"_TznE38btYhyzWYsq9XK\"\n",
    "    client_secret = \"06UYVlSHF9\"\n",
    "\n",
    "    encoded_query = urllib.parse.quote(query)\n",
    "    headers = {\n",
    "        \"X-Naver-Client-Id\": client_id,\n",
    "        \"X-Naver-Client-Secret\": client_secret\n",
    "    }\n",
    "\n",
    "    display = 100\n",
    "    saved_count_by_domain = {domain: 0 for domain in CRAWLER_FUNCTION_MAP.keys()}\n",
    "\n",
    "    for start in range(1, 1000 + 1, display):\n",
    "        url = f\"https://openapi.naver.com/v1/search/news.json?query={encoded_query}&display={display}&start={start}&sort=date\"\n",
    "        response = requests.get(url, headers=headers)\n",
    "\n",
    "        if response.status_code != 200:\n",
    "            print(f\"âŒ ìš”ì²­ ì‹¤íŒ¨ at start={start}: {response.status_code}\")\n",
    "            continue\n",
    "\n",
    "        data = response.json()\n",
    "        items = data.get(\"items\", [])\n",
    "        if not items:\n",
    "            break\n",
    "\n",
    "        for item in items:\n",
    "            originallink = item.get(\"originallink\", \"\")\n",
    "            domain = urlparse(originallink).netloc\n",
    "\n",
    "            if domain in CRAWLER_FUNCTION_MAP:\n",
    "                article = CRAWLER_FUNCTION_MAP[domain](originallink)\n",
    "                if article:\n",
    "                    success = save_to_supabase(article)\n",
    "                    if success:\n",
    "                        saved_count_by_domain[domain] += 1\n",
    "\n",
    "        if len(items) < display:\n",
    "            break\n",
    "\n",
    "    # âœ… ìµœì¢… ê²°ê³¼ ì¶œë ¥\n",
    "    print(\"\\nâœ… ì €ì¥ ìš”ì•½\")\n",
    "    for domain, count in saved_count_by_domain.items():\n",
    "        media = MEDIA_NAME_MAP.get(domain, domain)\n",
    "        print(f\"ğŸ“° {media} ê¸°ì‚¬ ì´ {count}ê±´ Supabaseì— ì €ì¥ ì™„ë£Œ\")\n",
    "\n",
    "# âœ… ì‹¤í–‰\n",
    "save_articles_from_naver(\"ë”¥í˜ì´í¬\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
