{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4a2edb1",
   "metadata": {},
   "source": [
    "# **연합뉴스 크롤러만 적용함**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "aeeb5cda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 저장 완료: 美 공무원들도 당하는 AI 사기…FBI서 음성메시지 주의 경보\n",
      "✅ 저장 완료: \"여자라서 죽었다\" 강남역 살인사건 9주기…'여혐 규탄' 시위 곳곳서\n",
      "✅ 저장 완료: 소개팅 여성이 신체 접촉 거절하자 '백초크'한 20대남\n",
      "✅ 저장 완료: 이재명 \"여성 받는 차별 개선 노력해야…남녀 구분해 갈등하는 것 옳지 않아\"\n",
      "✅ 저장 완료: 위장전입 이재명·울보 김문수…'수백만 클릭' 부르는 매운맛 '가짜뉴스 밈'[가짜 판치는 SNS정치①]\n",
      "✅ 저장 완료: 부산교육청, 교사 350명 대상 ‘디지털 성폭력 예방 연수’…딥페이크 대응\n",
      "✅ 저장 완료: 李대행 \"대통령 선거, 국민 통합 이루는 역사적 전환점 돼야\"\n",
      "✅ 저장 완료: 오는 12일부터 6·3대선 공식선거운동…\"일반 유권자도 선거운동 가능\"\n",
      "✅ 저장 완료: 광주경찰, 카카오T와 보이스피싱·딥페이크 예방 홍보\n",
      "✅ 저장 완료: ‘대선 대비’ 전국 경찰 지휘부 화상회의…“우발상황 철저 대비”\n",
      "✅ 저장 완료: 라온시큐어, 주식 액면병합 후 거래 재개…모바일신분증·양자내성암호·AI보안 성장기반 강화\n",
      "✅ 저장 완료: \"유전이야말로 가장 강력한 AI\"…예쁜 맨얼굴 영상으로 '합성' 논란 잠재운 신부\n",
      "✅ 저장 완료: 여가부, 10.9억원 추경…디지털성범죄 피해자 지원\n",
      "\n",
      "📰 기사 총 13건 Supabase에 저장 완료\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import urllib.parse\n",
    "from urllib.parse import urlparse\n",
    "from bs4 import BeautifulSoup\n",
    "from supabase import create_client, Client\n",
    "\n",
    "# ✅ Supabase 설정\n",
    "SUPABASE_URL = \"https://ypyujiaoeaqykbqetjef.supabase.co\"\n",
    "SUPABASE_KEY = \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6InlweXVqaWFvZWFxeWticWV0amVmIiwicm9sZSI6ImFub24iLCJpYXQiOjE3NDY1NDUyNTQsImV4cCI6MjA2MjEyMTI1NH0.RuR9l89gxCcMkSzO053EHluQ0ers-piN4SUjZ-LtWjU\"\n",
    "supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)\n",
    "\n",
    "# ✅ 아시아경제 크롤링 함수\n",
    "def crawl_asiae_news(url):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0'\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        print(\"❌ 기사 요청 실패:\", response.status_code)\n",
    "        return None\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # 제목 추출\n",
    "    title_tag = soup.find('h1')  # 그냥 첫 번째 h1 태그 찾기\n",
    "    title = title_tag.get_text(strip=True) if title_tag else \"제목 없음\"\n",
    "    \n",
    "    body_tag = soup.find('div', class_='article')\n",
    "\n",
    "    if body_tag:\n",
    "        # class가 txt_prohibition인 p 태그는 제외하고 텍스트 추출\n",
    "        p_tags = [p for p in body_tag.find_all('p') if 'txt_prohibition' not in p.get('class', [])]\n",
    "\n",
    "        raw_text = \"\\n\".join([p.get_text(strip=True) for p in p_tags]).strip()\n",
    "\n",
    "        # 각 줄 공백 제거 및 빈 줄 제거\n",
    "        body_lines = [line.strip() for line in raw_text.split(\"\\n\") if line.strip()]\n",
    "\n",
    "        body = ''.join(body_lines)\n",
    "    else:\n",
    "        body = \"본문 없음\"\n",
    "\n",
    "\n",
    "    return {\n",
    "        \"title\": title,\n",
    "        \"url\": url,\n",
    "        \"body\": body,\n",
    "        \"media\": \"아시아경제\"\n",
    "    }\n",
    "\n",
    "# ✅ Supabase에 저장\n",
    "def save_to_supabase(data):\n",
    "    try:\n",
    "        # 중복 확인 (url 기준)\n",
    "        existing = supabase.table(\"articles\").select(\"id\").eq(\"url\", data[\"url\"]).execute()\n",
    "        if existing.data:\n",
    "            print(\"⚠️ 이미 저장된 기사:\", data[\"url\"])\n",
    "            return\n",
    "        response = supabase.table(\"articles\").insert([data]).execute()\n",
    "        print(\"✅ 저장 완료:\", data[\"title\"])\n",
    "    except Exception as e:\n",
    "        print(\"❌ 저장 실패:\", e)\n",
    "\n",
    "# ✅ 네이버 뉴스 검색 후 연합뉴스 기사만 저장\n",
    "def save_articles_from_naver(query):\n",
    "    client_id = \"_TznE38btYhyzWYsq9XK\"\n",
    "    client_secret = \"06UYVlSHF9\"\n",
    "\n",
    "    encoded_query = urllib.parse.quote(query)\n",
    "    headers = {\n",
    "        \"X-Naver-Client-Id\": client_id,\n",
    "        \"X-Naver-Client-Secret\": client_secret\n",
    "    }\n",
    "\n",
    "    display = 100\n",
    "    total_saved = 0\n",
    "\n",
    "    for start in range(1, 1000 + 1, display):\n",
    "        url = f\"https://openapi.naver.com/v1/search/news.json?query={encoded_query}&display={display}&start={start}&sort=date\"\n",
    "        response = requests.get(url, headers=headers)\n",
    "\n",
    "        if response.status_code != 200:\n",
    "            print(f\"❌ 요청 실패 at start={start}: {response.status_code}\")\n",
    "            continue\n",
    "\n",
    "        data = response.json()\n",
    "        items = data.get(\"items\", [])\n",
    "        if not items:\n",
    "            break\n",
    "\n",
    "        for item in items:\n",
    "            originallink = item.get(\"originallink\", \"\")\n",
    "            domain = urlparse(originallink).netloc\n",
    "            if domain == \"view.asiae.co.kr\": # 수정 ✅✅✅✅✅✅\n",
    "                article = crawl_asiae_news(originallink) # ✅✅✅✅✅✅\n",
    "                if article:\n",
    "                    save_to_supabase(article)\n",
    "                    total_saved += 1\n",
    "\n",
    "        if len(items) < display:\n",
    "            break\n",
    "\n",
    "    print(f\"\\n📰 기사 총 {total_saved}건 Supabase에 저장 완료\")\n",
    "\n",
    "# ✅ 실행\n",
    "save_articles_from_naver(\"딥페이크\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537bb5c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ 이미 저장된 기사: https://www.news1.kr/politics/assembly/5785957\n",
      "⚠️ 이미 저장된 기사: https://www.newsis.com/view/NISX20250516_0003178759\n",
      "⚠️ 이미 저장된 기사: https://www.newsis.com/view/NISX20250517_0003179429\n",
      "⚠️ 이미 저장된 기사: https://www.newsis.com/view/NISX20250516_0003178893\n",
      "✅ 저장 완료: 디지털 아노미, 알고크러시... AI 문명 시대에 대한 사회학자의 경고\n",
      "⚠️ 이미 저장된 기사: https://www.chosun.com/politics/election2025/2025/05/16/3KMOI3VZRZGW7OJQ4IZ42RPSJY/?utm_source=naver&utm_medium=referral&utm_campaign=naver-news\n",
      "⚠️ 이미 저장된 기사: https://www.yna.co.kr/view/AKR20250516115200001?input=1195m\n",
      "✅ 저장 완료: 이재명, 여성공약 \"교제폭력·스토킹·디지털성범죄 강력 대응할 것\"\n",
      "⚠️ 이미 저장된 기사: https://www.newsis.com/view/NISX20250516_0003178868\n",
      "⚠️ 이미 저장된 기사: https://www.news1.kr/politics/assembly/5785352\n",
      "✅ 저장 완료: [속보]이재명 \"딥페이크 등 디지털 성범죄 집중 모니터링\"\n",
      "⚠️ 이미 저장된 기사: https://www.newsis.com/view/NISX20250516_0003178806\n",
      "⚠️ 이미 저장된 기사: https://www.newsis.com/view/NISX20250516_0003178097\n",
      "⚠️ 이미 저장된 기사: https://www.chosun.com/national/court_law/2025/05/15/3T2BMR6CZ5ACVCGK6HHG6WAZIY/?utm_source=naver&utm_medium=referral&utm_campaign=naver-news\n",
      "⚠️ 이미 저장된 기사: https://www.newsis.com/view/NISX20250515_0003177499\n",
      "⚠️ 이미 저장된 기사: https://www.news1.kr/it-science/general-it/5784220\n",
      "✅ 저장 완료: \"AI 확산에 통신서비스 행태 변화...이용자보호 새 접근 방식 필요\"\n",
      "⚠️ 이미 저장된 기사: https://www.yna.co.kr/view/AKR20250515098500056?input=1195m\n",
      "⚠️ 이미 저장된 기사: https://www.yna.co.kr/view/AKR20250515061200001?input=1195m\n",
      "⚠️ 이미 저장된 기사: https://www.newsis.com/view/NISX20250515_0003176700\n",
      "⚠️ 이미 저장된 기사: https://www.news1.kr/local/jeju/5783604\n",
      "⚠️ 이미 저장된 기사: https://www.news1.kr/politics/assembly/5783580\n",
      "⚠️ 이미 저장된 기사: https://www.news1.kr/society/incident-accident/5783095\n",
      "⚠️ 이미 저장된 기사: https://www.yna.co.kr/view/AKR20250514101700505?input=1195m\n",
      "⚠️ 이미 저장된 기사: https://www.chosun.com/national/education/2025/05/15/KXEGRV2C4JAUJGSDG3WU2HXHMM/?utm_source=naver&utm_medium=referral&utm_campaign=naver-news\n",
      "⚠️ 이미 저장된 기사: https://www.news1.kr/it-science/cc-newmedia/5782936\n",
      "⚠️ 이미 저장된 기사: https://www.yna.co.kr/view/AKR20250514138700017?input=1195m\n",
      "⚠️ 이미 저장된 기사: https://www.newsis.com/view/NISX20250514_0003175833\n",
      "✅ 저장 완료: 하루 만에 270건 늘었다… '불법 딥페이크'에 선관위·경찰 비상\n",
      "⚠️ 이미 저장된 기사: https://www.newsis.com/view/NISX20250514_0003175121\n",
      "⚠️ 이미 저장된 기사: https://www.yna.co.kr/view/AKR20250514039200017?input=1195m\n",
      "⚠️ 이미 저장된 기사: https://www.newsis.com/view/NISX20250514_0003174700\n",
      "✅ 저장 완료: \"다음 대통령은 다음(Daum)과 함께\"…제21대 대선 특집 페이지\n",
      "⚠️ 이미 저장된 기사: https://www.news1.kr/it-science/general-it/5782070\n",
      "⚠️ 이미 저장된 기사: https://www.newsis.com/view/NISX20250514_0003174607\n",
      "⚠️ 이미 저장된 기사: https://www.news1.kr/society/incident-accident/5781886\n",
      "⚠️ 이미 저장된 기사: https://www.newsis.com/view/NISX20250513_0003173845\n",
      "⚠️ 이미 저장된 기사: https://www.yna.co.kr/view/AKR20250513114500017?input=1195m\n",
      "⚠️ 이미 저장된 기사: https://www.yna.co.kr/view/AKR20250513047600530?input=1195m\n",
      "✅ 저장 완료: 작년 교권침해 4200여 건…'정당한 생활지도 불응'이 32%\n",
      "⚠️ 이미 저장된 기사: https://www.newsis.com/view/NISX20250513_0003173430\n",
      "⚠️ 이미 저장된 기사: https://www.news1.kr/society/education/5781170\n",
      "⚠️ 이미 저장된 기사: https://www.yna.co.kr/view/AKR20250513041200009?input=1195m\n",
      "⚠️ 이미 저장된 기사: https://www.yna.co.kr/view/AKR20250512074900004?input=1195m\n",
      "⚠️ 이미 저장된 기사: https://www.newsis.com/view/NISX20250512_0003172026\n",
      "⚠️ 이미 저장된 기사: https://www.news1.kr/society/incident-accident/5779858\n",
      "⚠️ 이미 저장된 기사: https://www.newsis.com/view/NISX20250512_0003171989\n",
      "⚠️ 이미 저장된 기사: https://www.news1.kr/politics/general-politics/5779752\n",
      "✅ 저장 완료: 이주호 권한대행 \"국민통합 위해 공정 선거 중요..위법시 무관용\"\n",
      "⚠️ 이미 저장된 기사: https://www.yna.co.kr/view/AKR20250512056800530?input=1195m\n",
      "⚠️ 이미 저장된 기사: https://www.newsis.com/view/NISX20250512_0003171697\n",
      "⚠️ 이미 저장된 기사: https://www.news1.kr/local/daegu-gyeongbuk/5778854\n",
      "⚠️ 이미 저장된 기사: https://www.news1.kr/politics/assembly/5777732\n",
      "⚠️ 이미 저장된 기사: https://www.yna.co.kr/view/AKR20250509066400052?input=1195m\n",
      "⚠️ 이미 저장된 기사: https://www.yna.co.kr/view/AKR20250509050300017?input=1195m\n",
      "⚠️ 이미 저장된 기사: https://www.news1.kr/it-science/general-it/5777457\n",
      "✅ 저장 완료: 강경윤 기자, 김세의 고소…\"故김새론 제보자 찾아간 적 없어\"\n",
      "⚠️ 이미 저장된 기사: https://www.newsis.com/view/NISX20250508_0003167665\n",
      "✅ 저장 완료: \"투자 성공, 40억 집 샀다\" 120억 뜯어간 그녀 정체에 '깜짝'…대처 어떻게\n",
      "⚠️ 이미 저장된 기사: https://www.news1.kr/local/gwangju-jeonnam/5776222\n",
      "✅ 저장 완료: 경찰, 대선 투표일에 16만8000명 투입…\"우발상황 철저 대비\"\n",
      "⚠️ 이미 저장된 기사: https://www.newsis.com/view/NISX20250508_0003167022\n",
      "⚠️ 이미 저장된 기사: https://www.news1.kr/society/incident-accident/5775389\n",
      "⚠️ 이미 저장된 기사: https://www.yna.co.kr/view/AKR20250508047200004?input=1195m\n",
      "⚠️ 이미 저장된 기사: https://www.news1.kr/society/general-society/5775503\n",
      "⚠️ 이미 저장된 기사: https://www.newsis.com/view/NISX20250507_0003166634\n",
      "⚠️ 이미 저장된 기사: https://www.yna.co.kr/view/AKR20250507090300505?input=1195m\n",
      "⚠️ 이미 저장된 기사: https://www.newsis.com/view/NISX20250507_0003165527\n",
      "✅ 저장 완료: 라온시큐어 5대1 액면병합 후 거래 재개... \"국내외 성장기반 강화\"\n",
      "✅ 저장 완료: 시작도 못한 대한민국 AI산업, 발목부터 잡는 '이것'\n",
      "✅ 저장 완료: 챗GPT 3년 됐는데 한국은?…'더 강한 AI규제' 만들기 바빴다\n",
      "⚠️ 이미 저장된 기사: https://www.news1.kr/society/incident-accident/5773697\n",
      "✅ 저장 완료: 어제의 피해자가 오늘의 가해자로…사이버 성폭력 늪에 빠진 10대들\n",
      "⚠️ 이미 저장된 기사: https://www.newsis.com/view/NISX20250504_0003164362\n",
      "✅ 저장 완료: 'AI 윤석열·이재명'이 대선판 흔드나…'불법 딥페이크' 주의보\n",
      "✅ 저장 완료: '쿵쿵' 심장박동까지 훔친 딥페이크…\"구분 더 어렵네\"\n",
      "⚠️ 이미 저장된 기사: https://www.newsis.com/view/NISX20250502_0003162438\n",
      "⚠️ 이미 저장된 기사: https://www.yna.co.kr/view/AKR20250503034400530?input=1195m\n",
      "⚠️ 이미 저장된 기사: https://www.news1.kr/finance/blockchain-fintech/5771501\n",
      "✅ 저장 완료: '딥페이크·불법 리베이트' 등 경찰관 11명, 특진\n",
      "⚠️ 이미 저장된 기사: https://www.news1.kr/society/incident-accident/5772097\n",
      "⚠️ 이미 저장된 기사: https://www.yna.co.kr/view/AKR20250501112800004?input=1195m\n",
      "✅ 저장 완료: \"초등생 자녀 성교육 어쩌지\"…부모들 신청 몰리더니 '조기 마감'\n",
      "⚠️ 이미 저장된 기사: https://www.yna.co.kr/view/AKR20250502070000530?input=1195m\n",
      "✅ 저장 완료: 여가부, 추경 10.9억...디지털성범죄 피해자 지원\n",
      "⚠️ 이미 저장된 기사: https://www.news1.kr/society/general-society/5771963\n",
      "⚠️ 이미 저장된 기사: https://www.newsis.com/view/NISX20250502_0003162646\n",
      "\n",
      "✅ 저장 요약\n",
      "📰 연합뉴스 기사 총 0건 Supabase에 저장 완료\n",
      "📰 조선일보 기사 총 0건 Supabase에 저장 완료\n",
      "📰 뉴시스 기사 총 0건 Supabase에 저장 완료\n",
      "📰 뉴스1 기사 총 0건 Supabase에 저장 완료\n",
      "📰 머니투데이 기사 총 20건 Supabase에 저장 완료\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import urllib.parse\n",
    "from urllib.parse import urlparse\n",
    "from bs4 import BeautifulSoup\n",
    "from supabase import create_client, Client\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import time\n",
    "\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "# ✅ Supabase 설정\n",
    "SUPABASE_URL = \"https://ypyujiaoeaqykbqetjef.supabase.co\"\n",
    "SUPABASE_KEY = \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6InlweXVqaWFvZWFxeWticWV0amVmIiwicm9sZSI6ImFub24iLCJpYXQiOjE3NDY1NDUyNTQsImV4cCI6MjA2MjEyMTI1NH0.RuR9l89gxCcMkSzO053EHluQ0ers-piN4SUjZ-LtWjU\"\n",
    "supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)\n",
    "\n",
    "# ✅ 연합뉴스 크롤링 함수\n",
    "def crawl_yonhap_news(url):\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        return None\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    title_tag = soup.find('h1', class_='tit01')\n",
    "    title = title_tag.get_text(strip=True) if title_tag else \"제목 없음\"\n",
    "\n",
    "    body_tag = soup.find('div', class_='story-news article')\n",
    "    if body_tag:\n",
    "        paragraphs = body_tag.find_all('p')\n",
    "        body_lines = []\n",
    "        for p in paragraphs:\n",
    "            if 'txt-copyright' in p.get('class', []): continue\n",
    "            text = p.get_text(strip=True)\n",
    "            if text:\n",
    "                body_lines.append(text)\n",
    "        body = '\\n'.join(body_lines)\n",
    "    else:\n",
    "        body = \"본문 없음\"\n",
    "\n",
    "    return {\"title\": title, \"url\": url, \"body\": body, \"media\": \"연합뉴스\"}\n",
    "\n",
    "# ✅ 조선일보 크롤링 함수\n",
    "def crawl_chosun_news(url):\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument('--headless')\n",
    "    options.add_argument('--no-sandbox')\n",
    "    options.add_argument('--disable-dev-shm-usage')\n",
    "\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "    driver.get(url)\n",
    "    time.sleep(3)\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    driver.quit()\n",
    "\n",
    "    title_tag = soup.find('h1')\n",
    "    title = title_tag.get_text(strip=True) if title_tag else \"제목 없음\"\n",
    "\n",
    "    body_paragraphs = soup.select('p.article-body__content.article-body__content-text')\n",
    "    body_lines = []\n",
    "    for p in body_paragraphs:\n",
    "        text = p.get_text(strip=True)\n",
    "        if not text:\n",
    "            continue\n",
    "        if any(keyword in text for keyword in ['기자', '무단 전재', '구독', 'Copyright']):\n",
    "            continue\n",
    "        body_lines.append(text)\n",
    "    body = '\\n'.join(body_lines) if body_lines else \"본문 없음\"\n",
    "\n",
    "    return {\"title\": title, \"url\": url, \"body\": body, \"media\": \"조선일보\"}\n",
    "\n",
    "# ✅ 뉴시스 크롤링 함수\n",
    "def crawl_newsis_news(url):\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        return None\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    title_tag = soup.select_one('h1.tit.title_area')\n",
    "    title = title_tag.get_text(strip=True) if title_tag else \"제목 없음\"\n",
    "\n",
    "    body_tag = soup.find('div', class_='viewer')\n",
    "    if body_tag:\n",
    "        raw_text = body_tag.get_text(separator=\"\\n\").strip()\n",
    "        body_lines = [line.strip() for line in raw_text.split(\"\\n\") if line.strip()]\n",
    "        body = '\\n'.join(body_lines)\n",
    "    else:\n",
    "        body = \"본문 없음\"\n",
    "\n",
    "    return {\"title\": title, \"url\": url, \"body\": body, \"media\": \"뉴시스\"}\n",
    "\n",
    "\n",
    "def crawl_news1_news(url):\n",
    "    # ✅ 셀레니움 옵션 설정\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument('--headless')  # 창 없이 실행\n",
    "    options.add_argument('--no-sandbox')\n",
    "    options.add_argument('--disable-dev-shm-usage')\n",
    "\n",
    "    # ✅ 드라이버 실행\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "    driver.get(url)\n",
    "\n",
    "    # ✅ 명시적 대기: 본문이 로딩될 때까지 최대 10초 대기\n",
    "    try:\n",
    "        WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.ID, \"articleBodyContent\"))\n",
    "        )\n",
    "    except:\n",
    "        print(\"⏰ 로딩 실패 또는 타임아웃 발생\")\n",
    "        driver.quit()\n",
    "        return None\n",
    "\n",
    "    # ✅ 페이지 파싱\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    driver.quit()\n",
    "\n",
    "    # ✅ 제목 추출\n",
    "    title_tag = soup.select_one('h1.article-h2-header-title.mb-40')\n",
    "    title = title_tag.get_text(strip=True) if title_tag else \"제목 없음\"\n",
    "\n",
    "    # ✅ 본문 추출\n",
    "    body_tag = soup.find('div', id='articleBodyContent')\n",
    "    if body_tag:\n",
    "        paragraphs = body_tag.find_all('p')\n",
    "        body_lines = [p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True)]\n",
    "        body = '\\n'.join(body_lines)\n",
    "    else:\n",
    "        body = \"본문 없음\"\n",
    "\n",
    "    return {\n",
    "        \"title\": title,\n",
    "        \"url\": url,\n",
    "        \"body\": body,\n",
    "        \"media\": \"뉴스1\"\n",
    "    }\n",
    "    \n",
    "# ✅ 머니투데이 뉴스 크롤링 함수\n",
    "def crawl_mt_news(url):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0'\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        print(\"❌ 기사 요청 실패:\", response.status_code)\n",
    "        return None\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # 제목 추출\n",
    "    title_tag = soup.find('h1', class_=['subject'])\n",
    "    title = title_tag.get_text(strip=True) if title_tag else \"제목 없음\"\n",
    "\n",
    "    # 본문 추출 (p 태그 중에서 저작권 정보 제외)\n",
    "    # 'view_text' 클래스를 가진 div를 찾음 (본문 전체 영역)\n",
    "    body_tag = soup.find('div', class_='view_text')\n",
    "\n",
    "    if body_tag:\n",
    "    # 본문 텍스트만 추출 (줄바꿈 기준 분리)\n",
    "        raw_text = body_tag.get_text(separator=\"\\n\").strip()\n",
    "\n",
    "    # 각 줄의 공백 제거 및 빈 줄 제거\n",
    "        body_lines = [line.strip() for line in raw_text.split(\"\\n\") if line.strip()]\n",
    "\n",
    "    # 다시 줄바꿈 문자로 합침\n",
    "        body = '\\n'.join(body_lines)\n",
    "    else:\n",
    "        body = \"본문 없음\"\n",
    "\n",
    "    return {\n",
    "        \"title\": title,\n",
    "        \"url\": url,\n",
    "        \"body\": body,\n",
    "        \"media\": \"머니투데이\"\n",
    "    }\n",
    "\n",
    "# ✅ 헤럴드경제 크롤링 함수\n",
    "def crawl_heraldcorp_news(url):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0'\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        print(\"❌ 기사 요청 실패:\", response.status_code)\n",
    "        return None\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # 제목 추출\n",
    "    title = soup.select_one('div.news_title > h1')\n",
    "    title = title.get_text(strip=True) if title else \"제목 없음\"\n",
    "\n",
    "    # 본문 추출 (p 태그 중에서 저작권 정보 제외)\n",
    "    # 'view_text' 클래스를 가진 div를 찾음 (본문 전체 영역)\n",
    "    body_tag = soup.find('article', id='articleText')\n",
    "\n",
    "    if body_tag:\n",
    "        # article 안의 모든 p 태그 텍스트를 줄바꿈 기준으로 합침\n",
    "        raw_text = \"\\n\".join([p.get_text(strip=True) for p in body_tag.find_all('p')]).strip()\n",
    "\n",
    "        # 각 줄의 공백 제거 및 빈 줄 제거\n",
    "        body_lines = [line.strip() for line in raw_text.split(\"\\n\") if line.strip()]\n",
    "\n",
    "        # 다시 줄바꿈 문자로 합침\n",
    "        body = '\\n'.join(body_lines)\n",
    "    else:\n",
    "        body = \"본문 없음\"\n",
    "\n",
    "    return {\n",
    "        \"title\": title,\n",
    "        \"url\": url,\n",
    "        \"body\": body,\n",
    "        \"media\": \"헤럴드경제\"\n",
    "    }\n",
    "\n",
    "\n",
    "# ✅ 서울경제 크롤링 함수\n",
    "def crawl_sedaily_news(url):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0'\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        print(\"❌ 기사 요청 실패:\", response.status_code)\n",
    "        return None\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # 제목 추출\n",
    "    title_tag = soup.find('h1', class_=['art_tit'])\n",
    "    title = title_tag.get_text(strip=True) if title_tag else \"제목 없음\"\n",
    "\n",
    "    # 본문 추출 (p 태그 중에서 저작권 정보 제외)\n",
    "    # 'view_text' 클래스를 가진 div를 찾음 (본문 전체 영역)\n",
    "    body_tag = soup.find('div', class_='article_view')\n",
    "\n",
    "    if body_tag:\n",
    "    # figure 태그 제거 (caption 포함)\n",
    "        for fig in body_tag.find_all('figure', class_='art_photo'):\n",
    "            fig.decompose()  # 해당 태그 및 하위 내용 완전 삭제\n",
    "\n",
    "    # <br> 태그를 '\\n'으로 변환\n",
    "        for br in body_tag.find_all('br'):\n",
    "            br.replace_with('\\n')\n",
    "\n",
    "    # 텍스트 추출 후 strip\n",
    "        raw_text = body_tag.get_text(separator=\"\\n\").strip()\n",
    "\n",
    "    # 공백 줄 제거\n",
    "        body_lines = [line.strip() for line in raw_text.split(\"\\n\") if line.strip()]\n",
    "\n",
    "    # 다시 줄바꿈으로 합침\n",
    "        body = '\\n'.join(body_lines)\n",
    "    else:\n",
    "        body = \"본문 없음\"\n",
    "\n",
    "    return {\n",
    "        \"title\": title,\n",
    "        \"url\": url,\n",
    "        \"body\": body,\n",
    "        \"media\": \"서울경제\"\n",
    "    }\n",
    "\n",
    "# ✅ 뉴스핌 크롤링 함수\n",
    "def crawl_newspim_news(url):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0'\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        print(\"❌ 기사 요청 실패:\", response.status_code)\n",
    "        return None\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # 제목 추출\n",
    "    title_tag = soup.find('h2')\n",
    "    title = title_tag.get_text(strip=True) if title_tag else \"제목 없음\"\n",
    "\n",
    "    body_tag = soup.find('div', class_='contents', itemprop='articleBody')\n",
    "\n",
    "    if body_tag:\n",
    "        # articleBody 내 모든 p 태그 텍스트를 줄바꿈 기준으로 합침\n",
    "        raw_text = \"\\n\".join([p.get_text(strip=True) for p in body_tag.find_all('p')]).strip()\n",
    "\n",
    "        # 각 줄 공백 제거 및 빈 줄 제거\n",
    "        body_lines = [line.strip() for line in raw_text.split(\"\\n\") if line.strip()]\n",
    "\n",
    "        body = ''.join(body_lines)\n",
    "\n",
    "    else:\n",
    "        body = \"본문 없음\"\n",
    "\n",
    "    return {\n",
    "        \"title\": title,\n",
    "        \"url\": url,\n",
    "        \"body\": body,\n",
    "        \"media\": \"뉴스핌\"\n",
    "    }\n",
    "\n",
    "\n",
    "# ✅ 데일리안 크롤링 함수\n",
    "def crawl_dailian_news(url):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0'\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        print(\"❌ 기사 요청 실패:\", response.status_code)\n",
    "        return None\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # 제목 추출\n",
    "    title_tag = soup.find('h1', class_=['title'])\n",
    "    title = title_tag.get_text(strip=True) if title_tag else \"제목 없음\"\n",
    "\n",
    "    body_tag = soup.find('div', class_='article')\n",
    "\n",
    "    if body_tag:\n",
    "        # articleBody 내 모든 p 태그 텍스트를 줄바꿈 기준으로 합침\n",
    "        raw_text = \"\\n\".join([p.get_text(strip=True) for p in body_tag.find_all('p')]).strip()\n",
    "\n",
    "        # 각 줄 공백 제거 및 빈 줄 제거\n",
    "        body_lines = [line.strip() for line in raw_text.split(\"\\n\") if line.strip()]\n",
    "\n",
    "        body = ''.join(body_lines)\n",
    "\n",
    "    else:\n",
    "        body = \"본문 없음\"\n",
    "\n",
    "    return {\n",
    "        \"title\": title,\n",
    "        \"url\": url,\n",
    "        \"body\": body,\n",
    "        \"media\": \"데일리안\"\n",
    "    }\n",
    "\n",
    "# ✅ 매일경제 크롤링 함수\n",
    "def crawl_mk_news(url):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0'\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        print(\"❌ 기사 요청 실패:\", response.status_code)\n",
    "        return None\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # 제목 추출\n",
    "    title_tag = soup.find('h2', class_=['news_ttl'])\n",
    "    title = title_tag.get_text(strip=True) if title_tag else \"제목 없음\"\n",
    "    \n",
    "    body_tag = soup.find('div', class_='news_cnt_detail_wrap', itemprop='articleBody')\n",
    "\n",
    "    if body_tag:\n",
    "        p_texts = [p.get_text(strip=True) for p in body_tag.find_all('p')]\n",
    "        p_texts = [text for text in p_texts if text]\n",
    "\n",
    "        if p_texts:\n",
    "            body = ''.join(p_texts)\n",
    "        else:\n",
    "            # p 태그 없거나 빈 경우 br 기준 텍스트 노드 추출\n",
    "            br_texts = [str(t).strip() for t in body_tag.children if t and str(t).strip() and not getattr(t, 'name', None)]\n",
    "            br_texts = [text for text in br_texts if text]\n",
    "            if br_texts:\n",
    "                body = ''.join(br_texts)\n",
    "            else:\n",
    "                body = \"본문 없음\"\n",
    "    else:\n",
    "        body = \"본문 없음\"\n",
    "        \n",
    "    return {\n",
    "        \"title\": title,\n",
    "        \"url\": url,\n",
    "        \"body\": body,\n",
    "        \"media\": \"매일경제\"\n",
    "    }\n",
    "\n",
    "# ✅ 아시아경제 크롤링 함수\n",
    "def crawl_asiae_news(url):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0'\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        print(\"❌ 기사 요청 실패:\", response.status_code)\n",
    "        return None\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # 제목 추출\n",
    "    title_tag = soup.find('h1')  # 그냥 첫 번째 h1 태그 찾기\n",
    "    title = title_tag.get_text(strip=True) if title_tag else \"제목 없음\"\n",
    "    \n",
    "    body_tag = soup.find('div', class_='article')\n",
    "\n",
    "    if body_tag:\n",
    "        # class가 txt_prohibition인 p 태그는 제외하고 텍스트 추출\n",
    "        p_tags = [p for p in body_tag.find_all('p') if 'txt_prohibition' not in p.get('class', [])]\n",
    "\n",
    "        raw_text = \"\\n\".join([p.get_text(strip=True) for p in p_tags]).strip()\n",
    "\n",
    "        # 각 줄 공백 제거 및 빈 줄 제거\n",
    "        body_lines = [line.strip() for line in raw_text.split(\"\\n\") if line.strip()]\n",
    "\n",
    "        body = ''.join(body_lines)\n",
    "    else:\n",
    "        body = \"본문 없음\"\n",
    "\n",
    "\n",
    "    return {\n",
    "        \"title\": title,\n",
    "        \"url\": url,\n",
    "        \"body\": body,\n",
    "        \"media\": \"아시아경제\"\n",
    "    }\n",
    "\n",
    "\n",
    "# ✅ Supabase 저장 함수\n",
    "def save_to_supabase(data):\n",
    "    try:\n",
    "        existing = supabase.table(\"articles\").select(\"id\").eq(\"url\", data[\"url\"]).execute()\n",
    "        if existing.data:\n",
    "            print(\"⚠️ 이미 저장된 기사:\", data[\"url\"])\n",
    "            return False\n",
    "        supabase.table(\"articles\").insert([data]).execute()\n",
    "        print(\"✅ 저장 완료:\", data['title'])\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(\"❌ 저장 실패:\", e)\n",
    "        return False\n",
    "\n",
    "# ✅ 언론사 도메인 → 크롤링 함수 매핑\n",
    "CRAWLER_FUNCTION_MAP = {\n",
    "    \"www.yna.co.kr\": crawl_yonhap_news,\n",
    "    \"www.chosun.com\": crawl_chosun_news,\n",
    "    \"www.newsis.com\": crawl_newsis_news,\n",
    "    \"www.news1.kr\" : crawl_news1_news,\n",
    "    \"news.mt.co.kr\" : crawl_mt_news,\n",
    "    \"biz.heraldcorp.com\" : crawl_heraldcorp_news,\n",
    "    \"www.sedaily.com\" : crawl_sedaily_news,\n",
    "    \"www.newspim.com\" : crawl_newspim_news,\n",
    "    \"www.dailian.co.kr\" : crawl_dailian_news,\n",
    "    \"www.mk.co.kr\" : crawl_mk_news,\n",
    "    \"view.asiae.co.kr\" : crawl_asiae_news,\n",
    "    \"www.khan.co.kr\" : crawl_khan_news,\n",
    "    \n",
    "    \n",
    "}\n",
    "\n",
    "# ✅ 언론사 도메인 → 언론사 이름 매핑\n",
    "MEDIA_NAME_MAP = {\n",
    "    \"www.yna.co.kr\": \"연합뉴스\",\n",
    "    \"www.chosun.com\": \"조선일보\",\n",
    "    \"www.newsis.com\": \"뉴시스\",\n",
    "    \"www.news1.kr\" : \"뉴스1\",\n",
    "    \"news.mt.co.kr\" : \"머니투데이\",\n",
    "    \"biz.heraldcorp.com\" : \"헤럴드경제\",\n",
    "    \"www.sedaily.com\" : \"서울경제\",\n",
    "    \"www.newspim.com\" : \"뉴스핌\",\n",
    "    \"www.dailian.co.kr\" : \"데일리안\",\n",
    "    \"www.mk.co.kr\" : \"매일경제\",\n",
    "    \"view.asiae.co.kr\" : \"아시아경제\",\n",
    "    \"www.khan.co.kr\" : \"경향신문\",\n",
    "    \n",
    "    \n",
    "}\n",
    "\n",
    "# ✅ 네이버 뉴스 수집 및 저장\n",
    "def save_articles_from_naver(query):\n",
    "    client_id = \"_TznE38btYhyzWYsq9XK\"\n",
    "    client_secret = \"06UYVlSHF9\"\n",
    "\n",
    "    encoded_query = urllib.parse.quote(query)\n",
    "    headers = {\n",
    "        \"X-Naver-Client-Id\": client_id,\n",
    "        \"X-Naver-Client-Secret\": client_secret\n",
    "    }\n",
    "\n",
    "    display = 100\n",
    "    saved_count_by_domain = {domain: 0 for domain in CRAWLER_FUNCTION_MAP.keys()}\n",
    "\n",
    "    for start in range(1, 1000 + 1, display):\n",
    "        url = f\"https://openapi.naver.com/v1/search/news.json?query={encoded_query}&display={display}&start={start}&sort=date\"\n",
    "        response = requests.get(url, headers=headers)\n",
    "\n",
    "        if response.status_code != 200:\n",
    "            print(f\"❌ 요청 실패 at start={start}: {response.status_code}\")\n",
    "            continue\n",
    "\n",
    "        data = response.json()\n",
    "        items = data.get(\"items\", [])\n",
    "        if not items:\n",
    "            break\n",
    "\n",
    "        for item in items:\n",
    "            originallink = item.get(\"originallink\", \"\")\n",
    "            domain = urlparse(originallink).netloc\n",
    "\n",
    "            if domain in CRAWLER_FUNCTION_MAP:\n",
    "                article = CRAWLER_FUNCTION_MAP[domain](originallink)\n",
    "                if article:\n",
    "                    success = save_to_supabase(article)\n",
    "                    if success:\n",
    "                        saved_count_by_domain[domain] += 1\n",
    "\n",
    "        if len(items) < display:\n",
    "            break\n",
    "\n",
    "    # ✅ 최종 결과 출력\n",
    "    print(\"\\n✅ 저장 요약\")\n",
    "    for domain, count in saved_count_by_domain.items():\n",
    "        media = MEDIA_NAME_MAP.get(domain, domain)\n",
    "        print(f\"📰 {media} 기사 총 {count}건 Supabase에 저장 완료\")\n",
    "\n",
    "# ✅ 실행\n",
    "save_articles_from_naver(\"딥페이크\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
